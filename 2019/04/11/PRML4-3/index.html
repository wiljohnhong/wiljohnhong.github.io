<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
<title>(PRML Notes) 4.3 Probabilistic Discriminative Models - Wiljohn&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">








    <meta name="description" content="A series of notes taken from Pattern Recognition and Machine Learning.  Previously the discussed generative approach of classification is indirect that fits conditional densities and priors separatel">
<meta name="keywords" content="PRML">
<meta property="og:type" content="article">
<meta property="og:title" content="(PRML Notes) 4.3 Probabilistic Discriminative Models">
<meta property="og:url" content="http://wiljohn.top/2019/04/11/PRML4-3/index.html">
<meta property="og:site_name" content="Wiljohn&#39;s Blog">
<meta property="og:description" content="A series of notes taken from Pattern Recognition and Machine Learning.  Previously the discussed generative approach of classification is indirect that fits conditional densities and priors separatel">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-04-04/download.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-04-04/Screenshot%20from%202019-04-14%2015-49-43.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-04-04/Screenshot%20from%202019-04-14%2015-58-16.png">
<meta property="og:updated_time" content="2019-04-14T08:31:45.317Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="(PRML Notes) 4.3 Probabilistic Discriminative Models">
<meta name="twitter:description" content="A series of notes taken from Pattern Recognition and Machine Learning.  Previously the discussed generative approach of classification is indirect that fits conditional densities and priors separatel">
<meta name="twitter:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-04-04/download.png">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">

<link rel="stylesheet" href="/css/style.css">

<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    

    
    
    
    
    


</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item " href="/archives">Archives</a>
            
            <a class="navbar-item " href="/tags">Tags</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/wiljohnhong">
                
                <i class="fab-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            (PRML Notes) 4.3 Probabilistic Discriminative Models
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-04-11T02:30:04.000Z" itemprop="datePublished">Apr 11 2019</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            18 minutes read (About 2690 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<p>Previously the discussed generative approach of classification is indirect that fits conditional densities and priors separately then applies Bayes' theorem. Now a direct discriminative approach will be discussed with following advantages:</p>
<ul>
<li>Typically fewer adaptive parameters to be determined.</li>
<li>May led to improved predictive performance particularly when conditional density is poorly approximated.</li>
</ul>
<a id="more"></a>
<h2 id="fixed-basis-functions">Fixed Basis Functions</h2>
<p>Later we will consider the fixed nonlinear transformation of the inputs using a vector of basis functions <span class="math inline">\(\boldsymbol\phi(\mathbf{x})​\)</span> as before, to make linear inseparable data separable in higher transformed space.</p>
<p>However, we should note that <span class="math inline">\(\boldsymbol\phi(\mathbf{x})\)</span> will increase the overlap level of conditional density <span class="math inline">\(p\left(\mathbf{x} | \mathcal{C}_{k}\right)\)</span> of different classes, since <span class="math inline">\(\boldsymbol\phi(\mathbf{x})\)</span> is a multi-to-one mapping, so <span class="math inline">\(\boldsymbol\phi(\mathbf{x})\)</span> should be chosen appropriately.</p>
<h2 id="logistic-regression">Logistic Regression</h2>
<p>Recall that for two-class problem the posterior of <span class="math inline">\(\mathcal{C_1}\)</span> can be written in a sigmoid form <span class="math display">\[
p\left(\mathcal{C}_{1} | \boldsymbol{\phi}\right)=y(\boldsymbol{\phi})=\sigma\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\right)
\]</span> and in the previous generative approach, the parameter <span class="math inline">\(\mathbf{w}\)</span> is expressed by <span class="math display">\[
\begin{aligned} \mathbf{w} &amp;=\mathbf{\Sigma}^{-1}\left(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2}\right) \\ w_{0} &amp;=-\frac{1}{2} \boldsymbol{\mu}_{1}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_{1}+\frac{1}{2} \boldsymbol{\mu}_{2}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_{2}+\ln \frac{p\left(\mathcal{C}_{2}\right)}{p\left(\mathcal{C}_{2}\right)} \end{aligned}
\]</span> For <span class="math inline">\(M\)</span>-dimensional feature space <span class="math inline">\(\boldsymbol\phi\)</span>, this results in <span class="math inline">\(O(M^2)\)</span> adjustable parameters. So we can alternatively use <span class="math inline">\(M+1\)</span> adjustable parameters to represent <span class="math inline">\(\mathbf{w}\)</span> directly, and this model is known as <strong>linear regression</strong>, <em>though it is indeed a model for classification</em>.</p>
<h3 id="maximum-likelihood-solution">Maximum Likelihood Solution</h3>
<p>For logistic regression given a dataset <span class="math inline">\(\left\{\boldsymbol\phi_{n}, t_{n}\right\}​\)</span>, the likelihood function of the dataset takes the form <span class="math display">\[
\begin{align}
p(\boldsymbol{\mathsf{t}},\mathbf{\Phi}) &amp;= p(\mathbf{\Phi})p(\boldsymbol{\mathsf{t}}|\mathbf{\Phi}) 
\\&amp;\propto p(\boldsymbol{\mathsf{t}}|\mathbf{\Phi}) &amp;&amp; \scriptstyle{(\text{Since now adjustable parameters only exist in }p(\boldsymbol{\mathsf{t}}|\mathbf{\Phi}).)}
\\ &amp;=\prod_{n=1}^{N}p(t_n|\boldsymbol{\phi}_n)
\\&amp;=\prod_{n=1}^{N} y_{n}^{t_{n}}\left\{1-y_{n}\right\}^{1-t_{n}}
\end{align}
\]</span> where <span class="math inline">\(y_{n}=p\left(\mathcal{C}_{1} | \boldsymbol{\phi_{n}}\right)=\sigma\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}_n\right)​\)</span>. As usual, error function could be defined to take the negative logarithm of <span class="math inline">\(p(\boldsymbol{\mathsf{t}})​\)</span>, which gives <span class="math display">\[
E(\mathbf{w})=-\ln p(\boldsymbol{\mathsf{t}},\mathbf{\Phi})\propto-\sum_{n=1}^{N}\ln p(t_n|\boldsymbol{\phi}_n)=-\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\}
\]</span> which takes the form of the <strong>cross entropy</strong> error function, since by sampling we have <span class="math display">\[
KL(p_\mathrm{real}\|p) = \sum_{n=1}^N \ln \frac{p_\mathrm{real}(t_n,_n)}{p(t_n,_n)} = -\sum_{n=1}^N \ln p(t_n,_n) + \mathrm{const}= -\sum_{n=1}^N \ln p(t_n|_n) + \mathrm{const}
\]</span> Taking the gradient of <span class="math inline">\(E(\mathbf{w})​\)</span>, and use the fact that <span class="math inline">\({d \sigma(a)}/{d a}=\sigma(a)(1-\sigma(a))​\)</span>, we have <span class="math display">\[
\begin{align}
\nabla E(\mathbf{w})&amp;= -\sum_{n=1}^{N} \left\{ t_n\frac{y_n(1-y_n) }{y_n}\boldsymbol{\phi}_n + (1-t_n)\frac{-y_n(1-y_n) }{1-y_n} \boldsymbol{\phi}_n \right\}
\\ &amp;= -\sum_{n=1}^{N} \left\{ t_n(1-y_n)\boldsymbol{\phi}_n - (1-t_n)y_n \boldsymbol{\phi}_n \right\}
\\ &amp;= \sum_{n=1}^{N}\left(y_{n}-t_{n}\right) \boldsymbol{\phi}_{n} \tag{1}
\\ &amp;= \boldsymbol{\Phi}^\mathrm{T}\left(\boldsymbol{\mathsf{y}}-\boldsymbol{\mathsf{t}}\right) 
\end{align}
\]</span> (Refresh: <span class="math inline">\(\boldsymbol{\Phi}^\mathrm{T} = \left\{\boldsymbol\phi_1^{\mathrm{T}} ,...,\boldsymbol\phi_n^{\mathrm{T}} \right\}​\)</span>) Note that this give rise to a sequential update form where patterns are presented one at a time.</p>
<p>It is also worth noting that MLE can exhibit severe over-fitting for datasets that are linearly separable, which means the sigmoid function becomes infinitely steep. This problem will arise even the dataset is large, so long as the dataset is linearly separable. We can solve this by using regularization or introducing prior of <span class="math inline">\(\mathbf{w}​\)</span>.</p>
<h2 id="iterative-reweighted-least-squares">Iterative Reweighted Least Squares</h2>
<p>Note that logistic regression has no closed-form solution of <span class="math inline">\(\nabla E(\mathbf{w})=0​\)</span> due to the nonlinearity in the part <span class="math inline">\(\boldsymbol{\mathsf{y}}=\sigma\left( \boldsymbol{\Phi}\mathbf{w} \right)​\)</span>, but as we will see that <span class="math inline">\(E(\mathbf{w})​\)</span> is indeed still convex. So SGD can be used to approximate to the unique global minimum.</p>
<p>This section introduces a more efficient approach called <strong>Newton-Raphson</strong>, an iterative optimization technique which iteratively make <span class="math inline">\(\nabla E(\mathbf{w})​\)</span> closer to <span class="math inline">\(0​\)</span> with a local quadratic approximation according to <span class="math display">\[
\mathbf{w}^{(\text {new})}=\mathbf{w}^{(\text {old})}-\mathbf{H}^{-1} \nabla E(\mathbf{w}^{(\text {old})})
\]</span> where <span class="math inline">\(\mathbf{H}=\nabla^2 E(\mathbf{w})​\)</span>. It is more clear to understand the idea by changing to form to <span class="math inline">\(\nabla E(\mathbf{w}^{(\text {old})})=\mathbf{H}\left(\mathbf{w}^{(\text {old})}-\mathbf{w}^{(\text {new})}\right)​\)</span>, or refer to the following one dimension illustration.</p>
<p><img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-04-04/download.png"></p>
<p>To gain some intuition, we first apply the this method to the linear regression model with the sum-of-squares error function to see what happens. Recall that in this case <span class="math inline">\(\nabla E(\mathbf{w})=\boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi} \mathbf{w}-\boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}}\)</span>, so <span class="math display">\[
\mathbf{H}=\nabla^2 E(\mathbf{w})=\boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}
\]</span> and thus the update rule becomes <span class="math display">\[
\mathbf{w}^{(\text {new})}=\mathbf{w}^{(\text {old})}-\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1}\left\{\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}^{(\mathrm{old})}-\mathbf{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}}\right\}
=\left(\boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
\]</span> which is updated to the global minimum after just one step, since <span class="math inline">\(\nabla E(\mathbf{w})\)</span> is quadratic.</p>
<p>Now apply to the logistic regression model with the cross-entropy error function, recall <span class="math inline">\(\nabla E(\mathbf{w})= \boldsymbol{\Phi}^\mathrm{T}\left(\boldsymbol{\mathsf{y}}-\boldsymbol{\mathsf{t}}\right) ​\)</span>. To figure out the <span class="math inline">\(\mathbf{H}​\)</span>, first by differential <span class="math display">\[
\begin{align}
\mathrm{d}\nabla E(\mathbf{w}) &amp;= \boldsymbol{\Phi}^\mathrm{T}\mathrm{d}\boldsymbol{\mathsf{y}}
\\ &amp;= \boldsymbol{\Phi}^\mathrm{T} \left(\sigma&#39;\left( \boldsymbol{\Phi}\mathbf{w} \right) \odot \mathrm{d}  (\boldsymbol{\Phi}\mathbf{w})\right)
\\ &amp;= \boldsymbol{\Phi}^\mathrm{T} \mathbf{R}\boldsymbol{\Phi} \mathrm{d}\mathbf{w}
\end{align}
\]</span> where <span class="math inline">\(\mathbf{R} = \text{diag}\left(y_1(1-y_1),...,y_n(1-y_n)\right)​\)</span>, (refresher: <span class="math inline">\(y_n=\sigma\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}_n\right)​\)</span>). so we have <span class="math display">\[
\mathbf{H} = \nabla^2 E(\mathbf{w}) = \left(\boldsymbol{\Phi}^\mathrm{T} \mathbf{R}\boldsymbol{\Phi}\right) ^\mathrm{T} = \boldsymbol{\Phi}^\mathrm{T} \mathbf{R}\boldsymbol{\Phi}
\]</span></p>
<blockquote>
<p>We can now show <span class="math inline">\(E(\mathbf{w})\)</span> is convex, due to <span class="math inline">\(\mathbf{H}\)</span> is definite, i.e. <span class="math inline">\(\mathbf{u}^{\mathrm{T}} \mathbf{H} \mathbf{u}&gt;0, \ \forall \mathbf{u} \neq \mathbf{0}\)</span>, since any element in <span class="math inline">\(\mathbf{R}\)</span> is greater than 0, thus <span class="math display">\[
\mathbf{u}^{\mathrm{T}} \mathbf{H} \mathbf{u} = \mathbf{u}^{\mathrm{T}} \boldsymbol{\Phi}^\mathrm{T} \mathbf{R}\boldsymbol{\Phi} \mathbf{u} = \|\mathbf{R}^{1/2}\boldsymbol{\Phi} \mathbf{u} \|^2 &gt;0
\]</span></p>
</blockquote>
<p>So the update rule takes the form <span class="math display">\[
\begin{align}
\mathbf{w}^{(\text {new})}&amp;=\mathbf{w}^{(\text {old})}-\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}}(\boldsymbol{\mathsf{y}}-\boldsymbol{\mathsf{t}})
\\ &amp;=\left(\boldsymbol{\Phi}^{\mathrm{T}} \mathbf{R} \boldsymbol{\Phi}\right)^{-1}\left\{\boldsymbol{\Phi}^{\mathrm{T}} \mathbf{R} \mathbf{\Phi} \mathbf{w}^{(\mathrm{old})}-\mathbf{\Phi}^{\mathrm{T}}(\boldsymbol{\mathsf{y}}-\boldsymbol{\mathsf{t}})\right\}
\\ &amp;= \left(\boldsymbol{\Phi}^{\mathrm{T}} \mathbf{R} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{\mathrm{T}} \mathbf{R} \mathbf{z}
\end{align}
\]</span> where <span class="math inline">\(\mathbf{z}=\mathbf{\Phi} \mathbf{w}^{(\mathrm{old})}-\mathbf{R}^{-1}(\boldsymbol{\mathsf{y}}-\boldsymbol{\mathsf{t}})\)</span>. We see that this takes the form of normal equations for a weighted least-squares problem with <span class="math inline">\(\mathbf{z}\)</span> as the target vector. Note that the weight matrix <span class="math inline">\(\mathbf{R}\)</span> is not constant since it depends on <span class="math inline">\(\mathbf{w}\)</span> through <span class="math inline">\(y_n=\sigma\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}_n\right)\)</span>, it should be reweighted after each update. So the algorithm is known as <strong>iterative reweighted least squares</strong>, or <strong>IRLS</strong>.</p>
<blockquote>
<p>There is some more content in text about the interpretation of <span class="math inline">\(\mathbf{R}\)</span> and <span class="math inline">\(\mathbf{z}\)</span>.</p>
</blockquote>
<h2 id="multiclass-logistic-regression">Multiclass Logistic Regression</h2>
<p>For <span class="math inline">\(K&gt;2\)</span> classes logistic regression, the posterior are again determined directly through <span class="math inline">\(\mathbf{w}\)</span>, given by a softmax function as <span class="math display">\[
y_{nk}=p\left(\mathcal{C}_{k} | \boldsymbol{\phi}_n\right) = \frac{\exp \left(\mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}_n\right)}{\sum_{j} \exp \left(\mathbf{w}_{j}^{\mathrm{T}} \boldsymbol{\phi}_n\right)}
\]</span> We can express it in a more compact way as <span class="math display">\[
\mathbf{y}_{n}= \frac{\exp \left(\mathbf{W} \boldsymbol{\phi}_n\right)}{ \mathbf{1}^\mathrm{T} \exp \left(\mathbf{W} \boldsymbol{\phi}_n\right)} \equiv \mathrm{softmax} \left(\mathbf{W} \boldsymbol{\phi}_n\right)
\]</span> where <span class="math inline">\(\mathbf{W} = \left\{ \mathbf{w}_1,...,\mathbf{w}_K \right\}^\mathrm{T}​\)</span>. Then the likelihood function of a given dataset is given by <span class="math display">\[
\begin{align}
p(\mathbf{T},\mathbf{\Phi}) &amp;= p(\mathbf{\Phi})p(\mathbf{T}|\mathbf{\Phi}) 
\\&amp;\propto p(\mathbf{T}|\mathbf{\Phi})
\\ &amp;=\prod_{n=1}^{N}p(\mathbf{t}_n|\boldsymbol{\phi}_n)
\\&amp;=\prod_{n=1}^{N} \mathbf{t}_n^\mathrm{T}\mathbf{y}_n
\end{align}
\]</span> where <span class="math inline">\(\mathbf{T} = \left\{ \mathbf{t}_1,...,\mathbf{t}_N \right\}^\mathrm{T}\)</span> is an <span class="math inline">\(N\times K\)</span> matrix of target variables. The cross-entropy error function is again given by taking the negative logarithm of <span class="math inline">\(p(\mathbf{T},\mathbf{\Phi})\)</span> <span class="math display">\[
\begin{align}
E(\mathbf{W}) &amp;= - \ln p(\mathbf{T},\mathbf{\Phi})
\\ &amp;\propto -\sum_{n=1}^{N} \mathbf{t}_n^\mathrm{T}\ln \mathbf{y}_n
\\ &amp;= -\sum_{n=1}^{N} \mathbf{t}_n^\mathrm{T}\ln \frac{\exp \left(\mathbf{W} \boldsymbol{\phi}_n\right)}{ \mathbf{1}^\mathrm{T} \exp \left(\mathbf{W} \boldsymbol{\phi}_n\right)}
\\ &amp;= -\sum_{n=1}^{N} \left(\mathbf{t}_n^\mathrm{T}\mathbf{W} \boldsymbol{\phi}_n - \mathbf{t}_n^\mathrm{T} \mathbf{1} \ln\left( \mathbf{1}^\mathrm{T} \exp \left(\mathbf{W} \boldsymbol{\phi}_n\right) \right) \right)  &amp;&amp;\scriptstyle{(\text{Note that } \mathbf{t}_n^\mathrm{T} \mathbf{1} = 1)}
\\ &amp;= \sum_{n=1}^{N} \left(-\mathbf{t}_n^\mathrm{T}\mathbf{W} \boldsymbol{\phi}_n + \ln\left( \mathbf{1}^\mathrm{T} \exp \left(\mathbf{W} \boldsymbol{\phi}_n\right) \right) \right)
\end{align}
\]</span> Again to figure out <span class="math inline">\(\mathrm{d} E(\mathbf{W}) / \mathrm{d}\mathbf{W}​\)</span>, first by differential we have <span class="math display">\[
\begin{align}
\mathrm{d}E(\mathbf{W}) &amp;\propto \sum_{n=1}^{N} \left(-\mathbf{t}_n^\mathrm{T}\mathrm{d}\mathbf{W} \boldsymbol{\phi}_n + \left( \mathbf{1}^\mathrm{T} \exp \left(\mathbf{W} \boldsymbol{\phi}_n\right) \right)^{-1} \mathbf{1}^\mathrm{T} \left(\exp \left(\mathbf{W} \boldsymbol{\phi}_n\right) \odot (\mathrm{d}\mathbf{W}\boldsymbol{\phi}_n) \right) \right)
\\ &amp;= \sum_{n=1}^{N} \left(-\mathbf{t}_n^\mathrm{T}\mathrm{d}\mathbf{W} \boldsymbol{\phi}_n + \left( \mathbf{1}^\mathrm{T} \exp \left(\mathbf{W} \boldsymbol{\phi}_n\right) \right)^{-1}  \exp \left(\mathbf{W} \boldsymbol{\phi}_n\right) ^\mathrm{T} \mathrm{d}\mathbf{W}\boldsymbol{\phi}_n  \right)
\\ &amp;= \sum_{n=1}^{N} \left(-\mathbf{t}_n^\mathrm{T}\mathrm{d}\mathbf{W} \boldsymbol{\phi}_n + \mathrm{softmax} \left(\mathbf{W} \boldsymbol{\phi}_n\right) ^\mathrm{T} \mathrm{d}\mathbf{W}\boldsymbol{\phi}_n  \right)
\\ &amp;= \sum_{n=1}^{N} \left(-\boldsymbol{\phi}_n^\mathrm{T}  \mathbf{t}_n^\mathrm{T}\mathrm{d}\mathbf{W}  + \boldsymbol{\phi}_n^\mathrm{T}  \mathrm{softmax} \left(\mathbf{W} \boldsymbol{\phi}_n\right) ^\mathrm{T} \mathrm{d}\mathbf{W}  \right)
\end{align}
\]</span> So we obtain <span class="math display">\[
\nabla_{\mathbf{W}} E(\mathbf{W}) = \sum_{n=1}^{N} \left( \mathrm{softmax} \left(\mathbf{W} \boldsymbol{\phi}_n\right)   -  \mathbf{t}_n \right)\boldsymbol{\phi}_n = \sum_{n=1}^{N} \left( \mathbf{y}_n - \mathbf{t}_n \right)\boldsymbol{\phi}_n
\]</span> This result can be decomposed directly to get the equation (4.109) in textbook, since <span class="math inline">\(\mathbf{w}_j\)</span> is the <span class="math inline">\(j\)</span>th row of <span class="math inline">\(\mathbf{W}\)</span> <span class="math display">\[
\nabla_{\mathbf{w}_j} E(\mathbf{W}) = \sum_{n=1}^{N} \left( {y}_{nj} - {t}_{nj} \right)\boldsymbol{\phi}_n \tag{2}
\]</span> To apply IRLS algorithm to multiclass logistic regression, we should use the following result that <span class="math display">\[
\nabla_{\mathbf{w}_{k}} \nabla_{\mathbf{w}_{j}} E\left(\mathbf{W}\right)=-\sum_{n=1}^{N} y_{n k}\left(I_{k j}-y_{n j}\right) \boldsymbol{\phi}_{n} \boldsymbol{\phi}_{n}^{\mathrm{T}}
\]</span> where <span class="math inline">\(I_{kj}\)</span> are the elements of the identity matrix. (proof omitted.)</p>
<h2 id="probit-regression">Probit Regression</h2>
<p>The discussion within this part will be restricted to the simple case of <span class="math inline">\(K=2\)</span> classes.</p>
<p>In practice, not all choices of class-conditional density give rise to such a simple form for the posterior like sigmoid, so it is worth exploring more general types of posterior. Recall that we call the general linear models which take the form <span class="math display">\[
p(t=1 | a)=f(a),\text{ where } a = \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}
\]</span> and the target value can be set according to <span class="math display">\[
\left\{\begin{array}{ll}{t_{n}=1} &amp; {\text { if } a_{n} \geqslant \theta} \\ {t_{n}=0} &amp; {\text { otherwise }}\end{array}\right.
\]</span> We can model <span class="math inline">\(\theta\)</span> as drawn from a probability density <span class="math inline">\(p(\theta)\)</span>, then the corresponding activation function <span class="math inline">\(f\)</span> will be given by the cumulative distribution function <span class="math display">\[
f(a)=\int_{-\infty}^{a} p(\theta) \mathrm{d} \theta
\]</span> as illustrated in the following red line.</p>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-04-04/Screenshot%20from%202019-04-14%2015-49-43.png" alt="The cumulative distribution of Gaussian mixture."><figcaption>The cumulative distribution of Gaussian mixture.</figcaption>
</figure>
<p>As a specific example, where <span class="math inline">\(p(\theta)\)</span> is set as <span class="math inline">\(\mathcal{N}(\theta|0,1)\)</span>, and the corresponding <span class="math inline">\(f\)</span> is given by <span class="math display">\[
\Phi(a)\equiv f(a)=\int_{-\infty}^{a} \mathcal{N}(\theta | 0,1) \mathrm{d} \theta
\]</span></p>
<p>which is known as the <strong>inverse probit function</strong>, and the generalized linear model based on such <span class="math inline">\(\Phi(a)\)</span> as posterior is known as <strong>probit regression</strong>.</p>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-04-04/Screenshot%20from%202019-04-14%2015-58-16.png" alt="Plot of the logistic sigmoid function \sigma(a) and the scaled inverse probit function \Phi(\lambda a), where \lambda^2=\pi/8"><figcaption>Plot of the logistic sigmoid function <span class="math inline">\(\sigma(a)\)</span> and the scaled inverse probit function <span class="math inline">\(\Phi(\lambda a)\)</span>, where <span class="math inline">\(\lambda^2=\pi/8\)</span></figcaption>
</figure>
<p>The inverse probit function also has a sigmoidal shape, and we can find a scaling value <span class="math inline">\(\lambda​\)</span> to make it similar to the sigmoid function by having the same slope at the origin</p>
<p><span class="math display">\[
\begin{align}
\Phi&#39;(\lambda a)|_{a=0} &amp;= \sigma&#39;(a)|_{a=0}
\\ \lambda\mathcal{N}(\lambda a | 0,1)|_{a=0} &amp;= \sigma(a)(1-\sigma(a))|_{a=0}
\\ \frac{\lambda}{\sqrt{2\pi}} &amp;= \frac{1}{4}
\\ \lambda^2 &amp;= \frac{\pi}{8}
\end{align}
\]</span></p>
<h3 id="robustness">Robustness</h3>
<p>The result found by probit regression is similar to those of logistic regression, but it is significantly more sensitive to outliers, since the tails of logistic sigmoid decay asymptotically like <span class="math inline">\(\exp(-x)\)</span> for <span class="math inline">\(x\rightarrow \infty\)</span>, while the inverse probit decays like <span class="math inline">\(\exp(-x^2)\)</span>.</p>
<p>One way to reduce this impact is to introduce a probability <span class="math inline">\(\epsilon\)</span> that the target value <span class="math inline">\(t\)</span> is mislabeled, which leads to a target value distribution for data point <span class="math inline">\(\mathbf{x}\)</span> of the form <span class="math display">\[
\begin{aligned} p(t=1 | \mathbf{x}) &amp;=(1-\epsilon) p(\mathcal{C}_1|\mathbf{x})+\epsilon p(\mathcal{C}_2|\mathbf{x}) \\ &amp;=(1-\epsilon) \sigma(\mathbf{x})+\epsilon(1-\sigma(\mathbf{x})) \\ &amp;=\epsilon+(1-2 \epsilon) \sigma(\mathbf{x}) \\ &amp;\in (\epsilon,1-\epsilon) \end{aligned}
\]</span> This approach can also be viewed as setting the target variable to <span class="math display">\[
\left\{\begin{array}{ll}{t_{n}=\epsilon} &amp; {\text { if } a_{n} \geqslant \theta} \\ {t_{n}=1-\epsilon} &amp; {\text { otherwise }}\end{array}\right.
\]</span></p>
<h2 id="canonical-link-functions">Canonical Link Functions</h2>
<p>We have seen that the same form arising for the gradient like <span class="math inline">\(\sum_n (y_n-t_n)\boldsymbol{\phi}_n\)</span> as was found for</p>
<ul>
<li>the sum-of-squares error function with the linear model in section 3.1.1</li>
<li>the cross-entropy error for the two-class logistic regression model as in equation <span class="math inline">\((1)\)</span></li>
<li>the cross-entropy error for the multi-class logistic regression model as in equation <span class="math inline">\((2)\)</span></li>
</ul>
<p>This section shows that this is a general result of assuming a conditional distribution for the target variable from the exponential family, along with a corresponding choice for the activation function known as the <strong>canonical link function</strong>.</p>
<blockquote>
<p>Details omitted.</p>
</blockquote>

    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/PRML/">#PRML</a></span>
    
    </div>
    
    
    <div class="columns is-mobile is-multiline article-nav">
        <span class="column is-12-mobile is-half-desktop  article-nav-prev">
            
            <a href="/2019/04/14/PRML4-4/">(PRML Notes) 4.4 The Laplace Approximation</a>
            
        </span>
        <span class="column is-12-mobile is-half-desktop  article-nav-next">
            
            <a href="/2019/04/07/PRML4-2/">(PRML Notes) 4.2 Probabilistic Generative Model</a>
            
        </span>
    </div>
    
</article>


<div class="sharebox">
    
<div class="sharethis-inline-share-buttons"></div>
<script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5c1be99859895a00110ffa34&amp;product=inline-share-buttons" async="async"></script>

</div>



<div class="comments">
    <h3 class="title is-4">Comments</h3>
    
<script>
    var disqus_config = function () {
        this.page.url = 'http://wiljohn.top/2019/04/11/PRML4-3/';
        this.page.identifier = 'prml4-3';
        
        this.language = 'en';
        
    };
    (function() {
        var d = document, s = d.createElement('script');  
        s.src = '//' + 'wiljohnhong-github-io' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<div id="disqus_thread">
    
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>

    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2019 wiljohn&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" href="https://github.com/wiljohnhong">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>


    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    


<script src="/js/script.js"></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script>
    
</body>
</html>