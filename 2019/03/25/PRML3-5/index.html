<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
<title>(PRML Notes) 3.5 The Evidence Approximation - Wiljohn&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">








    <meta name="description" content="A series of notes taken from Pattern Recognition and Machine Learning.  General Framework of Fully Bayesian Approach In a fully Bayesian treatment, a prior will be introduced over precision hyperpara">
<meta name="keywords" content="PRML">
<meta property="og:type" content="article">
<meta property="og:title" content="(PRML Notes) 3.5 The Evidence Approximation">
<meta property="og:url" content="http://wiljohn.top/2019/03/25/PRML3-5/index.html">
<meta property="og:site_name" content="Wiljohn&#39;s Blog">
<meta property="og:description" content="A series of notes taken from Pattern Recognition and Machine Learning.  General Framework of Fully Bayesian Approach In a fully Bayesian treatment, a prior will be introduced over precision hyperpara">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-03-22/Screenshot%20from%202019-03-25%2017-15-47.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-23/27012787.jpg">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-03-22/Screenshot%20from%202019-03-25%2017-29-27.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-03-22/Screenshot%20from%202019-03-25%2022-09-59.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-03-22/Screenshot%20from%202019-03-25%2022-10-15.png">
<meta property="og:updated_time" content="2019-03-25T15:01:31.543Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="(PRML Notes) 3.5 The Evidence Approximation">
<meta name="twitter:description" content="A series of notes taken from Pattern Recognition and Machine Learning.  General Framework of Fully Bayesian Approach In a fully Bayesian treatment, a prior will be introduced over precision hyperpara">
<meta name="twitter:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-03-22/Screenshot%20from%202019-03-25%2017-15-47.png">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">

<link rel="stylesheet" href="/css/style.css">

<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    

    
    
    
    
    


</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item " href="/archives">Archives</a>
            
            <a class="navbar-item " href="/tags">Tags</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/wiljohnhong">
                
                <i class="fab-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            (PRML Notes) 3.5 The Evidence Approximation
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-03-25T07:14:04.000Z" itemprop="datePublished">Mar 25 2019</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            20 minutes read (About 2990 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<h2 id="general-framework-of-fully-bayesian-approach">General Framework of Fully Bayesian Approach</h2>
<p>In a fully Bayesian treatment, a prior will be introduced over precision hyperparameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, and predictions are made by marginalizing w.r.t. these hyperparameters as well as <span class="math inline">\(\mathbf{w}\)</span> <span class="math display">\[
p(t | \boldsymbol{\mathsf{t}})=\iiint p(t | \mathbf{w}, \beta) p(\mathbf{w} | \boldsymbol{\mathsf{t}}, \alpha, \beta) p(\alpha, \beta | \boldsymbol{\mathsf{t}}) \mathrm{d} \mathbf{w} \mathrm{d} \alpha \mathrm{d} \beta \tag{1}
\]</span> where the posterior distribution of <span class="math inline">\(\alpha​\)</span> and <span class="math inline">\(\beta​\)</span> is given by <span class="math display">\[
p(\alpha, \beta | \boldsymbol{\mathsf{t}}) \propto p(\boldsymbol{\mathsf{t}} | \alpha, \beta) p(\alpha, \beta)
\]</span> <a id="more"></a></p>
<p>Usually the complete marginalization over all the parameters in <span class="math inline">\(p(t | \boldsymbol{\mathsf{t}})\)</span> is analytically intractable, but we can integrate over <span class="math inline">\(\mathbf{w}\)</span> under specific values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, so an approximation will be applied for the fully Bayesian treatment.</p>
<blockquote>
<p>Under such approximation, the fully Bayesian treatment just takes one more step to find out the optimal <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, instead of manually set them.</p>
</blockquote>
<p>The approximation is based on the assumption that the prior of hyperparameters <span class="math inline">\(p(\alpha, \beta)\)</span> is relatively flat, and the posterior <span class="math inline">\(p(\alpha, \beta | \boldsymbol{\mathsf{t}})\)</span> will be sharply peaked around values <span class="math inline">\(\widehat{\alpha}\)</span> and <span class="math inline">\(\widehat{\beta}\)</span>. Then we can set <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to the most proper values by maximizing the marginal likelihood function <span class="math inline">\(p(\boldsymbol{\mathsf{t}} | \alpha, \beta)\)</span>, which itself is obtained by integrating over <span class="math inline">\(\mathbf{w}\)</span> <span class="math display">\[
\widehat{\alpha}, \widehat{\beta} = \mathop{\arg\max}_{\alpha, \beta} p(\boldsymbol{\mathsf{t}} | \alpha, \beta)=\mathop{\arg\max}_{\alpha, \beta} \int p(\boldsymbol{\mathsf{t}} | \mathbf{w}, \beta) p(\mathbf{w} | \alpha) \mathrm{d} \mathbf{w}
\]</span> Then the predictive distribution <span class="math inline">\((1)\)</span> now becomes <span class="math display">\[
p(t | \boldsymbol{\mathsf{t}}) \simeq p(t | \boldsymbol{\mathsf{t}}, \widehat{\alpha}, \widehat{\beta})=\int p(t | \mathbf{w}, \widehat{\beta}) p(\mathbf{w} | \boldsymbol{\mathsf{t}}, \widehat{\alpha}, \widehat{\beta}) \mathrm{d} \mathbf{w}
\]</span> This framework is known in the machine learning literature as <strong>evidence approximation</strong>.</p>
<blockquote>
<p>Honestly speaking, the decision of choosing <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> is just kind of MLE, which maximizes the model evidence <span class="math inline">\(p(\boldsymbol{\mathsf{t}} | \alpha, \beta)\)</span> under different choices of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, and I think the Bayesian are just not willing to admit that so they introduce such assumption to explain the choice of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> in a Bayesian way :)</p>
</blockquote>
<h2 id="evaluation-of-the-evidence-function">Evaluation of the Evidence Function</h2>
<p>In this section we aim to find out the exact expression for the evidence function <span class="math inline">\(p(\boldsymbol{\mathsf{t}} | \alpha, \beta)\)</span>, under the case of linear regression as we discussed before.</p>
<p>Recall that we have defined <span class="math display">\[
p(\mathbf{w} | \alpha)=\mathcal{N}(\mathbf{w} | \mathbf{0}, \alpha^{-1} \mathbf{I})
\\
p(\boldsymbol{\mathsf{t}}|\mathbf{w}, \beta) = \mathcal{N}\left(\boldsymbol{\mathsf{t}}| \boldsymbol{\Phi}\mathbf{w}, \beta^{-1}\mathbf{I}\right)
\]</span> To evaluate the evidence, one way is to simply use the result in section 2.3.3. However, here we need to make use of the standard form for the normalization coefficient of a Gaussian, so we will evaluate the integral from scratch by completing the suqare. <span class="math display">\[
\begin{align}
p(\boldsymbol{\mathsf{t}} | \alpha, \beta) &amp;= \int p(\boldsymbol{\mathsf{t}} | \mathbf{w}, \beta) p(\mathbf{w} | \alpha) \mathrm{d} \mathbf{w}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \left(\frac{\alpha}{2 \pi}\right)^{M / 2}\int \exp \left\{ -\frac{\beta}{2}\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi} \mathbf{w}\|^{2}-\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \right\} \mathrm{d} \mathbf{w}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \left(\frac{\alpha}{2 \pi}\right)^{M / 2}\int \exp \left\{ -\frac{\beta}{2}\left(\boldsymbol{\mathsf{t}}^{\mathrm{T}} \boldsymbol{\mathsf{t}}-2 \mathbf{w}^{\mathrm{T}} \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}}+\mathbf{w}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}\right)-\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \right\} \mathrm{d} \mathbf{w}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \left(\frac{\alpha}{2 \pi}\right)^{M / 2}\int \exp \left\{ 
-\frac{1}{2}\mathbf{w}^{\mathrm{T}} \left(\alpha\mathbf{I}+\beta\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \right) \mathbf{w}
+\beta \mathbf{w}^{\mathrm{T}} \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
-\frac{\beta}{2} \boldsymbol{\mathsf{t}}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
\right\} \mathrm{d} \mathbf{w}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \left(\frac{\alpha}{2 \pi}\right)^{M / 2}\int \exp \left\{ 
-\frac{1}{2}\mathbf{w}^{\mathrm{T}} \mathbf{A} \mathbf{w}
+ \mathbf{w}^{\mathrm{T}} \mathbf{A} \left(\beta\mathbf{A}^{-1}\boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}}\right)
-\frac{\beta}{2} \boldsymbol{\mathsf{t}}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
\right\} \mathrm{d} \mathbf{w}
&amp;&amp; \scriptstyle{(\text{define } \mathbf{A} = \alpha\mathbf{I}+\beta\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\text{, note that } \mathbf{A}^\mathrm{T} = \mathbf{A})}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \left(\frac{\alpha}{2 \pi}\right)^{M / 2}\int \exp \left\{ 
-\frac{1}{2}\mathbf{w}^{\mathrm{T}} \mathbf{A} \mathbf{w}
+ \mathbf{w}^{\mathrm{T}} \mathbf{A} \mathbf{m}_{N}
-\frac{\beta}{2} \boldsymbol{\mathsf{t}}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
\right\} \mathrm{d} \mathbf{w}
&amp;&amp; \scriptstyle{(\text{define } \mathbf{m}_{N}=\beta \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}})}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \left(\frac{\alpha}{2 \pi}\right)^{M / 2}\int \exp \left\{ 
-\frac{1}{2}\left(\mathbf{w}-\mathbf{m}_{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}_{N}\right)
+\frac{1}{2}\mathbf{m}_{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}_{N}
-\frac{\beta}{2} \boldsymbol{\mathsf{t}}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
\right\} \mathrm{d} \mathbf{w} \tag{2}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \left(\frac{\alpha}{2 \pi}\right)^{M / 2} (2 \pi)^{M / 2}|\mathbf{A}|^{-1 / 2}\exp \left\{
-\frac{\beta}{2} \boldsymbol{\mathsf{t}}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
+\frac{1}{2}\mathbf{m}_{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}_{N}
\right\}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \alpha^{M / 2} |\mathbf{A}|^{-1 / 2}\exp \left\{
-\frac{\beta}{2} \boldsymbol{\mathsf{t}}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
+\mathbf{m}_{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}_{N}
-\frac{1}{2}\mathbf{m}_{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}_{N}
\right\}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \alpha^{M / 2} |\mathbf{A}|^{-1 / 2}\exp \left\{
-\frac{\beta}{2} \boldsymbol{\mathsf{t}}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
+\beta\boldsymbol{\mathsf{t}}^{\mathrm{T}}\boldsymbol{\Phi} \mathbf{m}_{N}
-\frac{1}{2}\mathbf{m}_{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}_{N}
\right\}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \alpha^{M / 2} |\mathbf{A}|^{-1 / 2}\exp \left\{
-\frac{\beta}{2} \|\boldsymbol{\mathsf{t}} - \boldsymbol{\Phi}\mathbf{m}_N\|^2
+\frac{\beta}{2} \mathbf{m}_{N}^{\mathrm{T}} \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi} \mathbf{m}_{N}
-\frac{1}{2}\mathbf{m}_{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}_{N}
\right\}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \alpha^{M / 2} |\mathbf{A}|^{-1 / 2}\exp \left\{
-\frac{\beta}{2} \|\boldsymbol{\mathsf{t}} - \boldsymbol{\Phi}\mathbf{m}_N\|^2
- \mathbf{m}_{N}^{\mathrm{T}}\left( -\frac{\beta}{2} \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi} + \frac{\alpha}{2}\mathbf{I}+\frac{\beta}{2}\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \right)\mathbf{m}_{N}
\right\}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \alpha^{M / 2} |\mathbf{A}|^{-1 / 2}\exp \left\{
-\frac{\beta}{2} \|\boldsymbol{\mathsf{t}} - \boldsymbol{\Phi}\mathbf{m}_N\|^2
-\frac{\alpha}{2} \mathbf{m}_{N}^{\mathrm{T}}  \mathbf{m}_{N}
\right\}
\\
&amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \alpha^{M / 2} |\mathbf{A}|^{-1 / 2}\exp \{
-E\left(\mathbf{m}_{N}\right)
\} &amp;&amp;\scriptstyle{(\text{define } E\left(\mathbf{m}_{N}\right)=\frac{\beta}{2}\left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2}+\frac{\alpha}{2} \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N} )}
\end{align}
\]</span></p>
<p>So we have the log of evidence function to be <span class="math display">\[
\ln p(\boldsymbol{\mathsf{t}} | \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi)
\]</span></p>
<h3 id="polynomial-curve-fitting-example">Polynomial Curve FItting Example</h3>
<p>Return to the problem of polynomial curve fitting problem in section 1.1, if we plot the model evidence against the order of the polynomial <span class="math inline">\(M​\)</span>, under some fixed <span class="math inline">\(\alpha​\)</span> and <span class="math inline">\(\beta​\)</span>, we see that the evidence favors the model with <span class="math inline">\(M=3​\)</span>.</p>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-03-22/Screenshot%20from%202019-03-25%2017-15-47.png" alt="Plot of the model evidence versus the order M"><figcaption>Plot of the model evidence versus the order <span class="math inline">\(M\)</span></figcaption>
</figure>
<p>Recall that the data is generated from a sinusoidal function, whose Taylor expansion consists of only odd polynomial functions, so even model with <span class="math inline">\(M=2​\)</span> is more complex than <span class="math inline">\(M=1​\)</span>, the data fit is improved little, resulting in low model evidence.</p>
<p>And for the larger <span class="math inline">\(M&gt;3\)</span>, only small improvements is obtained in fitting data, while suffering more from increasing model complexity.</p>
<p>These can be seen from the previous plots in section 1.1.</p>
<p><img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-23/27012787.jpg"></p>
<p>Also note that the generalization error is roughly constant between <span class="math inline">\(M=3\)</span> and <span class="math inline">\(M=8\)</span>, however, the evidence values show a clear preference for <span class="math inline">\(M=3\)</span> since it is the simplest.</p>
<p><img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-03-22/Screenshot%20from%202019-03-25%2017-29-27.png"></p>
<h2 id="maximizing-the-evidence-function">Maximizing the Evidence Function</h2>
<p>In this part we consider the maximization of model evidence <span class="math inline">\(p(\boldsymbol{\mathsf{t}} | \alpha, \beta)\)</span> w.r.t. <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p>First define the eigenvector equation <span class="math display">\[
\left(\beta \boldsymbol\Phi^{\mathrm{T}} \boldsymbol\Phi\right) \mathbf{u}_{i}=\lambda_{i} \mathbf{u}_{i}
\]</span> then it follows that <span class="math display">\[
\mathbf{A}\mathbf{u}_{i} = \left(\alpha\mathbf{I}+ \beta \boldsymbol\Phi^{\mathrm{T}} \boldsymbol\Phi\right) \mathbf{u}_{i}=(\alpha+\lambda_{i}) \mathbf{u}_{i}
\]</span></p>
<h3 id="maximizing-alpha">Maximizing <span class="math inline">\(\alpha\)</span></h3>
<p>If we regard <span class="math inline">\(\mathbf{m}_{N}​\)</span> as a fixed value <span class="math display">\[
\begin{align}
\frac{d}{d \alpha}\ln p(\boldsymbol{\mathsf{t}} | \alpha, \beta) &amp;= \frac{d}{d \alpha} \left(\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi)\right)
\\
&amp;= \frac{M}{2\alpha}+  \frac{d}{d \alpha} \left(-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|\right)
\\
&amp;= \frac{M}{2\alpha}+  \frac{d}{d \alpha} \left(-\frac{\beta}{2}\left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2}-\frac{\alpha}{2} \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}-\frac{1}{2} \sum_{i} \ln \left(\lambda_{i}+\alpha\right)\right)
\\
&amp;= \frac{M}{2\alpha} -\frac{1}{2} \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}-\frac{1}{2} \sum_{i} \frac{1}{\lambda_{i}+\alpha}
\end{align}
\]</span> By setting the derivative to zero, we have <span class="math display">\[
\begin{align}
&amp; \frac{M}{2\alpha} -\frac{1}{2} \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}-\frac{1}{2} \sum_{i} \frac{1}{\lambda_{i}+\alpha} = 0
\\
\Rightarrow \quad &amp; M - \sum_{i} \frac{\alpha}{\lambda_{i}+\alpha} = \alpha \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}
\\
\Rightarrow \quad &amp; \sum_{i} \frac{\lambda_{i}+\alpha}{\lambda_{i}+\alpha} - \sum_{i} \frac{\alpha}{\lambda_{i}+\alpha} = \alpha \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}
\\
\Rightarrow \quad &amp; \sum_{i} \frac{\lambda_i}{\lambda_{i}+\alpha} = \alpha \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}
\end{align}
\]</span> If we define <span class="math display">\[
\gamma = \sum_{i} \frac{\lambda_i}{\lambda_{i}+\alpha}
\]</span> Then we get an implicit solution for <span class="math inline">\(\alpha\)</span> <span class="math display">\[
\alpha=\frac{\gamma}{\mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}}
\]</span> Note that in this solution both <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\mathbf{m}_{N}\)</span> depend on the choice of <span class="math inline">\(\alpha\)</span>, so we can take an iterative procedure to find out the optimal <span class="math inline">\(\alpha\)</span>, by first pick an initial value <span class="math inline">\(\alpha_0\)</span>, then evaluate <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\mathbf{m}_{N}\)</span>, compute the new <span class="math inline">\(\alpha\)</span> and repeat until convergence.</p>
<h3 id="maximizing-beta">Maximizing <span class="math inline">\(\beta\)</span></h3>
<p>Again we regard <span class="math inline">\(\mathbf{m}_{N}\)</span> as a fixed value <span class="math display">\[
\begin{align}
\frac{d}{d \beta}\ln p(\boldsymbol{\mathsf{t}} | \alpha, \beta) &amp;= \frac{d}{d \beta} \left(\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi)\right)
\\
&amp;= \frac{N}{2\beta}+  \frac{d}{d \beta} \left(-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|\right)
\\
&amp;= \frac{N}{2\beta}+  \frac{d}{d \beta} \left(-\frac{\beta}{2}\left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2}-\frac{\alpha}{2} \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}-\frac{1}{2} \sum_{i} \ln \left(\lambda_{i}+\alpha\right)\right)
\\
&amp;= \frac{N}{2\beta} -\frac{1}{2}\left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2}-\frac{1}{2\beta} \sum_{i} \frac{\lambda_i}{\lambda_{i}+\alpha}
\\
&amp;= \frac{N}{2\beta} -\frac{1}{2}\left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2}-\frac{\gamma}{2\beta}
\end{align}
\]</span> In the last second line we use the fact that <span class="math inline">\(\lambda_i\)</span> is proportional to <span class="math inline">\(\beta\)</span> since we defined that <span class="math inline">\(\left(\beta \boldsymbol\Phi^{\mathrm{T}} \boldsymbol\Phi\right) \mathbf{u}_{i}=\lambda_{i} \mathbf{u}_{i}\)</span>, where we have <span class="math inline">\(d \lambda_{i} / d \beta=\lambda_{i} / \beta\)</span>. Then by setting the derivative to zero, we have <span class="math display">\[
\begin{align}
&amp; \frac{N}{2\beta} -\frac{1}{2}\left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2}-\frac{\gamma}{2\beta} = 0
\\
\Rightarrow \quad &amp; \frac{N-\gamma}{\beta} -\left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2} = 0
\\
\Rightarrow \quad &amp; \frac{1}{\beta}  =  \frac{1}{N-\gamma} \left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2}
\end{align}
\]</span> Again we can take an iterative procedure to find out the optimal <span class="math inline">\(\beta\)</span>.</p>
<h2 id="effective-number-of-parameters">Effective Number of Parameters</h2>
<h3 id="insight-from-alpha">Insight from <span class="math inline">\(\alpha\)</span></h3>
<p>In this part we will see that the variable <span class="math inline">\(\gamma​\)</span> has an elegant interpretation as the effective number of model parameters.</p>
<p>Recall that in equation <span class="math inline">\((2)\)</span> we see the term <span class="math inline">\(\mathbf{m}_{N}\)</span> is actually just the maximum posterior estimation <span class="math inline">\(\mathbf{w}_\mathrm{MAP}\)</span>, that takes the form <span class="math display">\[
\mathbf{w}_\mathrm{MAP} = \mathbf{m}_{N}=\beta \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}} = \beta \left(\alpha \mathbf{I}+\beta \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
\]</span> and without any prior, the MLE of the parameter is <span class="math display">\[
\mathbf{w}_\mathrm{ML} = \beta \left(\beta \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}}
\]</span> If we rotate the set of axes in parameter space to align with the eigenvectors <span class="math inline">\(\mathbf{u}_i​\)</span> defined in <span class="math inline">\(\left(\beta \boldsymbol\Phi^{\mathrm{T}} \boldsymbol\Phi\right) \mathbf{u}_{i}=\lambda_{i} \mathbf{u}_{i}​\)</span>, and then project the vector <span class="math inline">\(\mathbf{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}}​\)</span> onto this new axes to find out its new coordinates <span class="math inline">\((c_1,c_2,...,c_M)​\)</span> according to the unique linear combination <span class="math display">\[
\mathbf{\Phi}^{\mathrm{T}} \boldsymbol{\mathsf{t}} = \sum_i c_i \mathbf{u}_i
\]</span> we can get the new coordinates for <span class="math inline">\(\mathbf{w}_\mathrm{MAP}​\)</span> and <span class="math inline">\(\mathbf{w}_\mathrm{ML}​\)</span> <span class="math display">\[
\mathbf{w}_\mathrm{MAP} = \beta\sum_i c_i \left(\alpha \mathbf{I}+\beta \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right)^{-1}  \mathbf{u}_i= \beta\sum_i \frac{c_i}{\alpha+\lambda_i}  \mathbf{u}_i
\\
\mathbf{w}_\mathrm{ML} = \beta\sum_i c_i \left(\beta \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right)^{-1}  \mathbf{u}_i= \beta\sum_i \frac{c_i}{\lambda_i}  \mathbf{u}_i
\]</span></p>
<p>So the coordinate value of <span class="math inline">\(\mathbf{w}_\mathrm{MAP}\)</span> on each axis is proportional to <span class="math inline">\(\mathbf{w}_\mathrm{ML}\)</span> with the magnitude <span class="math inline">\(\lambda_i / (\lambda_i+\alpha)\)</span>. This ratio will lie between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, and consequently we have <span class="math inline">\(0 \leqslant \gamma \leqslant M\)</span>.</p>
<p>Along some axis <span class="math inline">\(i\)</span>, if <span class="math inline">\(\lambda_i \gg \alpha\)</span>, i.e. there is little constraint for parameter that defined by the prior, then <span class="math inline">\(\lambda_i / (\lambda_i+\alpha) \rightarrow 1\)</span>. Such parameters are called <strong>well determined</strong>, since their values fit fully into the data.</p>
<p>While conversely, if <span class="math inline">\(\lambda_{i} \ll \alpha​\)</span>, then <span class="math inline">\(\lambda_i / (\lambda_i+\alpha) \rightarrow 0​\)</span>, the corresponding <span class="math inline">\(i​\)</span>th parameter of <span class="math inline">\(\mathbf{w}_\mathrm{MAP}​\)</span> will go to zero. In such directions the likelihood function is relatively insensitive to the parameter value and so the parameter has been set to a small value by the prior.</p>
<p>Till now we see that the quantity <span class="math inline">\(\gamma\)</span> therefore measures the effective total number of well determined parameters.</p>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-03-22/Screenshot%20from%202019-03-25%2022-09-59.png" alt="Contours of the likelihood function (red) and the prior (green) in which the axes in parameter space have been rotated to align with the \mathbf{u}_i."><figcaption>Contours of the likelihood function (red) and the prior (green) in which the axes in parameter space have been rotated to align with the <span class="math inline">\(\mathbf{u}_i\)</span>.</figcaption>
</figure>
<p>Intuitively, as shown in the figure above, smaller <span class="math inline">\(\lambda_i​\)</span> corresponds to a greater elongation of the contour of likelihood (recall that in section 2.3.0, <span class="math inline">\(\lambda_i​\)</span> means the precision of Gaussian along each axis, under a transformed coordinate), so the ratio <span class="math inline">\(\lambda_i / (\lambda_i+\alpha) \rightarrow 0​\)</span>, and we see that the value of <span class="math inline">\(\mathbf{w}_\mathrm{MAP}​\)</span> in the first axis <span class="math inline">\(w_1​\)</span> is smaller than that in the second axis <span class="math inline">\(w_2​\)</span>.</p>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-03-22/Screenshot%20from%202019-03-25%2022-10-15.png" alt="An example showing the changes of different w_i and \gamma as the result of tuning the parameter \alpha​ in the range [0,\infty​]"><figcaption>An example showing the changes of different <span class="math inline">\(w_i\)</span> and <span class="math inline">\(\gamma\)</span> as the result of tuning the parameter <span class="math inline">\(\alpha​\)</span> in the range <span class="math inline">\([0,\infty​]\)</span></figcaption>
</figure>
<h3 id="insight-from-beta">Insight from <span class="math inline">\(\beta\)</span></h3>
<p>Recall that we have found the MLE for <span class="math inline">\(\beta\)</span> in section 3.1 which takes the form <span class="math display">\[
\frac{1}{\beta_{\mathrm{ML}}} = \frac{1}{N}\| \boldsymbol{\mathsf{t}} - \mathbf{\Phi w} \|^2
\]</span> and we know that it is a biased estimation. Actually for model with <span class="math inline">\(M\)</span> parameters, the unbiased estimation for <span class="math inline">\(\beta​\)</span> is <span class="math display">\[
\frac{1}{\beta_{\mathrm{ML}}} = \frac{1}{N-M}\| \boldsymbol{\mathsf{t}} - \mathbf{\Phi w} \|^2
\]</span> where we subtract <span class="math inline">\(M\)</span> degrees of freedom from <span class="math inline">\(N\)</span> in the denominator. With the same interpretation, we see that the evidence maximization result for <span class="math inline">\(\beta\)</span> subtract <span class="math inline">\(\gamma\)</span> degrees of freedom from <span class="math inline">\(N\)</span>, which is copied here for easier comparison <span class="math display">\[
\frac{1}{\beta}  =  \frac{1}{N-\gamma} \left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2}
\]</span></p>
<h3 id="approximation-for-maximizing-alpha-and-beta">Approximation for Maximizing <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span></h3>
<p>Recall that the MAP solution get similar to MLE solution if we have large number of data, i.e. <span class="math inline">\(N \gg M​\)</span>. And since <span class="math inline">\(\lambda_i​\)</span> means the precision of <span class="math inline">\(\mathbf{w}_\mathrm{ML}​\)</span> along each axis, so the ratio <span class="math inline">\(\lambda_i / (\lambda_i+\alpha) \rightarrow 1​\)</span> after observing enough data (i.e. high precision). In this case, <span class="math inline">\(\gamma = M​\)</span> and we have <span class="math display">\[
\alpha=\frac{\gamma}{\mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}} \simeq \frac{M}{\mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}}
\\
\beta  =  \frac{N-\gamma}{\left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2}} \simeq \frac{N}{\left\|\boldsymbol{\mathsf{t}}-\mathbf{\Phi m}_{N}\right\|^{2}}
\]</span> these results can be used as an easy-to-compute approximation of the optimal <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, since they do not require to compute <span class="math inline">\(\lambda_i\)</span>.</p>

    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/PRML/">#PRML</a></span>
    
    </div>
    
    
    <div class="columns is-mobile is-multiline article-nav">
        <span class="column is-12-mobile is-half-desktop  article-nav-prev">
            
            <a href="/2019/04/03/PRML4-1/">(PRML Notes) 4.1 Discriminant Functions</a>
            
        </span>
        <span class="column is-12-mobile is-half-desktop  article-nav-next">
            
            <a href="/2019/03/24/PRML3-4/">(PRML Notes) 3.4 Bayesian Model Comparison</a>
            
        </span>
    </div>
    
</article>


<div class="sharebox">
    
<div class="sharethis-inline-share-buttons"></div>
<script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5c1be99859895a00110ffa34&amp;product=inline-share-buttons" async="async"></script>

</div>



<div class="comments">
    <h3 class="title is-4">Comments</h3>
    
<script>
    var disqus_config = function () {
        this.page.url = 'http://wiljohn.top/2019/03/25/PRML3-5/';
        this.page.identifier = 'prml3-5';
        
        this.language = 'en';
        
    };
    (function() {
        var d = document, s = d.createElement('script');  
        s.src = '//' + 'wiljohnhong-github-io' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<div id="disqus_thread">
    
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>

    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2019 wiljohn&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" href="https://github.com/wiljohnhong">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>


    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    


<script src="/js/script.js"></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script>
    
</body>
</html>