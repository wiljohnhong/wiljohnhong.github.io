<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
<title>(PRML Notes) 2.3.4-5 Frequentist Estimate for Gaussian - Wiljohn&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">








    <meta name="description" content="A series of notes taken from Pattern Recognition and Machine Learning.  Maximum Likelihood for Gaussian Given a dataset \(\mathbf{X} = (\mathbf{x}_1,...,\mathbf{N})^T\) where \(\mathbf{x_n}\) are dra">
<meta name="keywords" content="PRML">
<meta property="og:type" content="article">
<meta property="og:title" content="(PRML Notes) 2.3.4-5 Frequentist Estimate for Gaussian">
<meta property="og:url" content="http://wiljohn.top/2019/03/02/PRML2-3-4~5/index.html">
<meta property="og:site_name" content="Wiljohn&#39;s Blog">
<meta property="og:description" content="A series of notes taken from Pattern Recognition and Machine Learning.  Maximum Likelihood for Gaussian Given a dataset \(\mathbf{X} = (\mathbf{x}_1,...,\mathbf{N})^T\) where \(\mathbf{x_n}\) are dra">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-27/Screenshot%20from%202019-03-02%2023-30-11.png">
<meta property="og:updated_time" content="2019-03-02T15:32:33.449Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="(PRML Notes) 2.3.4-5 Frequentist Estimate for Gaussian">
<meta name="twitter:description" content="A series of notes taken from Pattern Recognition and Machine Learning.  Maximum Likelihood for Gaussian Given a dataset \(\mathbf{X} = (\mathbf{x}_1,...,\mathbf{N})^T\) where \(\mathbf{x_n}\) are dra">
<meta name="twitter:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-27/Screenshot%20from%202019-03-02%2023-30-11.png">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">

<link rel="stylesheet" href="/css/style.css">

<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    

    
    
    
    
    


</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item " href="/archives">Archives</a>
            
            <a class="navbar-item " href="/tags">Tags</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/wiljohnhong">
                
                <i class="fab-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            (PRML Notes) 2.3.4-5 Frequentist Estimate for Gaussian
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-03-02T06:20:04.000Z" itemprop="datePublished">Mar 2 2019</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            11 minutes read (About 1576 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<h2 id="maximum-likelihood-for-gaussian">Maximum Likelihood for Gaussian</h2>
<p>Given a dataset <span class="math inline">\(\mathbf{X} = (\mathbf{x}_1,...,\mathbf{N})^T\)</span> where <span class="math inline">\(\mathbf{x_n}\)</span> are drawn independently from a multivariate Gaussian. To derive MLE, the log likelihood of the dataset is given by <span class="math display">\[
\begin{align}
\ln p(\mathbf{X}|\boldsymbol{\mu},\mathbf{\Sigma}) &amp;= \ln \prod_{n=1}^{N}\frac{1}{(2\pi)^{D/2}} \frac{1}{|\mathbf{\Sigma}|^{1/2}} \exp\left\{ -\frac{1}{2} (\mathbf{x}_n- \boldsymbol{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}_n- \boldsymbol{\mu}) \right\}
\\ &amp;= -\frac{ND}{2}\ln (2\pi) - \frac{N}{2}\ln|\mathbf{\Sigma}|  - \frac{1}{2}\sum_{n=1}^N (\mathbf{x}_n- \boldsymbol{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}_n- \boldsymbol{\mu})
\end{align}
\]</span> <a id="more"></a></p>
<p>First take the derivative w.r.t. <span class="math inline">\(\boldsymbol{\mu}\)</span> <span class="math display">\[
\frac{\partial}{\partial\boldsymbol{\mu}} \ln p(\mathbf{X}|\boldsymbol{\mu},\mathbf{\Sigma}) = \sum_{n=1}^N \mathbf{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu})
\]</span> By setting it to zero we get <span class="math display">\[
\color{red}{\boldsymbol{\mu}_{\mathrm{ML}} = \frac{1}{N}\sum_{n=1}^N \mathbf{x}_n}
\]</span></p>
<blockquote>
<p>Some useful matrix derivative that will be used in deriving <span class="math inline">\(\mathbf{\Sigma}_{\mathrm{ML}}\)</span>:</p>
<p><span class="math display">\[
\frac{\partial |\mathbf{X}|}{\partial \mathbf{X}} = |\mathbf{X}| \mathbf{X}^{-T}
\\ \frac{\partial \mathbf{a}^T\mathbf{Xb}}{\partial \mathbf{X}} = \mathbf{a} \mathbf{b}^T
\\ \frac{\partial \mathbf{a}^T\mathbf{X^{-1}b}}{\partial \mathbf{X}} = -\mathbf{X}^{-T}\mathbf{a} \mathbf{b}^T\mathbf{X}^{-T}
\]</span></p>
</blockquote>
<p>Next take the derivative w.r.t. <span class="math inline">\(\mathbf{\Sigma}​\)</span> <span class="math display">\[
\frac{\partial}{\partial\mathbf{\Sigma}} \ln p(\mathbf{X}|\boldsymbol{\mu},\mathbf{\Sigma}) = - \frac{N}{2}\mathbf{\Sigma}^{-T} + \frac{1}{2} \sum_{n=1}^{N} \mathbf{\Sigma}^{-T}(\mathbf{x}_n- \boldsymbol{\mu})(\mathbf{x}_n- \boldsymbol{\mu})^T\mathbf{\Sigma}^{-T}
\]</span> By setting it to zero we get <span class="math display">\[
\begin{align}
&amp;&amp;\frac{N}{2}\mathbf{\Sigma}^{-T} &amp;= \frac{1}{2} \sum_{n=1}^{N} \mathbf{\Sigma}^{-T}(\mathbf{x}_n- \boldsymbol{\mu})(\mathbf{x}_n- \boldsymbol{\mu})^T\mathbf{\Sigma}^{-T}
\\ \Rightarrow &amp;&amp;N  &amp;= \sum_{n=1}^{N} \mathbf{\Sigma}^{-T}(\mathbf{x}_n- \boldsymbol{\mu})(\mathbf{x}_n- \boldsymbol{\mu})^T
\\ \Rightarrow &amp;&amp;\mathbf{\Sigma}^T  &amp;= \frac{1}{N}\sum_{n=1}^{N} (\mathbf{x}_n- \boldsymbol{\mu})(\mathbf{x}_n- \boldsymbol{\mu})^T = \mathbf{\Sigma}
\end{align}
\]</span> Note that the solution of <span class="math inline">\(\boldsymbol{\mu}_{\mathrm{ML}}​\)</span> is independent of <span class="math inline">\(\boldsymbol{\mathbf{\Sigma}}_{\mathrm{ML}}​\)</span>, so we can first evaluate <span class="math inline">\(\boldsymbol{\mu}_{\mathrm{ML}}​\)</span> and then use this to get <span class="math display">\[
\color{red}{\mathbf{\Sigma}_{\mathrm{ML}} = \frac{1}{N}\sum_{n=1}^{N} (\mathbf{x}_n- \boldsymbol{\mu}_{\mathrm{ML}})(\mathbf{x}_n- \boldsymbol{\mu}_{\mathrm{ML}})^T}
\]</span> Also note that we ignored the symmetric constraint on <span class="math inline">\(\mathbf{\Sigma}\)</span> during derivation while the resulting solution is indeed symmetric as required.</p>
<p>If we further evaluate the expectations of MLE results under the true distribution we can get <span class="math display">\[
\begin{align}
\mathbb{E}[\boldsymbol{\mu}_{\mathrm{ML}}] &amp;= \boldsymbol{\mu} &amp;&amp; \text{(unbiased)}
\\
\mathbb{E}[\mathbf{\Sigma}_{\mathrm{ML}}] &amp;= \frac{N-1}{N} \mathbf{\Sigma} &amp;&amp; \text{(biased)}
\end{align}
\]</span> <span class="math inline">\(\mathbb{E}[\boldsymbol{\mu}_{\mathrm{ML}}]\)</span> can be calculated straightforward. As for <span class="math inline">\(\mathbb{E}[\mathbf{\Sigma}_{\mathrm{ML}}]​\)</span> <span class="math display">\[
\begin{align}
\mathbb{E}[\mathbf{\Sigma}_{\mathrm{ML}}] &amp;= \frac{1}{N}\sum_{n=1}^{N} \mathbb{E}[(\mathbf{x}_n- \boldsymbol{\mu}_{\mathrm{ML}})(\mathbf{x}_n- \boldsymbol{\mu}_{\mathrm{ML}})^T]
\\ &amp;= \frac{1}{N}\sum_{n=1}^{N}\mathbb{E}\left[ \left(\mathbf{x}_n- \frac{1}{N}\sum_{k=1}^N \mathbf{x}_k\right)\left(\mathbf{x}_n- \frac{1}{N}\sum_{k=1}^N \mathbf{x}_k\right)^T \right]
\\ &amp;= \frac{1}{N^3}\sum_{n=1}^{N}\mathbb{E}\left[ \left(N\mathbf{x}_n-\sum_{k=1}^N \mathbf{x}_k\right)\left(N\mathbf{x}_n- \sum_{k=1}^N \mathbf{x}_k\right)^T \right]
\\ &amp;= \frac{1}{N^3}\sum_{n=1}^{N}\mathbb{E}\left[ \left((N-1)\mathbf{x}_n-\sum_{k\neq n} \mathbf{x}_k\right)\left((N-1)\mathbf{x}_n-\sum_{k\neq n} \mathbf{x}_k\right)^T \right]
\\ &amp;= \frac{1}{N^3}\sum_{n=1}^{N}\mathbb{E}\left[(N-1)^2\mathbf{x}_n\mathbf{x}_n^T  -(N-1)\sum_{k\neq n} \mathbf{x}_n\mathbf{x}_k^T -(N-1)\sum_{k\neq n} \mathbf{x}_k\mathbf{x}_n^T + \left(\sum_{k\neq n} \mathbf{x}_k\right)\left(\sum_{k\neq n} \mathbf{x}_k^T\right) \right]
\\ &amp; \ \ \ \ \scriptstyle{(\text{Recall that }\mathbb{E}[\mathbf{x}_i\mathbf{x}_j^T] = \boldsymbol{\mu}\boldsymbol{\mu}^T + \mathbf{\Sigma},\ \forall \ i\neq j, \text{ and }\mathbb{E}[\mathbf{x}_i\mathbf{x}_j^T] = \boldsymbol{\mu}\boldsymbol{\mu}^T,\ \text{if} \ i= j)}
\\ &amp;= \frac{1}{N^3}\sum_{n=1}^{N}\left[(N-1)^2(\boldsymbol{\mu}\boldsymbol{\mu}^T + \mathbf{\Sigma})  -2(N-1)^2\boldsymbol{\mu}\boldsymbol{\mu}^T + \left((N-1)^2\boldsymbol{\mu}\boldsymbol{\mu}^T + (N-1)\mathbf{\Sigma}\right)\right]
\\ &amp;= \frac{1}{N^3}\sum_{n=1}^{N}N(N-1)\mathbf{\Sigma}
\\ &amp;= \frac{N-1}{N}\mathbf{\Sigma}
\end{align}
\]</span> We can correct the bias of <span class="math inline">\(\mathbb{E}[\mathbf{\Sigma}_{\mathrm{ML}}]​\)</span> by using <span class="math inline">\(\tilde{\mathbf{\Sigma}}​\)</span> instead <span class="math display">\[
\tilde{\mathbf{\Sigma}} = \frac{1}{N-1}\sum_{n=1}^{N} (\mathbf{x}_n- \boldsymbol{\mu}_{\mathrm{ML}})(\mathbf{x}_n- \boldsymbol{\mu}_{\mathrm{ML}})^T
\]</span></p>
<h2 id="sequential-estimation">Sequential Estimation</h2>
<p>Sequential estimation allows data points to be processed one at a time and then discarded. It is important for on-line applications.</p>
<h3 id="a-naive-approach">A Naive Approach</h3>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\mu}_{\mathrm{ML}}^{(N)} &amp;= \frac{1}{N}\sum_{n=1}^N \mathbf{x}_n
\\ &amp;= \frac{1}{N}\mathbf{x}_N + \frac{1}{N}\sum_{n=1}^{N-1} \mathbf{x}_n
\\ &amp;= \frac{1}{N}\mathbf{x}_N + \frac{N-1}{N}\boldsymbol{\mu}_{\mathrm{ML}}^{(N-1)}
\\ &amp;= \boldsymbol{\mu}_{\mathrm{ML}}^{(N-1)} + \frac{1}{N}(\mathbf{x}_N -\boldsymbol{\mu}_{\mathrm{ML}}^{(N-1)})
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu}_{\mathrm{ML}}^{(N)}\)</span> is the MLE for <span class="math inline">\(\boldsymbol{\mu}_{\mathrm{ML}}\)</span> based on <span class="math inline">\(N\)</span> observations. However, we will not always be able to derive such a sequential algorithm by this route.</p>
<h3 id="robbins-monro-algorithm">Robbins-Monro Algorithm</h3>
<p>Here comes a general formulation of sequential learning. Consider a pair of random variables <span class="math inline">\(\theta\)</span> and <span class="math inline">\(z\)</span> governed by a joint distribution <span class="math inline">\(p(z, \theta)\)</span>. And a deterministic function <span class="math inline">\(f(\theta)\)</span>, which is called a <strong>regression function</strong>, is defined by <span class="math display">\[
f(\theta) \equiv \mathbb{E}[z|\theta] = \int zp(z|\theta)\ dz
\]</span> Our goal is to find the root <span class="math inline">\(\theta^*\)</span> satisfying <span class="math inline">\(f(\theta^*)=0\)</span>, i.e. <span class="math inline">\(\mathbb{E}[z|\theta^*]=0\)</span>, and we need to find a sequential estimation scheme for <span class="math inline">\(\theta^*\)</span> when the values of <span class="math inline">\(z\)</span> are observed one at a time, instead of modeling the <span class="math inline">\(f\)</span> directly then working out the root when a large dataset is obtained.</p>
<p>First we shall assume that</p>
<ul>
<li>The conditional variance of <span class="math inline">\(z\)</span> is finite: <span class="math inline">\(\mathrm{var}[z|\theta]\equiv\mathbb{E}[(z-f)|\theta]&lt;\infty\)</span></li>
<li>w.l.o.g. <span class="math inline">\(f(\theta)&gt;0\)</span> for <span class="math inline">\(\theta &gt; \theta^*\)</span> and <span class="math inline">\(f(\theta)&lt;0\)</span> for <span class="math inline">\(\theta &lt; \theta^*\)</span>, as in the following figure</li>
</ul>
<p><img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-27/Screenshot%20from%202019-03-02%2023-30-11.png"></p>
<p>Then the Robbins-Monro algorithm defines a sequence estimate of <span class="math inline">\(\theta^*\)</span> given by <span class="math display">\[
\theta^{(N)} = \theta^{(N-1)} - a_{N-1}z(\theta^{(N-1)})
\]</span> where <span class="math inline">\(z(\theta^{(N)})\)</span> is an observed value of <span class="math inline">\(z\)</span> when <span class="math inline">\(\theta\)</span> takes the value <span class="math inline">\(\theta^{(N)}\)</span>.</p>
<blockquote>
<p>Intuitively, as illustrated in the above figure, when the <span class="math inline">\(\theta^{(N-1)}\)</span> is estimated to be larger than <span class="math inline">\(\theta^*\)</span>, then the corresponding <span class="math inline">\(p(z|\theta)\)</span> will assign higher probability to those <span class="math inline">\(z(\theta^{(N-1)})&gt;0\)</span>, which results in <span class="math inline">\(\theta^{(N)}\)</span> shifting to <span class="math inline">\(\theta^*\)</span>.</p>
</blockquote>
<p>The coefficients <span class="math inline">\(\{a_N\}\)</span> represents a sequence of positive numbers that satisfy the conditions <span class="math display">\[
\begin{align}
\lim_{N\rightarrow\infty} a_N &amp;= 0
\\
\sum_{N=1}^\infty a_N &amp;= \infty
\\
\sum_{N=1}^\infty a_N^2 &amp;&lt; \infty
\end{align}
\]</span> a common example is <span class="math inline">\(a_N = 1/N\)</span>.</p>
<blockquote>
<p><span class="math inline">\(a_N\)</span> is just the same concept as <em>learning rate</em> in deep learning literal. And we will see that the upcoming <em>Robbins-Monro algorithm</em> is just like the gradient ascent method that maximizing the log likelihood.</p>
</blockquote>
<h3 id="apply-robbins-monro-algorithm-to-general-mle">Apply Robbins-Monro Algorithm to General MLE</h3>
<p>We can apply Robbins-Monro algorithm to general MLE in this form <span class="math display">\[
\theta^{(N)} = \theta^{(N-1)} - a_{N-1}\color{blue}{\frac{\partial}{\partial\theta^{(N-1)}}\left[ -\ln p(x_N|\theta^{(N-1)}) \right]}
\]</span> where we aim to find a maximum likelihood solution <span class="math inline">\(\theta_\mathrm{ML}\)</span> for <span class="math inline">\(p(x|\theta)\)</span>, which we assume to be <em>unimodal</em>. To see why, note that here we define <span class="math inline">\(z​\)</span> to be the derivative of the <em>negative</em> log likelihood</p>
<p><span class="math display">\[
z \equiv \frac{\partial}{\partial\theta}\left[ -\ln p(x|\theta)\right]
\]</span></p>
<ul>
<li>By sampling we know that now the <span class="math inline">\(\theta^*\)</span> that satisfying <span class="math inline">\(f(\theta^*)=0\)</span> is just the stationary point for MLE <span class="math display">\[
f(\theta) = \mathbb{E}[z|\theta] = \lim_{N\rightarrow\infty}\frac{1}{N} \sum_{n=1}^N \frac{\partial}{\partial\theta}\left[ -\ln p(x|\theta)\right] = \frac{\partial}{\partial\theta}\left[\lim_{N\rightarrow\infty}-\frac{1}{N} \sum_{n=1}^N  \ln p(x_n|\theta)\right]
\]</span></li>
<li>The <em>negative</em> keeps the property that <span class="math inline">\(f(\theta)&gt;0\)</span> for <span class="math inline">\(\theta &gt; \theta^*\)</span> and <span class="math inline">\(f(\theta)&lt;0\)</span> for <span class="math inline">\(\theta &lt; \theta^*\)</span></li>
</ul>
<blockquote>
<p>Intuitively, from the expression <span class="math inline">\(\theta^{(N)} = \theta^{(N-1)} + a_{N-1}\frac{\partial}{\partial\theta^{(N-1)}}\left[\ln p(x_N|\theta^{(N-1)}) \right]\)</span> we know that for every step <span class="math inline">\(\theta^{(N)}\)</span> is improved according to the gradient that maximizes <span class="math inline">\(p(x_N|\theta^{(N-1)})\)</span>.</p>
</blockquote>
<h3 id="case-study-apply-to-univariate-gaussian">Case Study: Apply to Univariate Gaussian</h3>
<p>For univariate Gaussian distribution with fixed variance, we have <span class="math display">\[
z = \frac{\partial}{\partial\mu_\mathrm{ML}}\left[ -\ln \frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{(x-\mu_\mathrm{ML})^2}{2\sigma^2} \right\}\right] = -\frac{x-\mu_\mathrm{ML}}{\sigma^2}
\]</span> then the sequential estimate rule becomes <span class="math display">\[
\theta^{(N)} = \theta^{(N-1)} + a_{N-1}\frac{x-\mu_\mathrm{ML}}{\sigma^2}
\]</span> If we choose <span class="math inline">\(a_{N-1} = \sigma^2/N\)</span>, we can obtain the same univariate form of the naive sequential estimate approach.</p>

    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/PRML/">#PRML</a></span>
    
    </div>
    
    
    <div class="columns is-mobile is-multiline article-nav">
        <span class="column is-12-mobile is-half-desktop  article-nav-prev">
            
            <a href="/2019/03/02/PRML2-3-6/">(PRML Notes) 2.3.6 Bayesian Inference for Gaussian</a>
            
        </span>
        <span class="column is-12-mobile is-half-desktop  article-nav-next">
            
            <a href="/2019/03/01/PRML2-3-1~3/">(PRML Notes) 2.3.1-3 Conditional and Marginal Gaussian</a>
            
        </span>
    </div>
    
</article>


<div class="sharebox">
    
<div class="sharethis-inline-share-buttons"></div>
<script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5c1be99859895a00110ffa34&amp;product=inline-share-buttons" async="async"></script>

</div>



<div class="comments">
    <h3 class="title is-4">Comments</h3>
    
<script>
    var disqus_config = function () {
        this.page.url = 'http://wiljohn.top/2019/03/02/PRML2-3-4~5/';
        this.page.identifier = 'prml2-3-4';
        
        this.language = 'en';
        
    };
    (function() {
        var d = document, s = d.createElement('script');  
        s.src = '//' + 'wiljohnhong-github-io' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<div id="disqus_thread">
    
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>

    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2019 wiljohn&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" href="https://github.com/wiljohnhong">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>


    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    


<script src="/js/script.js"></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script>
    
</body>
</html>