{"pages":[{"title":"(PRML Notes) 1.1 Polynomial Curve Fitting as an Introductory Example","text":"A series of notes taken from Pattern Recognition and Machine Learning. This introduction chapter is mainly composed of these three theories: Probability theory provides a framework for expressing uncertainty in a precise and quantitative manner. Decision theory allows us to exploit probabilistic representation in order to make predictions that are optimal. Information theory studies the quantification, storage, and communication of information. Example: Polynomial Curve Fitting Consider a given data set \\(\\mathbf{x} \\equiv (x_1, ..., x_N)^T\\) and with the corresponding values \\(\\mathbf{t} \\equiv (t_1, ..., t_N)^T\\), where the values are generated by \\[ t_i = \\sin(2\\pi x_i) + \\epsilon, \\ i= 1,2,...,N \\] Here \\(\\epsilon\\) is a small random noise following a Gaussian distribution. A simple approach of curve fitting is to fit the data using a polynomial function \\[ y(x, \\mathbf{w}) = w_0 + w_1x + w_2x^2 + ... + w_Mx^M = \\sum_{j=0}^M w_j x^j \\] It is not a linear function of \\(x\\), but linear in \\(\\mathbf{w}\\) by viewing \\(x^j\\) as the parameter of \\(w_j\\). The curve fitting can be done by minimizing an error function like \\[ E(\\mathbf{w} ) = \\frac{1}{2} \\sum_{n=1}^N [y(x_n ,\\mathbf{w}) - t_n]^2 \\] Since the error function is quadratic of \\(\\mathbf{w}\\), the minimization of \\(E(\\mathbf{w})\\) has a unique solution \\(\\mathbf{x}^*\\). To compare different sizes of data sets on an equal footing, it is sometimes more convenient to use the root-mean-square(RMS) error \\[ E_{RMS} = \\sqrt{2E(\\mathbf{w}^*)/N} \\] However, minimizing the error is not the same thing as getting a better model. if our model is more complex, for example if \\(M\\) is too large, we may suffer from the over-fitting problem, namely our model will perform much worse on unseen data. Over-fitting problem. Generally speaking, I think the most data generated from nature follows the law of simplicity, something like the Occam's Razor, so one should select the model that makes the fewest assumptions. This is expressed as prior knowledge later in Bayesian probability later. One approach to avoid over-fitting is regularization, adding a penalty term to the error function, for example \\[ \\widetilde{E}(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N [y(x_n ,\\mathbf{w}) - t_n]^2 + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 \\] Note that often the coefficient \\(w_0\\) is omitted from the regularization because its inclusion causes the results to depend on the choice of origin for the target variable. It may be included but with its own regularization coefficient. Perhaps under many circumstances, the data may be generated from some rule like \\(t_i = \\sin(2\\pi x_i) + b + \\epsilon\\), not simply as \\(t_i = \\sin(2\\pi x_i) + \\epsilon\\), and we should learn the natural bias instead of penalize it. Such techniques are known as shrinkage in statistics literature, the particular case of a quadratic regularizer is called ridge regression, and in the context of neural networks, this approach is known as weight decay. Selection of Models with different complexity To determine the suitable value of model complexity (either \\(M\\) or \\(\\lambda\\) here in the context), we can partition the available data into a training set, used to determine the coefficients \\(\\mathbf{w}\\), and a separate validation set, or called a hold-out set, used to optimize the model complexity.","link":"/old/PRML1-1.html"},{"title":"(PRML Notes) 1.3 Model Selection","text":"A series of notes taken from Pattern Recognition and Machine Learning. Training set is used to generate the predictive models and validation set to select the one with the best predictive performance. But if the model design is iterated many times using a limited size data set, then some over-fitting to the validation data can occur and we keep aside a third test set to evaluate the performance of the final selected model. Since separating a finite data set into these subsets is wasteful for data, we use K-fold cross-validation to assess performance. At the beginning the data is separated into groups, each time groups are used to train model and the remained one to evaluate. This procedure is repeated for all possible choices. When data is particularly scarce, we consider the case where the number of groups equals to the data points, which gives out the leave-one-out technique. 4-fold example. It's computationally expensive if \\(K\\) is large, and often we should evaluate a model under every possible hyperparameter choice, which require training runs exponential in the number of parameters. So we need to find a measure of performance which depends only on the training data and not suffers from bias due to over-fitting (so that only a single run is needed). Akaike information criterion (AIC) chooses model for which the quantity \\[ \\ln p(\\mathcal{D}|\\mathbf{w}_\\mathrm{ML}) -M \\] is largest, where \\(p(\\mathcal{D}|\\mathbf{w}_\\mathrm{ML})\\) is the best-fit likelihood, and \\(M\\) is the number of adjustable parameters in the model. Bayesian information criterion (BIC) is a variant of this quantity, will be discussed in section 4.4.1.","link":"/old/PRML1-3.html"},{"title":"(PRML Notes) 1.4 The Curse of Dimensionality","text":"A series of notes taken from Pattern Recognition and Machine Learning. Consider a naive \"cell\" solution for classification, which divides the input space into many cells and each test point is assigned to the class that has a majority number of representatives in the same cell. One problem is we need to gather exponentially large quantity of data as the number of cells grows exponentially in higher dimensions in order to ensure that the cells are not empty. Similarly, the number of coefficients in polynomial curve fitting will grow in power law as the dimension increases, making the method becomes unwieldy. For example an order-3 polynomial takes the form \\[ y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{i=1}^D w_i x_i + \\sum_{i=1}^D\\sum_{j=1}^i w_{ij}x_ix_j + \\sum_{i=1}^D\\sum_{j=1}^i\\sum_{k=1}^j w_{ijk} x_i x_j x_k \\] On geometrical intuitions, the volume of \\(D\\)-dimensional spherecis \\(V_D(r)=K_Dr^D\\), where the constant \\(K_D\\) only depends on \\(D\\), and most of the volume is concentrated in a thin shell near the surface since \\[ \\frac{V_D(1) - V_D(1-\\epsilon)}{V_D(1)} = 1-(1-\\epsilon)^D \\] For a Gaussian as well, most of the probability mass is located within a thin shell at a specific radius. Plot of the probability density of a Gaussian w.r.t. \\(r\\). So not all intuitions developed in spaces of low dimensionality will generalize to spaces of many dimensions, such difficulty is called the curse of dimensionality. However, It's not a severe problem in real application because Real data is often confined to a subspace in high dimensional space (like high dimensional image data generated from \\(3D\\) real world) Real data typically exhibit some smoothness properties, where interpolation can be applied to gain more data (the same reason for regularization being applied)","link":"/old/PRML1-4.html"},{"title":"(PRML Notes) 2.2 Multinomial Variables","text":"A series of notes taken from Pattern Recognition and Machine Learning. Multivariate Bernoulli Distribution For a random multivariate variable \\(\\mathbf{x}\\) that, for example, describes the \\(K=6\\) possible outcomes of rolling a damaged dice, each outcome is presented in an one-hot vector like \\(\\mathbf{x}=(0,0,1,0,0,0)^T\\). If we denote the probability of \\(x_k=1\\) by \\(u_k\\), then the distribution of \\(\\mathbf{x}\\) is given by \\[ p(\\mathbf{x}|\\boldsymbol{\\mu}) = \\prod_{k=1}^K \\mu_k^{x_k} \\] where \\(\\boldsymbol{\\mu}=(\\mu_1, ...,\\mu_K)^T​\\) satisfying \\(\\sum_k \\mu_k =1​\\). It can be easily verified that \\[ \\mathbb{E}[\\mathbf{x}|\\boldsymbol{\\mu}] = \\sum_\\mathbf{x} p(\\mathbf{x}|\\boldsymbol{\\mu})\\mathbf{x} =(\\mu_1, ...,\\mu_K)^T=\\boldsymbol{\\mu} \\] Maximum Likelihood Estimation Consider a dataset \\(\\mathcal{D} = \\{\\mathbf{x}_1, ...,\\mathbf{x}_N\\}\\), the corresponding likelihood function takes the form \\[ p(\\mathcal{D}|\\boldsymbol{\\mu}) = \\prod_{n=1}^N \\prod_{k=1}^K \\mu_k^{x_{nk}} = \\prod_{k=1}^K \\mu_k^{\\sum_n x_{nk}} = \\prod_{k=1}^K \\mu_k^{m_k} \\] where \\(m_k=\\sum_n x_{nk}\\) denotes the number of observations of \\(x_k=1\\), which is also the sufficient statistics for this distribution. To do MLE, we first take the log likelihood and apply Lagrange multiplier and maximizing \\[ \\sum_{k=1}^K m_k \\ln \\mu_k + \\lambda\\left( \\sum_{k=1}^K \\mu_k - 1\\right) \\] we get \\(\\mu_k^\\mathrm{ML} = m_k/N\\), which is the fraction of the \\(N\\) observations where \\(x_k = 1\\). Multinomial Distribution the multinomial distribution is the joint distribution of \\(m_1,...,m_K\\) that conditioned on the parameters \\(\\boldsymbol{\\mu}\\) and \\(N=\\sum_k m_k\\) \\[ \\mathrm{Mult}(m_1,...,m_K|\\boldsymbol{\\mu}, N) = \\binom{N}{m_1m_2... m_K}\\prod_{k=1}^K \\mu_k^{m_k} \\] Note that two-state quantities can either be represented as binary variables and modeled using the binomial distribution or as 1-of-2 variables and modeled using the multinomial distribution with \\(K = 2\\). Dirichlet Distribution The Dirichlet distribution takes the form \\[ \\mathrm{Dir}(\\boldsymbol{\\mu}|\\boldsymbol{\\alpha}) = \\frac{\\Gamma(\\alpha_0)}{\\Gamma(\\alpha_1)\\cdots\\Gamma(\\alpha_K)} \\prod_{k=1}^K \\mu_k^{\\alpha_k-1} \\] where \\(\\boldsymbol{\\alpha} = (\\alpha_0,...,\\alpha_K)^T​\\) and \\(\\alpha_0=\\sum_k \\alpha_k​\\). It is a conjugate prior for the multinomial. Note that the distribution over the space of the \\(\\{\\mu_k\\}​\\) is confined to a simplex of dimensionality \\(K=1​\\) because of the constraint \\(\\sum_k \\mu_k = 1​\\). Just like that discussed for beta distribution (with the same interpretation), the posterior for \\(\\{\\mu_k\\}\\) takes the form \\[ p(\\boldsymbol{\\mu}|\\mathcal{D},\\boldsymbol{\\alpha}) \\propto p(\\boldsymbol{\\mu}|\\mathcal{D}) p(\\boldsymbol{\\mu}|\\boldsymbol{\\alpha}) \\propto\\prod_{k=1}^K \\mu_k^{\\alpha_k+\\color{blue}{m_k}-1} \\] or we can write it in the normalized form \\[ \\mathrm{Dir}(\\boldsymbol{\\mu}|\\mathcal{D},\\boldsymbol{\\alpha}) = \\frac{\\Gamma(\\alpha_0+\\color{blue}{N})}{\\Gamma(\\alpha_1+\\color{blue}{m_1})\\cdots\\Gamma(\\alpha_K+\\color{blue}{m_K})} \\prod_{k=1}^K \\mu_k^{\\alpha_k+\\color{blue}{m_k}-1} \\] Plots of the Dirichlet distribution over three variables, where the two horizontal axes are coordinates in the plane of the simplex and the vertical axis corresponds to the value of the density. Here \\(\\alpha_k = 0.1, \\forall k\\) on the left plot, \\(\\alpha_k = 1, \\forall k\\) in the center plot, and \\(\\alpha_k = 10, \\forall k\\) in the right plot.","link":"/old/PRML2-2.html"},{"title":"(PRML Notes) 2.5 Nonparametric Methods","text":"A series of notes taken from Pattern Recognition and Machine Learning. Limitation of previous parametric approach to density modeling is that the chosen density might be a poor model of the distribution that generates data. This section will focus mainly on simple frequentist nonparametric methods. Histogram Methods Standard histogram methods simply partition \\(x\\) into distinct bins of width \\(\\Delta_i\\), and count the number of \\(n_i\\) of observations of \\(x\\) falling in bin \\(i\\). Then the probability density in each bin is given by \\[ p_{i}=\\frac{n_{i}}{N \\Delta_{i}} \\] it can be easily verified that \\(\\int p(x) \\mathrm{d} x=1\\). Illustration of the histogram approach to density estimation, where 50 data points are generated from the green curve distribution We see that \\(\\Delta​\\) acts as a smoothing parameter that for small \\(\\Delta​\\) (top figure), a lot of structure is not present in the underlying distribution, and for large \\(\\Delta​\\) (bottom figure), it is too smooth to capture the binomial property. Two advantages of this simple methods: Once the histogram has been computed, the data set itself can be discarded, also can be easily applied if the data points are arriving sequentially Can be useful for obtaining a quick visualization of data in low dimensions Two problems: The estimated density has discontinuities that are due to the bin edges Curse of dimensionality, few data in each bin in high dimensional space Despite the simplicity of this histogram approach, we can gain two insights from it: How should we define the concept of locality, since we consider the data points that lie within some local neighborhood. Can we apply other distance measures? Model complexity matters, like \\(\\Delta\\) here. General Local Density Estimate Consider \\(N​\\) observations drawn \\(p(\\mathbf{x})​\\) in \\(D​\\)-dimensional space and some small region \\(\\mathcal{R}​\\) containing \\(\\mathbf{x}​\\). The probability mass associated with this region is given by \\[ P=\\int_{\\mathcal{R}} p(\\mathbf{x}) \\mathrm{d} \\mathbf{x} \\simeq p(\\mathbf{x}) V \\] where \\(V\\) is the volume of \\(\\mathcal{R}\\), and we suppose \\(\\mathcal{R}\\) is sufficiently small that \\(p(\\mathbf{x})\\) is roughly constant over \\(\\mathcal{R}\\). The total number of \\(K\\) points lie inside \\(\\mathcal{R}\\) are distributed as follows \\[ \\operatorname{Bin}(K | N, P)=\\frac{N !}{K !(N-K) !} P^{K}(1-P)^{1-K} \\] And we know that for large \\(N\\), \\(K \\simeq N P\\). Combine them we get \\[ p(\\mathbf{x})=\\frac{K}{N V} \\] Note that the validity of this equation depends on two contradictory assumptions: \\(\\mathcal{R}​\\) be sufficiently small, so that the density is approximately constant over the region \\(\\mathcal{R}\\) be sufficiently large, so that the number \\(K​\\) of points is sufficient for the binomial distribution to be sharply peaked. The result can be exploited in two different ways: Fix \\(V\\), determine \\(K\\): the kernel approach Fix \\(K\\), determine \\(V\\): the K-nearest-neighbor Kernel Density Estimators Represent a unit cube centerd on the origin, which is an example of a kernel function, and in this context is also called a Parzen window \\[ k(\\mathbf{u})=\\left\\{\\begin{array}{ll}{1,} &amp; {\\left|u_{i}\\right| \\leqslant 1 / 2, \\quad i=1, \\ldots, D} \\\\ {0,} &amp; {\\text { otherwise }}\\end{array}\\right. \\] now we can use \\(k\\left(\\left(\\mathbf{x}-\\mathbf{x}_{n}\\right) / h\\right)\\) to represent if the data point \\(\\mathbf{X}_{n}\\) lies inside a cube of side \\(h\\) centered on \\(\\mathbf x\\). Therefore the total number of data points lying inside this cube will be \\[ K=\\sum_{n=1}^{N} k\\left(\\frac{\\mathbf{x}-\\mathbf{x}_{n}}{h}\\right) \\] Recall the result of general density estimate, where we have \\(p(\\mathbf{x})=K/NV​\\), and here \\(V=h^{D}​\\), so by substituting we have \\[ p(\\mathbf{x})=\\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{h^{D}} k\\left(\\frac{\\mathbf{x}-\\mathbf{x}_{n}}{h}\\right) \\] This approach again suffers from the discontinuity as histogram approach, so if we choose a smoother kernel function we can obtain a smoother density model. First of all, we can reinterpret the above equation, not as a single cube centered on \\(\\mathbf x\\), but as the sum of \\(N\\) cubes centered on \\(\\mathbf x_n\\). Under this view, and if we replace the cube density function instead by a Gaussian, we have \\[ p(\\mathbf{x})=\\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{\\left(2 \\pi h^{2}\\right)^{1 / 2}} \\exp \\left\\{-\\frac{\\left\\|\\mathbf{x}-\\mathbf{x}_{n}\\right\\|^{2}}{2 h^{2}}\\right\\} \\] where now \\(h\\) represents the standard deviation of the Gaussian components. Again we see that \\(h​\\) plays the role of a smoothing parameter. Illustration of the kernel density model applied to the same data set as before In general, any other kernel function can be chosen if s.t. \\[ \\begin{aligned} k(\\mathbf{u}) &amp; \\geqslant 0 \\\\ \\int k(\\mathbf{u}) \\mathrm{d} \\mathbf{u} &amp;=1 \\end{aligned} \\] Such approach has great merit that there is no computation involved in the ‘training’ phase, but great weaknesses because the computational cost of evaluating the density grows linearly with the size of the data set. Nearest-Neighbor Methods In kernel density estimators, the optimal choice for \\(h\\) may depend on location within the data space, since a large value of \\(h\\) may lead to over-smoothing so should be used in sparse area, and a small \\(h\\) may lead to noisy estimates so should be used in dense area. However actually \\(h\\) is fixed for all kernels. KNN addresses this problem. The key idea of KNN is to allow the radius of the sphere to grow until it contains precisely \\(K\\) data points, so the '\\(h\\)' now is large in sparse area and small in dense area. Note that the model produced by KNN is not a true density model because the integral over all space diverges. Illustration of K-nearest-neighbor density estimation using the same data set, and see that it's not a true density since it goes to infinity at \\(x_n\\) KNN can be extended to classification problems if we apply it to each class separately and then make use of Bayes’ theorem. Suppose a data set with \\(N_k\\) points in each class \\(\\mathcal{C}_k\\) and \\(\\sum_{k} N_{k}=N\\). Then we draw a sphere centered on \\(\\mathbf x\\) containing precisely \\(K\\) points irrespective of their class, and suppose this sphere has volume \\(V\\) with \\(K_k\\) points for each \\(\\mathcal{C}_k\\). Then we have \\[ \\begin{align} p(\\mathbf{x} | \\mathcal{C}_{k}) &amp;=\\frac{K_{k}}{N_{k} V} \\\\ p(\\mathbf{x})&amp;=\\frac{K}{N V} \\\\ p\\left(\\mathcal{C}_{k}\\right) &amp;=\\frac{N_{k}}{N} \\end{align} \\] So we have \\[ p\\left(\\mathcal{C}_{k} | \\mathbf{x}\\right)=\\frac{p(\\mathbf{x} | \\mathcal{C}_{k}) p\\left(\\mathcal{C}_{k}\\right)}{p(\\mathbf{x})}=\\frac{K_{k}}{K} \\] (a) Classify a point according to 3-NN (b) Decision boundary of 1-NN If we wish to minimize the probability of misclassification, we should assign \\(\\mathbf x\\) to the class with largest \\(K_k/K\\). Again, we see that \\(K\\) control the degree of smoothing. Plot of the result of KNN of 200 data points from the oil data Note that KNN can be solved by constructing tree-based search structures to allow (approximate) near neighbors to be found efficiently without doing an exhaustive search of the data set.","link":"/old/PRML2-5.html"},{"title":"(PRML Notes) 3.2 The Bias-Variance Decomposition","text":"A series of notes taken from Pattern Recognition and Machine Learning. In the last section we remain a question about how to determine the regularization parameter \\(\\lambda\\), so that the model can be applied better to new data set. Seeking solution that minimizes both \\(\\mathbf{w}\\) and \\(\\lambda\\) is not right approach since this leads to unregularized solution \\(\\lambda=0\\). This section focuses on the frequentist view of the model complexity issue, known as bias-variance trade-off. Recall of Decision Theory Recall that in section 1.5, the optimal prediction of regression problem under a squared loss function is given by the conditional mean, here we denote it by \\(h(\\mathbf{x})​\\) \\[ h(\\mathbf{x})=\\mathbb{E}[t | \\mathbf{x}]=\\int t p(t | \\mathbf{x}) \\mathrm{d} t \\] Difference between squared loss and sum-of-square error: ​ The squared loss arises form decision theory and is used to give prediction, given the distribution of \\(p(t | \\mathbf{x})\\). While the sum-of-square error arises from MLE of model parameters, which is used to determine \\(p(t | \\mathbf{x})\\). And we saw that the expected squared loss can be written as \\[ \\begin{align} \\mathbb{E}_{\\mathbf{x}, t}[L] &amp;= \\int\\int\\{y(\\mathbf{x})-t\\}^{2} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t \\\\ &amp;= \\int\\int\\{y(\\mathbf{x}) - h(\\mathbf{x}) + h(\\mathbf{x})-t\\}^{2} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t \\\\ &amp;= \\int\\int\\{y(\\mathbf{x})-h(\\mathbf{x})\\}^{2} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t + \\int\\int\\{h(\\mathbf{x})-t\\}^{2} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t + \\int\\int2\\{y(\\mathbf{x})-h(\\mathbf{x})\\}\\{h(\\mathbf{x})-t\\} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t \\\\ &amp;=\\int\\{y(\\mathbf{x})-h(\\mathbf{x})\\}^{2} p(\\mathbf{x}) \\mathrm{d} \\mathbf{x}+\\int\\int\\{h(\\mathbf{x})-t\\}^{2} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t \\quad\\ \\scriptstyle{(\\text{The last term vanishes since } \\int \\{h(\\mathbf{x})-t\\} p(t|\\mathbf{x}) \\mathrm{d} t=0 )} \\end{align} \\] where the second term arises from the intrinsic noise and is irreducible, and the first term depend on the choice of \\(f(\\mathbf{x})\\). If we have unlimited supply of data and \\(f(\\mathbf{x})​\\) is set to enough degrees of freedom, then the first term can be reduced to zero. Bayesian vs Frequentist If \\(y(\\mathbf{x})​\\) is modeled via a parametric approach as \\(y(\\mathbf{x}, \\mathbf{w})​\\), then from a Bayesian approach the uncertainty is expressed through a posterior distribution over \\(\\mathbf{w}​\\). While a frequentist treatment, involves making a point estimate of \\(\\mathbf{w}​\\), based on the given data set \\(\\mathcal{D}​\\). We denote the \\(y(\\mathbf{x})​\\) modeled in this way as \\(y(\\mathbf{x}; \\mathcal{D})​\\). The performance of a particular learning algorithm is then assessed by taking the average over \\(y(\\mathbf{x}; \\mathcal{D})​\\) on different data sets. Frequentist Model Comparison Frequentists further define the expected prediction loss over different data sets as \\[ \\begin{align} \\mathbb{E}_\\mathcal{D}[\\mathbb{E}_{\\mathbf{x}, t}[L]] &amp; = \\mathbb{E}_\\mathcal{D}\\left[\\int\\{y(\\mathbf{x}; \\mathcal{D})-h(\\mathbf{x})\\}^{2} p(\\mathbf{x}) \\mathrm{d} \\mathbf{x}+\\int\\int\\{h(\\mathbf{x})-t\\}^{2} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t\\right] \\\\&amp; = \\int\\mathbb{E}_\\mathcal{D}\\left[\\{y(\\mathbf{x; \\mathcal{D}})-h(\\mathbf{x})\\}^{2}\\right] p(\\mathbf{x}) \\mathrm{d} \\mathbf{x}+\\int\\int\\{h(\\mathbf{x})-t\\}^{2} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t \\\\&amp; = \\int\\mathbb{E}_\\mathcal{D}\\left[\\left\\{y(\\mathbf{x} ; \\mathcal{D})-\\mathbb{E}_{\\mathcal{D}}[y(\\mathbf{x} ; \\mathcal{D})]+\\mathbb{E}_{\\mathcal{D}}[y(\\mathbf{x} ; \\mathcal{D})]-h(\\mathbf{x})\\right\\}^{2}\\right] p(\\mathbf{x}) \\mathrm{d} \\mathbf{x}+\\int\\int\\{h(\\mathbf{x})-t\\}^{2} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t \\\\&amp; = \\int\\mathbb{E}_\\mathcal{D}\\left[{\\left\\{y(\\mathbf{x} ; \\mathcal{D})-\\mathbb{E}_{\\mathcal{D}}[y(\\mathbf{x} ; \\mathcal{D})]\\right\\}^{2}+\\left\\{\\mathbb{E}_{\\mathcal{D}}[y(\\mathbf{x} ; \\mathcal{D})]-h(\\mathbf{x})\\right\\}^{2}} {+2\\left\\{y(\\mathbf{x} ; \\mathcal{D})-\\mathbb{E}_{\\mathcal{D}}[y(\\mathbf{x} ; \\mathcal{D})]\\right\\}\\left\\{\\mathbb{E}_{\\mathcal{D}}[y(\\mathbf{x} ; \\mathcal{D})]-h(\\mathbf{x})\\right\\}}\\right] p(\\mathbf{x}) \\mathrm{d} \\mathbf{x}+\\int\\int\\{h(\\mathbf{x})-t\\}^{2} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t \\\\&amp; = \\underbrace{\\int\\mathbb{E}_\\mathcal{D}\\left[\\left\\{y(\\mathbf{x} ; \\mathcal{D})-\\mathbb{E}_{\\mathcal{D}}[y(\\mathbf{x} ; \\mathcal{D})]\\right\\}^{2}\\right]p(\\mathbf{x}) \\mathrm{d} \\mathbf{x}}_\\text{variance} + \\underbrace{\\int\\left\\{\\mathbb{E}_{\\mathcal{D}}[y(\\mathbf{x} ; \\mathcal{D})]-h(\\mathbf{x})\\right\\}^{2}p(\\mathbf{x}) \\mathrm{d} \\mathbf{x}}_{\\text{(bias)}^2}+ \\underbrace{\\int\\int\\{h(\\mathbf{x})-t\\}^{2} p(\\mathbf{x}, t) \\mathrm{d} \\mathbf{x} \\mathrm{d} t}_\\text{noise} \\end{align} \\] where the bias represents the extent to which the average prediction over all data sets differs from the desired regression function, and the variance measures the extent to which the solutions for individual data sets vary around their average, i.e. how sensitive \\(y(\\mathbf{x}; \\mathcal{D})\\) is to the particular choice of \\(\\mathcal{D}\\). And the noise remains the same as before. There is a trade-off between bias and variance, with very flexible models having low bias and high variance, and relatively rigid models having high bias and low variance. The optimal model leads to the best balance between bias and variance. To take an intuitive understand of this trade-off, we can examine the bias and variance quantitatively by sampling \\[ \\begin{aligned}(\\text {bias})^{2} &amp;=\\frac{1}{N} \\sum_{n=1}^{N}\\left\\{\\overline{y}\\left(x_{n}\\right)-h\\left(x_{n}\\right)\\right\\}^{2} \\\\ \\text { variance } &amp;=\\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{L} \\sum_{l=1}^{L}\\left\\{y^{(l)}\\left(x_{n}\\right)-\\overline{y}\\left(x_{n}\\right)\\right\\}^{2} \\end{aligned} \\] where we have \\(L\\) data sets, each having \\(N\\) data points, and \\(\\overline{y}(x_n)\\) and the averaged prediction of \\(x_n\\) on different models given by \\[ \\overline{y}(x_n)=\\frac{1}{L} \\sum_{l=1}^{L} y^{(l)}(x_n) \\] The following illustration shows the results of \\(L=100\\) data sets generated from \\(h(x)=\\sin(2\\pi x)\\), each data set has \\(N=25\\) data points and modeled with \\(24​\\) Gaussian basis functions. Left shows 20 fitted curves for 20 i.i.d. drawn \\(\\mathcal{D}\\)s, presents the variance, and the right shows the bias, that for more complex model (smaller \\(\\lambda\\), the averaged curve (red line) is closer to \\(h(x)\\) (green line). Plot of squared bias and variance, together with their sum. A last, the author as a Bayesian criticizes that frequentist perspective is of limited practical value since we only have a single data set. (Partitioning it into smaller data sets for validation wastes data).","link":"/old/PRML3-2.html"},{"title":"(PRML Notes) 3.4 Bayesian Model Comparison","text":"A series of notes taken from Pattern Recognition and Machine Learning. Here we consider the problem of model selection from a Bayesian perspective, which avoid the over-fitting of making point estimates of model parameters by marginalizing over these parameters. Through this way, models can be compared directly on the training data. General Procedure of Bayesian Model Selection Suppose we wish to compare \\(L\\) models \\(\\left\\{\\mathcal{M}_{i}\\right\\}\\), where \\(i=1, \\dots, L\\), and in each model we defined a prior over model parameters, \\(p(\\mathbf{w} | \\mathcal{M}_{i})\\), and for each choice of parameters in each model, the uncertainty over the observed data is described in a likelihood \\(p(\\mathcal{D} | \\mathbf{w}, \\mathcal{M}_{i})\\). And we also suppose that data is generated from one of these models, but we are just uncertain about which one. So the posterior after observing the data for each model is given by \\[ p\\left(\\mathcal{M}_{i} | \\mathcal{D}\\right) \\propto p\\left(\\mathcal{M}_{i}\\right) p(\\mathcal{D} | \\mathcal{M}_{i}) = p\\left(\\mathcal{M}_{i}\\right) \\int p(\\mathcal{D} | \\mathbf{w}, \\mathcal{M}_{i}) p(\\mathbf{w} | \\mathcal{M}_{i}) \\mathrm{d} \\mathbf{w} \\] If all models are given equal prior probability, i.e. \\(p\\left(\\mathcal{M}_{i}\\right)\\) is the same for all \\(i\\), then we only care about \\(p(\\mathcal{D} | \\mathcal{M}_{i})\\), called model evidence, which expresses the preference shown by data for different models. The ratio of model evidences \\(p(\\mathcal{D} | \\mathcal{M}_{i}) / p(\\mathcal{D} | \\mathcal{M}_{j})\\) is called Bayes factor. Once we know the posterior over models, and if we choose to average the predictive distributions \\(p(t | \\mathbf{x}, \\mathcal{M}_{i}, \\mathcal{D})\\), then the prediction is given by \\[ p(t | \\mathbf{x}, \\mathcal{D})=\\sum_{i=1}^{L} p(t | \\mathbf{x}, \\mathcal{M}_{i}, \\mathcal{D}) p\\left(\\mathcal{M}_{i} | \\mathcal{D}\\right) \\] or simply we can just do model selection, which use the single most probable model alone to make predictions. Interpretation from Internal View We can obtain some insight into model evidence by making a simple approximation to the integral over parameters. Consider the case of a model with a prior \\(p(\\mathbf{w} | \\mathcal{M}_{k})\\) distributed uniformly as an \\(M\\)-dimensional hypercube with length \\(\\Delta w_{\\text {prior}}\\) in an \\(M\\)-dimensional space, and assume the posterior is sharply peaked around the most probable value \\(\\mathbf{w}_\\mathrm{MAP}\\), within the hypercube with length \\(\\Delta w_{\\text {posterior}}\\), then we have the approximation \\[ p(\\mathcal{D}|\\mathcal{M}_{k})=\\int p(\\mathcal{D} | \\mathbf{w},\\mathcal{M}_{k}) p(\\mathbf{w}|\\mathcal{M}_{k}) \\mathrm{d} \\mathbf{w} \\simeq p(\\mathcal{D} | \\mathbf{w}_{\\mathrm{MAP}},\\mathcal{M}_{k}) \\left(\\frac{1}{\\Delta \\mathbf{w}_{\\mathrm{prior}}}\\right)^M (\\Delta \\mathbf{w}_{\\mathrm{posterior}})^M \\] consider the logarithm \\[ \\ln p(\\mathcal{D}|\\mathcal{M}_{k}) \\simeq \\ln p(\\mathcal{D} | \\mathbf{w}_{\\mathrm{MAP}},\\mathcal{M}_{k})+M \\ln \\left(\\frac{\\Delta w_{\\mathrm{posterior}}}{\\Delta w_{\\mathrm{prior}}}\\right) \\] The first term represents the fit to the data given by the most probable parameter values, and the second term penalizes the model according to its complexity. A rough approximation to the model evidence for the case \\(M=1\\). Since complex models are more finely tuned to the data, resulting in a high likelihood but small \\(\\Delta w_{\\mathrm{posterior}} /\\Delta w_{\\mathrm{prior}}​\\) ratio, while simple models are under-fit to the data but not still sure on the choice the parameter after observing data, resulting in a low likelihood but high \\(\\Delta w_{\\mathrm{posterior}} /\\Delta w_{\\mathrm{prior}}​\\) ratio. So the optimal model complexity, as determined by model evidence, will be given by a trade-off between these two competing terms. In Bayesian point of view, a good model should have a proper range of parameters that can fit the data well. Interpretation from External View This time from another point of view, consider running three models \\(\\mathcal{M}_{1}, \\mathcal{M}_{2}\\) and \\(\\mathcal{M}_{3}\\) generatively to produce example data sets, and then looking at the distribution of data sets that result. A simple model like \\(\\mathcal{M}_{1}​\\) has little variability so will generate data very similar to each other. And for complex model like \\(\\mathcal{M}_{3}​\\), it can generate a variety of different data sets. Because the distributions \\(p(\\mathcal{D} | \\mathcal{M}_{i})\\) are normalized, so for a particular data set \\(\\mathcal{D}_0\\), maybe \\(\\mathcal{M}_{1}\\) can not fit it well, while \\(\\mathcal{M}_{3}\\) spreads its predictive probability over too broad a range so assigns \\(\\mathcal{D}_0\\) with a relatively small probability, then only the model \\(\\mathcal{M}_{2}​\\) of intermediate complexity has the highest value of evidence. Illustration of the distribution of data sets for three models of different complexity Relation to KL Divergence Recall that we have made an assumption that data is generated from one of these models under consideration. We can show that Bayesian model comparison will on average favor the correct model. Consider two models \\(\\mathcal{M}_{1}\\) and \\(\\mathcal{M}_{2}\\), where the data actually generated from \\(\\mathcal{M}_1\\). The expected Bayes factor over the distribution of data sets is given by \\[ \\int p(\\mathcal{D} | \\mathcal{M}_{1}) \\ln \\frac{p(\\mathcal{D} | \\mathcal{M}_{1})}{p(\\mathcal{D} | \\mathcal{M}_{2})} \\mathrm{d} \\mathcal{D} = \\mathrm{KL}(p(\\mathcal{D} | \\mathcal{M}_{1}) \\| p(\\mathcal{D} | \\mathcal{M}_{2})) \\geq 0 \\] However, for a particular finite data set, it is possible for the Bayes factor to be larger for the incorrect model. Limitation Model evidence can be sensitive to many aspects of the prior, as we see the choice of prior plays an important role in the ratio \\(\\Delta w_{\\mathrm{posterior}} /\\Delta w_{\\mathrm{prior}}\\). And note that the evidence will be undefined if we take an improper prior. And if we consider a proper prior and take a suitable limit in order to obtain an approximate improper prior (e.g. infinite variance for Gaussian prior), then the evidence will go to zero. But if we really want to use such prior, it may be possible to consider the evidence ratio between two models first and then take a limit to obtain a meaningful answer. In practice, it will be wise to keep aside an independent test set to do evaluation.","link":"/old/PRML3-4.html"},{"title":"(PRML Notes) 4.4 The Laplace Approximation","text":"A series of notes taken from Pattern Recognition and Machine Learning. For a Bayesian treatment of logistic regression, it is more complex than that of linear regression discuss before, since we cannot integrate analytically over \\(\\mathbf{w}\\) due to the posterior is no longer Gaussian. Instead, we should introduce some form of approximation. Laplace Approximation Univariate Approximation Consider first the univariate case, the Laplace approximation aims to find a local Gaussian approximation \\(q(z)\\) to a probability density \\(p(z)\\) defined over a set of continuous variables. Suppose \\(p(z)​\\) is defined from any function with finite integral \\[ p(z)=\\frac{1}{Z} f(z), \\text{ where } Z=\\int f(z) \\mathrm{d} z \\] The first step of Laplace approximation is to find a mode of \\(p(z)\\), i.e. a point \\(z_0\\) such that \\(p&#39;(z_0)=0\\). Next, since Gaussian has the property that its logarithm is quadratic, to do approximation we can consider a Taylor expansion of \\(\\ln f(z)\\) centered on \\(z_0\\) \\[ \\begin{align} \\ln f(z) &amp;\\simeq \\ln f\\left(z_0\\right) + \\left. \\frac{\\mathrm{d}}{\\mathrm{d}z}\\ln f\\left(z\\right)\\right|_{z=z_0}(z-z_{0}) +\\frac{1}{2}\\left. \\frac{\\mathrm{d}^2}{\\mathrm{d}z^2}\\ln f\\left(z\\right)\\right|_{z=z_0}\\left(z-z_{0}\\right)^{2} \\\\ &amp;= \\ln f\\left(z_0\\right) +\\frac{1}{2}\\left. \\frac{\\mathrm{d}^2}{\\mathrm{d}z^2}\\ln f\\left(z\\right)\\right|_{z=z_0}\\left(z-z_{0}\\right)^{2} &amp;&amp; \\scriptstyle{(\\text{Note that at mode we have zero-value first-order term.})} \\\\ &amp; \\equiv \\ln f\\left(z_0\\right) -\\frac{1}{2}A\\left(z-z_{0}\\right)^{2} &amp;&amp; \\scriptstyle{(\\text{Where we have defined } A=-\\left. \\frac{\\mathrm{d}^2}{\\mathrm{d}z^2}\\ln f\\left(z\\right)\\right|_{z=z_0})} \\end{align} \\] Then take the exponential we have \\[ f(z) \\simeq f\\left(z_{0}\\right) \\exp \\left\\{-\\frac{A}{2}\\left(z-z_{0}\\right)^{2}\\right\\} \\] Then compute the normalization term of the approximated distribution \\[ Z_q=\\int f\\left(z_{0}\\right) \\exp \\left\\{-\\frac{A}{2}\\left(z-z_{0}\\right)^{2}\\right\\} \\mathrm{d} z = f(z_0) \\left(\\frac{2\\pi}{A}\\right)^{1/2} \\] Now we can obtain the normalized approximated distribution \\(q(z)\\) \\[ \\begin{align} q(z)&amp;=\\frac{1}{Z_q} f\\left(z_{0}\\right) \\exp \\left\\{-\\frac{A}{2}\\left(z-z_{0}\\right)^{2}\\right\\} \\\\ &amp;= \\left(\\frac{A}{2 \\pi}\\right)^{1 / 2} \\exp \\left\\{-\\frac{A}{2}\\left(z-z_{0}\\right)^{2}\\right\\} \\end{align} \\] By this approach, only when the stationary point \\(z_0\\) is a local maximum, i.e. when \\(A&gt;0\\), can the approximation be well defined. Illustration of the Laplace approximation applied to the distribution \\(p(z) \\propto \\exp \\left(-z^{2} / 2\\right) \\text{sigmoid}(20 z+4)\\). Multivariate Approximation To approximate an \\(M\\)-dimensional distribution \\(p(\\mathbf{z})=f(\\mathbf{z}) / Z\\), again we expand it around a stationary point \\(\\mathbf{z}_0\\) \\[ \\ln f(\\mathbf{z}) \\simeq \\ln f\\left(\\mathbf{z}_{0}\\right)-\\frac{1}{2}\\left(\\mathbf{z}-\\mathbf{z}_{0}\\right)^{\\mathrm{T}} \\mathbf{A}\\left(\\mathbf{z}-\\mathbf{z}_{0}\\right) \\] where \\(\\mathbf{A}=-\\nabla^2 \\ln f (\\mathbf{z}_0)\\). Again take exponential of both sides we obtain \\[ f(\\mathbf{z}) \\simeq f\\left(\\mathbf{z}_{0}\\right) \\exp \\left\\{-\\frac{1}{2}\\left(\\mathbf{z}-\\mathbf{z}_{0}\\right)^{\\mathrm{T}} \\mathbf{A}\\left(\\mathbf{z}-\\mathbf{z}_{0}\\right)\\right\\} \\] After normalization we have \\[ \\begin{align} Z_q &amp;= \\int f\\left(\\mathbf{z}_{0}\\right) \\exp \\left\\{-\\frac{1}{2}\\left(\\mathbf{z}-\\mathbf{z}_{0}\\right)^{\\mathrm{T}} \\mathbf{A}\\left(\\mathbf{z}-\\mathbf{z}_{0}\\right)\\right\\} \\mathrm{d} \\mathbf{z} \\\\ &amp;=f\\left(\\mathbf{z}_{0}\\right) \\frac{(2 \\pi)^{M / 2}}{|\\mathbf{A}|^{1 / 2}} \\tag{1} \\end{align} \\] Then the approximated distribution is given by \\[ q(\\mathbf{z})=\\frac{|\\mathbf{A}|^{1 / 2}}{(2 \\pi)^{M / 2}} \\exp \\left\\{-\\frac{1}{2}\\left(\\mathbf{z}-\\mathbf{z}_{0}\\right)^{\\mathrm{T}} \\mathbf{A}\\left(\\mathbf{z}-\\mathbf{z}_{0}\\right)\\right\\}=\\mathcal{N}\\left(\\mathbf{z} | \\mathbf{z}_{0}, \\mathbf{A}^{-1}\\right) \\] Some Analysis Many distributions are multimodal, so there will be various Laplace approximation according to different mode. Laplace approximation is most useful when number of observed points are large, since the posterior are more like Gaussian according to central limit theorem. Laplace approximation is only applicable to real variables, so for variable like \\(0 \\leqslant \\tau&lt;\\infty\\) we can consider an approximation of transformation of the variable like \\(\\ln\\tau\\). The most serious limitation is that it is based purely on the aspects of the true distribution at a specific local value of the variable, and so can fail to capture important global properties. Model Comparison and BIC The equation \\((1)\\), which is the approximation to the normalization constant of \\(f(\\mathbf{z})\\) \\[ Z \\simeq f\\left(\\mathbf{z}_{0}\\right) \\frac{(2 \\pi)^{M / 2}}{|\\mathbf{A}|^{1 / 2}} \\] can be used to obtain an approximation to the model evidence. Recall that the model evidence is given by \\[ p(\\mathcal{D} | \\mathcal{M}_{i}) = \\int p(\\mathcal{D} | \\mathbf{w}, \\mathcal{M}_{i}) p(\\mathbf{w} | \\mathcal{M}_{i}) \\mathrm{d} \\mathbf{w} \\] Let us omit the conditioning on \\(\\mathcal{M}_i​\\) to keep unclustered \\[ p(\\mathcal{D})=\\int p(\\mathcal{D} | \\mathbf{w}) p(\\mathbf{w}) \\mathrm{d} \\mathbf{w} \\] We can define \\(f(\\mathbf{w}) = p(\\mathcal{D} | \\mathbf{w}) p(\\mathbf{w})​\\), and \\(Z = p(\\mathcal{D})​\\), then we can obtain the Laplace approximation to this posterior as \\[ \\begin{align} \\ln p(\\mathcal{D}) &amp;= \\ln \\int f(\\mathbf{w}) \\mathrm{d} \\mathbf{w} \\\\ &amp;\\simeq \\ln \\left( f\\left(\\mathbf{w}_{\\mathrm{MAP}}\\right)\\frac{(2 \\pi)^{M / 2}}{|\\mathbf{A}|^{1 / 2}} \\right) \\\\ &amp;\\simeq \\ln \\left( p(\\mathcal{D} | \\mathbf{w}_{\\mathrm{MAP}}) p(\\mathbf{w}_{\\mathrm{MAP}})\\frac{(2 \\pi)^{M / 2}}{|\\mathbf{A}|^{1 / 2}} \\right) \\\\&amp;=\\ln p\\left(\\mathcal{D} | \\mathbf{w}_{\\mathrm{MAP}}\\right)+\\underbrace{\\ln p\\left(\\mathbf{w}_{\\mathrm{MAP}}\\right)+\\frac{M}{2} \\ln (2 \\pi)-\\frac{1}{2} \\ln |\\mathbf{A}|}_{\\text { Occam factor }} \\tag{2} \\end{align} \\] Here the Occam factor consists of terms that penalizes model complexity, and in this case \\[ \\mathbf{A}=-\\nabla_{\\mathbf{w}}^2f(\\mathbf{w}) =-\\nabla^2_{\\mathbf{w}} \\ln p\\left(\\mathcal{D} | \\mathbf{w}_{\\mathrm{MAP}}\\right) p\\left(\\mathbf{w}_{\\mathrm{MAP}}\\right)=-\\nabla^2_{\\mathbf{w}} \\ln p\\left(\\mathbf{w}_{\\mathrm{MAP}} | \\mathcal{D}\\right) \\] \\(|\\mathbf{A}|\\) would be large if \\(p\\left(\\mathbf{w} | \\mathcal{D}\\right)\\) is sharply peaked at \\(\\mathbf{w}_{\\mathrm{MAP}}\\) as more data observed. If we assume the prior \\(p(\\mathbf{w})\\) is flat and broad, and \\(\\mathbf{A}\\) is full rank, then we can approximate \\((2)\\) very roughly by \\[ \\ln p(\\mathcal{D}) \\simeq\\ln p\\left(\\mathcal{D} | \\color{red}{\\mathbf{w}_{\\mathrm{MAP}}}\\right) - \\frac{1}{2}M\\ln N \\] where \\(N​\\) is the number of data points and \\(M​\\) the adjustable parameters in \\(\\mathbf{w}​\\). This result is called Bayesian Information Criterion (BIC). Compare to AIC: \\(\\ln p(\\mathcal{D}|{\\color{red}\\mathbf{w}_{\\mathrm{ML}} }) -M\\), BIC penalizes model complexity more heavily. Both of them are easy toe valuate, but can also give misleading results in practice.","link":"/old/PRML4-4.html"},{"title":"(PRML Notes) 4.5 Bayesian Logistic Regression","text":"A series of notes taken from Pattern Recognition and Machine Learning. This section turns to a Bayesian treatment of logistic regression, as mentioned in the last section, we should apply the Laplace approximation. Laplace Approximation First we introduce a prior \\(p(\\mathbf{w})=\\mathcal{N}\\left(\\mathbf{w} | \\mathbf{m}_{0}, \\mathbf{S}_{0}\\right)\\), where \\(\\mathbf{m}_{0}\\) and \\(\\mathbf{S}_{0}\\) are fixed hyperparameters. And recall that the likelihood of logistic regression is given by \\[ p(\\boldsymbol{\\mathsf{t}} | \\mathbf{w}) = \\prod_{n=1}^{N} y_{n}^{t_{n}}\\left\\{1-y_{n}\\right\\}^{1-t_{n}} \\] where \\(y_{n}=\\sigma\\left(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}_{n}\\right)\\). The posterior is given by \\(p(\\mathbf{w} | \\boldsymbol{\\mathsf{t}}) \\propto p(\\mathbf{w}) p(\\boldsymbol{\\mathsf{t}} | \\mathbf{w})\\), and take log of both sides we have \\[ \\begin{aligned} \\ln p(\\mathbf{w} | \\boldsymbol{\\mathsf{t}})=&amp;-\\frac{1}{2}\\left(\\mathbf{w}-\\mathbf{m}_{0}\\right)^{\\mathrm{T}} \\mathbf{S}_{0}^{-1}\\left(\\mathbf{w}-\\mathbf{m}_{0}\\right) \\\\ &amp;+\\sum_{n=1}^{N}\\left\\{t_{n} \\ln y_{n}+\\left(1-t_{n}\\right) \\ln \\left(1-y_{n}\\right)\\right\\}+\\text { const } \\end{aligned} \\] Use the previous results in 4.3.3, we can find the Hessian matrix \\[ \\mathbf{S}_{N}^{-1}=-\\nabla^2_\\mathbf{w} \\ln p(\\mathbf{w} | \\boldsymbol{\\mathsf{t}})=\\mathbf{S}_{0}^{-1}+ \\boldsymbol{\\Phi}^\\mathrm{T} \\mathbf{R}\\boldsymbol{\\Phi} \\] Then the Laplace approximation to the posterior is given by \\[ q(\\mathbf{w})=\\mathcal{N}\\left(\\mathbf{w} | \\mathbf{w}_{\\mathrm{MAP}}, \\mathbf{S}_{N}\\right) \\] Predictive Distribution The remained task is now to marginalizing w.r.t. \\(\\mathbf{w}​\\) to make predictions for a new point \\(\\boldsymbol{\\phi}​\\). In the two classes case we have \\[ p\\left(\\mathcal{C}_{1} | \\boldsymbol{\\phi}, \\boldsymbol{\\mathsf{t}}\\right)=\\int p\\left(\\mathcal{C}_{1} | \\boldsymbol{\\phi}, \\mathbf{w}\\right) p(\\mathbf{w} | \\boldsymbol{\\mathsf{t}}) \\mathrm{d} \\mathbf{w} \\simeq \\int \\sigma\\left(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right) q(\\mathbf{w}) \\mathrm{d} \\mathbf{w} \\] Now to evaluate this expression, we will introduce the Dirac delta function \\(\\delta\\) \\[ \\sigma\\left(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right)=\\int \\delta\\left(a-\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right) \\sigma(a) \\mathrm{d} a \\] To my intuitive understanding, the Dirac delta function is used as a variable changing method for high dimension integral. In this case, \\(a=\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}​\\). From this we obtain \\[ \\begin{align} p\\left(\\mathcal{C}_{1} | \\boldsymbol{\\phi}, \\boldsymbol{\\mathsf{t}}\\right)&amp;\\simeq \\int \\sigma\\left(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right) q(\\mathbf{w}) \\mathrm{d} \\mathbf{w} \\\\ &amp;= \\int\\int \\delta\\left(a-\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right) q(\\mathbf{w}) \\mathrm{d} \\mathbf{w} \\ \\sigma(a)\\mathrm{d} a \\\\ &amp;\\equiv \\int p(a)\\sigma(a)\\mathrm{d} a \\end{align} \\] where we have defined \\[ p(a)=\\int \\delta\\left(a-\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right) q(\\mathbf{w}) \\mathrm{d} \\mathbf{w} \\] \\(p(a)\\) can be easily evaluated by noting that it is a Gaussian, since the delta function imposes a linear constraint on \\(\\mathbf{w}\\), i.e. \\(a=\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\). So we can evaluate the form \\(p(a)\\) by computing the mean \\[ \\begin{align} \\mu_{a}&amp;=\\int p(a) a \\mathrm{d} a \\\\ &amp;=\\int \\int \\delta\\left(a-\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right) q(\\mathbf{w}) \\mathrm{d} \\mathbf{w}\\ a \\mathrm{d} a \\\\ &amp;=\\int \\int \\delta\\left(a-\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right)a \\mathrm{d} a \\ q(\\mathbf{w}) \\mathrm{d} \\mathbf{w} \\\\ &amp;=\\int \\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\ q(\\mathbf{w}) \\mathrm{d} \\mathbf{w} \\\\&amp;=\\mathbf{w}_{\\mathrm{MAP}}^{\\mathrm{T}} \\boldsymbol{\\phi} \\end{align} \\] and the variance \\[ \\begin{align} \\sigma_{a}^{2} &amp;=\\int p(a)\\left\\{a^{2}-\\mathbb{E}[a]^{2}\\right\\} \\mathrm{d} a \\\\ &amp;= \\int\\int \\delta\\left(a-\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right) q(\\mathbf{w}) \\mathrm{d} \\mathbf{w}\\left\\{a^{2}-\\left(\\mathbf{w}_{\\mathrm{MAP}}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right)^{2}\\right\\} \\mathrm{d} a \\\\ &amp;= \\int\\int \\delta\\left(a-\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right) \\left\\{a^{2}-\\left(\\mathbf{w}_{\\mathrm{MAP}}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right)^{2}\\right\\} \\mathrm{d} a \\ q(\\mathbf{w}) \\mathrm{d} \\mathbf{w} \\\\ &amp;=\\int \\left\\{\\left(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right)^{2}-\\left(\\mathbf{w}_{\\mathrm{MAP}}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right)^{2}\\right\\} q(\\mathbf{w}) \\mathrm{d} \\mathbf{w} \\\\ &amp;=\\boldsymbol{\\phi}^{\\mathrm{T}}\\int \\left\\{ \\mathbf{w}\\mathbf{w}^{\\mathrm{T}} -\\mathbf{w}_{\\mathrm{MAP}}\\mathbf{w}_{\\mathrm{MAP}}^{\\mathrm{T}}\\right\\} q(\\mathbf{w}) \\mathrm{d} \\mathbf{w}\\ \\boldsymbol{\\phi} \\\\&amp;=\\boldsymbol{\\phi}^{\\mathrm{T}} \\mathbf{S}_{N} \\boldsymbol{\\phi} \\end{align} \\] So the predictive distribution becomes \\[ p\\left(\\mathcal{C}_{1} | \\boldsymbol{\\phi},\\boldsymbol{\\mathsf{t}}\\right) \\simeq\\int \\sigma(a) p(a) \\mathrm{d} a=\\int \\sigma(a) \\mathcal{N}\\left(a | \\mu_{a}, \\sigma_{a}^{2}\\right) \\mathrm{d} a \\] Note that we still cannot analytically evaluate the integral over \\(a\\), but we can obtain a good approximation by making use of the close similarity between the inverse probit function and sigmoid function, where we have introduced in 4.3.5 that \\[ \\sigma(a) \\simeq \\Phi (\\lambda a) \\] where \\(\\lambda^2 = \\pi/8\\). The reason to use inverse probit is that its convolution with a Gaussian can be expressed analytically in terms of another inverse probit function. (proof omitted.) Now by a series of approximation we can obtain \\[ \\begin{align} p\\left(\\mathcal{C}_{1} | \\boldsymbol{\\phi},\\boldsymbol{\\mathsf{t}}\\right) &amp;\\simeq\\int \\sigma(a) p(a) \\mathrm{d} a \\\\ &amp;\\simeq\\int \\Phi(\\lambda a) \\mathcal{N}\\left(a | \\mu_{a}, \\sigma_{a}^{2}\\right) \\mathrm{d} a \\\\ &amp; = \\Phi\\left(\\frac{\\mu_a}{\\left(\\lambda^{-2}+\\sigma_a^{2}\\right)^{1 / 2}}\\right) &amp;&amp; \\scriptstyle{\\text{(proof omitted.) }} \\\\ &amp;\\simeq \\sigma\\left(\\kappa\\left(\\sigma_a^{2}\\right) \\mu_a\\right) \\end{align} \\] where we have defined \\(\\kappa\\left(\\sigma_a^{2}\\right)=\\left(1+\\pi \\sigma_a^{2} / 8\\right)^{-1 / 2}​\\). Some Analysis Note that the decision boundary defined via this Bayesian treatment, i.e. \\(p\\left(\\mathcal{C}_{1} | \\boldsymbol{\\phi}, \\boldsymbol{\\mathsf{t}}\\right)=0.5\\), is the same as that obtained by using the MAP value for \\(\\mathbf{w}\\). Thus if the decision criterion is based on minimizing misclassification rate, then the marginalization over \\(\\mathbf{w}\\) has no effect. However, for more complex decision criteria this marginalization will play an important role. Intuitively, under a Bayesian treatment the slope around the origin will not be very large, the approximated predictive distribution as sigmoid will be more smooth, so for more complex decision criteria, we can witness the boundary move more far away from the origin than that obtained by MAP.","link":"/old/PRML4-5.html"},{"title":"Planning by Dynamic Programming","text":"Dynamic programming assumes full knowledge of the MDP. Dynamic Programming Dynamic: sequential or temporal component to the problem (like a table storing subproblem solutions) Programming: optimizing a \"problem\", i.e. a policy DP is a very general solution method for problems which have two properties (MDP satisfies both): Optimal substructure (see the first theorem for optimal policy in last note) Overlapping subproblems (recurring many times, reused by Bellman equation) DP is used for planning in an MDP (recall that it means we have full knowledge of the MDP): For prediction Input: MDP \\(\\left\\langle \\mathcal{S,A,P,R,\\gamma}\\right\\rangle​\\) and policy \\(\\pi​\\) (or MRP \\(\\left\\langle \\mathcal{S,P,R,\\boldsymbol{\\gamma}}\\right\\rangle​\\) without any policy) Output: value function \\(v_{\\pi}\\) For control Input: MDP \\(\\left\\langle \\mathcal{S,A,P,R,\\gamma}\\right\\rangle\\) without policy Output: optimal value function \\(v_*\\) and optimal policy \\(\\pi_*\\) Policy Evaluation Problem: evaluate a given policy \\(\\pi\\). Solution: iterative application of Bellman expectation backup. (here backup means using future state value to update the past) By using synchronous backups: at each iteration \\(k+1\\), for all states \\(s\\in S\\), we update \\(v_{k+1}(s)\\) from \\(v_k(s&#39;)\\), where \\(s&#39;\\) is a successor state of \\(s\\). (\\(v_k(s)\\) means the value of state \\(s\\) at \\(k\\)th iteration.) Asynchronous backups will be discussed later (Convergence has been proven. And note that before convergence, in general, the values for all states do not correspond to any policy, so we denote the change of state value as \\(v_1 \\rightarrow v_2 \\rightarrow\\dots\\rightarrow v_\\pi\\).) We use the same Bellman expectation equation for all states at each iteration: \\[ v_{k+1}(s) = \\sum_{a\\in{\\mathcal{A}}} \\pi{(a|s)} \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_k(s&#39;) \\right) \\] Example: Small Gridworld \\(\\mathcal{S}\\) : nonterminal states \\(1,...,14\\) and one terminal state shown twice as shaded squares. \\(\\mathcal{A}​\\) : four directions. \\(\\mathcal{P}\\) : leading out of the grid leave state unchanged, otherwise with probability \\(1\\) move to the state directed. \\(\\mathcal{R}\\) : Reward is \\(−1\\) until the terminal state is reached. \\(\\mathcal{\\gamma}\\) : \\(1\\) Agent follows uniform random policy, which is going to be evaluated \\[ \\pi(n|\\cdot) = \\pi(w|\\cdot) = \\pi(s|\\cdot) = \\pi(e|\\cdot) = 0.25 \\] Initially, all state values are \\(0\\). At each iteration, we synchronously backup the value. In \\(k=2\\), we gain the value \\(-1.7\\) from \\(-1+0.25\\times ((-1)+(-1)+(-1)+0)\\). The value eventually converges to the true value under random policy, \\(v_\\pi\\). (The second column of the two pictures correspond to the policy improvement using current value.) Policy Iteration Policy iteration is used to improve a policy \\(\\pi\\), by following the two steps: Evaluate the policy \\(\\pi\\) (namely, policy evaluation) Improve the policy by acting greedily w.r.t. \\(v_\\pi\\) : \\[\\pi&#39;(s)=\\text{greedy}(v_\\pi(s)) = \\mathop{\\arg\\max}_{a\\in\\mathcal{A}} q_\\pi{(s,a)}\\] In the small gridworld example above, the improved policy after one evaluation pass was already optimal, while in general, it need more iterations of evaluation and improvement. It has been proven that policy iteration always converges to \\(\\pi^*\\). Example: Car Rental \\(\\mathcal{S}\\) : two locations, maximum \\(20\\) of each (\\(400\\) states in total). \\(\\mathcal{A}\\) : move up to 5 cars between locations overnight. \\(\\mathcal{P}\\) : cars are returned and requested randomly with prob \\(\\frac{\\lambda^n}{n!}e^{-\\lambda}\\) (Poisson distribution). 1st location: average requests = 3, average returns = 3 1st location: average requests = 4, average returns = 2 \\(\\mathcal{R}\\) : $10 for each car rented. \\(\\mathcal{\\gamma}\\) : \\(0.9\\) (There is a demonstration for policy improvement will always converge to an optimal policy in David Silver's course, but it seems that this demonstration does not figure out it won't be stuck in a local maximum. A formal version of the demonstration is by using contraction mapping.) Modified Policy Iteration Note that in small gridworld and many other problems, after only a few iteration will it be able to achieve optimal policy, so We can simply stop after \\(k\\) iterations Or introduce a stopping condition, e.g. \\(\\epsilon\\)-convergence of value function since there's no need to figure out the true value for every policy. A more radical but efficient way is to update policy after only one iteration. This is equivalent to value iteration. Generalized Policy Iteration As we will see later, policy evaluation may not strictly follow the Bellman Equation, neither does the policy improvement algorithm to be greedy. Value Iteration Principle of optimality: A policy \\(\\pi(a|s)\\) achieves the optimal value from state \\(s\\), \\(v_\\pi(s) = v_*(s)\\), if and only if for any state \\(s&#39;\\) reachable from \\(s\\), \\(\\pi\\) achieves the optimal value from \\(s&#39;\\), \\(v_\\pi(s) = v_*(s)\\). Following this principle, if we know the solution to subproblems \\(v_*(s&#39;)\\), then solution \\(v_*(s)\\) can be found by one-step lookahead (just the same idea in Bellman optimality equation) \\[ v_*(s) = \\max_{a\\in{\\mathcal{A}}} \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_*(s&#39;) \\right) \\] So the basic idea is starting from the final rewards and working backwards to update values iteratively. However, such idea also works with loopy, stochastic MDPs (those without final state), and finally updates all values to the optimal even their subproblems' solutions are unknown from the beginning. Example: Shortest Path Each step with reward -1 Here we iteratively applies Bellman optimality backup, and we get \\(v_1 \\rightarrow v_2 \\rightarrow \\cdots \\rightarrow v_*\\). At each iteration, we use synchronous backups. Note that unlike policy iteration, there is no explicit policy, intermediate value functions may not correspond to any policy. Summary of Synchronous DP Algorithms Problem Equation Algorithm Prediction Bellman Expectation Equation Iterative Policy Evaluation Control Bellman Expectation Equation + Greedy Policy Improvement Policy Iteration Control Bellman Optimality Equation Value Iteration All the algorithms above are discussed based on state-value function, and the complexity is \\(O(mn^2)\\) per iteration, for \\(m\\) actions and \\(n\\) states. We could also apply the algorithms to action-value function, but the complexity will rise to \\(O(m^2n^2)\\). Reference [1] David Silver's RL course","link":"/old/RL_DP.html"},{"title":"Introduction to Reinforcement Learning","text":"RL is the study of agents and how they learn by trial and error. It formalizes the idea that rewarding or punishing an agent for its behavior makes it more likely to repeat or forego that behavior in the future. About Reinforcement Learning Many Faces of Reinforcement Learning Reinforcement learning is about a science of decision making, sitting in many different fields of science. Branches of Machine Learning What makes RL different from other ML paradigms? No supervisor, only a reward signal Feedback is delayed, not instantaneous Time really matters (sequential, non i.i.d data) Agent’s actions affect subsequent data it receives The Reinforcement Learning Problem Reward Reward Hypothesis: All goals can be described by the maximization of expected reward. A reward \\(R_t\\) is a scalar feedback signal, indicating how well agent is doing at step \\(t\\). RL is based on the hypothesis showed above, aiming to taking actions to maximize the cumulative reward. Actions may have long term consequences Reward may delay Not greedy: better to sacrifice immediate reward to gain more long-term reward Agent and Environment Here the brain stands for the agent, the earth stands for the environment, showing the interaction: State The history is the sequence of observations, actions, rewards \\[ H_t = O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t \\] The state is the information to determine what happens next. Formally, state is a function of the history \\[ S_t = f(H_t) \\] The environment state \\(S_t^e\\) is the environment’s private representation, not usually visible to the agent, and even visible it may contain irrelevant information. The agent state \\(S_t^a\\) is the agent’s internal representation, used by RL algorithms, and can be any function of history \\(S_t^a = f(H_t)\\). Another independent state definition is information state (a.k.a. Markov state), which contains all useful information from the history. A state is Markov if and only if \\[ \\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1,...,S_t] \\] Both the environment state \\(S_t^e\\) and \\(H_t\\) are Markov, but not always the agent state \\(S_t^a\\), which we need to design properly. Observability Fully observability: agent directly observes environment state, where \\(O_t=S_t^a=S_t^e\\). Formally, this is a Markov decision process (MDP). Partially observability: agent indirectly observes environment, now \\(S_t^a\\neq{S_t^e}\\). Formally, this is a partially observable Markov decision process (POMDP). When partially observable, agent must construct its own state representation \\(S_t^a\\), e.g. From complete history: \\(S_t^a=H_t\\) From beliefs of environment state: \\(S_t^a = (\\mathbb{P[S_t^e=s^1]},..,\\mathbb{P[S_t^e=s^n]})\\) From RNN: \\(S_{t}^{a} = \\sigma(S_{t-1}^aW_s+O_tW_o)\\) Inside RL Agent a maze example An RL agent may include: Policy: Agent’s Behavior It is a map from state to action, e.g. Deterministic policy: \\(a=\\pi(s)\\) Stochastic policy: \\(\\pi(a|s)=\\mathbb{P}[A_t=a|S_t=s]​\\) Arrows represents \\(\\pi(s)\\) at each state. Value Function: Prediction of Future Reward Numbers represents value \\(v_{\\pi}(s)\\) at each state. Used to select between actions, e.g. \\[ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1}+\\gamma{R}_{t+2}+\\gamma^2{R}_{t+3}+...|S_t=s] \\] Model: Representation of Environment Grid layout represents transition model and numbers represent immediate reward from each state. Transitions: \\(\\mathcal{P}\\) predicts the next state e.g. \\[ \\mathcal{P}_{s\\rightarrow{s&#39;}}^a = \\mathbb{P}[S_{t+1}=s&#39;|A_t=a,S_t=s] \\] Rewards: \\(\\mathcal{R}\\) predicts the next (immediate) reward, e.g. \\[ \\mathcal{R}_s^a = \\mathbb{E}[R_{t+1}|A_t=a,S_t=s] \\] Categorizing RL agents Problems within RL Learning (RL) and Planning They are the two fundamental problems in sequential decision making. For learning, the environment is initially unknown, then the agent interacts with the environment and improves its policy. (e.g. new-born baby) For planning, a model of the environment is known, the agent performs computations with its model and improves its policy. (e.g. Go) Exploration and Exploitation Exploration finds more information about the environment. (e.g. try a new restaurant, possibly delicious or awful…) Exploitation exploits known information to maximize reward. (e.g. stay at home, just sleep~) Prediction and Control Prediction: given one policy, evaluate the future. Control: find the best policy to optimize the future, which need to evaluate all policies. Again here’s a grid world example, each step with reward -1, and if arrives at point A (B), you are immediately teleported to A’ (B’), with reward 10 (5). Prediction example: (b) shows the value function in each grid for the uniform random policy (randomly select a direction). Control example: (b) shows the optimal value function in each grid over all possible policies. Reference [1] David Silver's RL course","link":"/old/RL_intro.html"},{"title":"Git 备忘","text":"Git 是一个开源的分布式版本控制系统。温习一下 Git 操作对于提升工作效率确实很有帮助。 本地基本操作 创建仓库: 1$ git init 将一个文件添加至暂存区 1$ git add &lt;file&gt; 将一个文件从暂存区中移除（同时也会从磁盘上移除） 1$ git rm &lt;file&gt; 提交改动 1$ git commit -m '&lt;message&gt;' 检查仓库状态 1$ git status 展示工作区和暂存区的不同之处 1$ git diff 查看提交历史 1$ git log 回退到某个提交版本 1$ git reset --hard &lt;commit_id&gt; 显示过去执行的命令（撤销改动时很有用） 1$ git reflog 丢弃工作区（还未被add到暂存区）的改动 1$ git checkout -- &lt;file&gt; 丢弃暂存区中的改动 1$ git reset HEAD &lt;file&gt; 分支管理 查看所有分支 1$ git branch 创建并切换分支 1$ git checkout -b &lt;branch&gt; 合并某分支到当前分支 1$ git merge &lt;branch&gt; 删除分支 1$ git branch -d &lt;branch&gt; 对于未被合并的分支，删除可能造成修改内容的丢失，需要使用大写参数-D 1$ git branch -D &lt;branch&gt; 合并分支时可能会使用fast forward模式进行合并，仅仅移动了分支指针，看不出做过的改动。加上--no-ff参数可以用普通模式合并，将合并操作作为一次提交写入历史，所以通常还要用-m对提交进行说明 1$ git merge &lt;branch&gt; --no-ff -m 'blabla' 切换分支时保存现场（即把暂存区和未保存的部分存到当前分支下，否则改动会随分支切换而转移），常见于临时进行其他与当前分支无关的改动（如修bug） 1$ git stash 需要恢复现场时先查看可恢复的现场列表 1$ git stash list 恢复某个现场并删除记录 1$ git stash pop &lt;stash_id&gt; pop命令相当于连续分别执行恢复apply和删除drop 12$ git stash apply &lt;stash_id&gt;$ git stash drop &lt;stash_id&gt; 在某一分支上修复bug后可能要将改动也重现在当前分支上，使用cherry-pick命令可以将一个特定的提交复制到当前分支 1$ git cherry-pick &lt;commit_id&gt; 分支开多了后，为了merge之后提交历史看起来更清晰，我们希望将提交历史整理成直线，可以使用rebase命令。但是注意，rebase仅可以把本地未push的分叉提交历史整理成直线 1$ git rebase 远程仓库 为本地仓库关联一个远程仓库（如在GItHub上新创建的仓库） 1$ git remote add origin &lt;remote_repo_addr&gt; 查看远程仓库的信息，-v显示更详细的信息 1$ git remote -v 进行第一次推送，例如推送本地master分支的所有内容 1$ git push origin master 如果要从已有的远程仓库克隆代码来创建本地仓库 1$ git clone &lt;remote_repo_addr&gt; 克隆后默认只能看到本地的master分支，如果要在其他分支如dev进行修改则要创建远程的dev分支到本地 1$ git checkout -b dev origin/dev 推送本地的dev分支到远程仓库也类似地是 1$ git push origin dev 多人协作时常常会因为他人更新而推送失败，需要先试图先用pull合并，如果遇到冲突则还要解决冲突，最后再push到远程仓库 1$ git pull 而对于非master分支，在第一次pull时会因为没有指定本地dev分支与远程origin/dev分支的链接而失败，需要设置关联如下 1$ git branch --set-upstream-to=origin/dev dev 标签 为当前分支上最近一次提交（即HEAD）打上标签 1$ git tag &lt;tag_name&gt; 对指定的提交打上标签，-a指定标签名，-m指定说明文字 1$ git tag -a &lt;tag_name&gt; -m '&lt;description&gt;' &lt;commit_id&gt; 查看所有标签 1$ git tag 向远程仓库推送一个标签 1$ git push origin &lt;tag_name&gt; 推送所有未被推送过的标签 1$ git push origin --tags 删除本地仓库的标签 1$ git tag -d &lt;tag_name&gt; 将删除推送到远程仓库 1$ git push origin :refs/tags/&lt;tag_name&gt; 配置 全局自报家门 12$ git config --global user.name &lt;name&gt;$ git config --global user.email &lt;email&gt; 或者直接修改全局配置文件，路径为~/.gitconfig 123[user] email = &lt;email&gt; name = &lt;name&gt; 在配置文件中还可以使用别名，常用的配置是优化git log输出格式的别名 12[alias] lg = log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit 本地仓库的配置文件路径为仓库根目录下的.git/config 资料 [1] 廖雪峰 Git 教程 [2] Git 官方网站 [3] Git Cheat Sheet [4] 有中文版的Git游戏 Learn Git Branching [5] 英文Git游戏 Githug","link":"/old/git.html"},{"title":"(PRML Notes) 1.2 Probability Theory","text":"A series of notes taken from Pattern Recognition and Machine Learning. Due to the finite size of data sets and the noise on measurements, we use probability to express the uncertainty in pattern recognition. Some concepts like sum rule, product rule, marginal probability, joint probability, conditional probability are very basic, but for this book it would be worthy of gaining insight into Bayes theorem, which plays a central role in PRML \\[ p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)} \\] Using sum rule, the denominator, which can be viewed as a normalizer, can also be expressed in terms of quantities appearing in the numerator \\[ p(X) = \\sum_Y p(X|Y)p(Y) \\] Probability Densities If the probability of a real-valued variable \\(x\\) falling in the interval \\((x, x+\\delta x)\\) is given by \\(p(x)\\delta x\\) for \\(\\delta x \\rightarrow 0\\), then \\(p(x)\\) is called the probability density over \\(x\\). If \\(x\\) is a discrete variable, then \\(p(x)\\) is sometimes called a probability mass function. The probability density \\(p(x)\\) must satisfy two conditions \\[ \\begin{align} p(x) &amp;\\geq 0 \\\\ \\int_{-\\infty}^\\infty p(x) \\mathrm{d}x &amp;= 1 \\end{align} \\] One point worth noting is that under a nonlinear change of variable, a probability density transforms differently from a simple function replacement, but due to a Jacobian factor. Consider a change \\(x=g(y)\\), and \\(p_x(x)\\) and \\(p_y(y)\\) are different densities corresponding to each variables, since for some small values of \\(\\delta x\\), the observed values falling in the range \\((x, x+\\delta x)\\) will be transformed into the range \\((y, y + \\delta y)\\), or sometimes \\((y - \\delta y, y)\\), we have \\[ p_x(x)|\\delta x| \\simeq p_y(y) |\\delta y| \\] and hence \\[ \\begin{align} p_y(y) &amp;= p_x(x)\\left| \\frac{\\mathrm{d}x}{\\mathrm{d}y} \\right| \\\\ &amp;= p_x(g(y))|g&#39;(y)| \\end{align} \\] Intuitively, the larger the derivative \\(g&#39;(y)\\) is, the sparser the changed distribution \\(p_x(x)\\) is. One consequence is that the maximum of a probability density is dependent on the choice of variable. Here \\(y = g^{-1}(x) =1/(1+exp(-x+5))\\) is a logistic sigmoid function, and \\(p_x(x)\\) is a Gaussian \\(\\mathcal{N}(6, 1)\\). The green curve stands for \\(p_x(g(y))\\), while the purple one stands for \\(p_x(g(y))|g&#39;(y)|\\), with a different maximum. Expectations and Covariances Most are mentioned in statistics class. Here a subscript is used when we consider expectations of functions of several variables to indicate which variable is being averaged over, e.g. below we only average over \\(x\\) \\[ \\mathbb{E}_x[f(x,y)] = \\sum_x p(x)f(x,y) \\] But personally I think this definition requires that \\(y\\) is irrelevant to \\(x\\), or just a parameter, otherwise it's somewhat nonsense if we multiply \\(f(x,y)\\), which considers a specific \\(y\\), with a marginal distribution \\(p(x)\\), which already integrated out the \\(y\\). Sometimes we also consider a conditional expectation, e.g. now we average \\(x\\) with \\(y\\) fixed \\[ \\mathbb{E}_x[f(x,y)|y] = \\sum_x p(x|y)f(x,y) \\] In the case of two random variables \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), the covariance is a matrix \\[ \\begin{align} \\mathrm{cov} [\\mathbf{x,y}] &amp;= \\mathbb{E}_\\mathbf{x,y}[\\{\\mathbf{x} - \\mathbb{E}[\\mathbf{x}]\\}\\{\\mathbf{y}^T - \\mathbb{E}[\\mathbf{y^T}]\\}] \\\\ &amp;= \\mathbb{E}_\\mathbf{x,y}[\\mathbf{xy}^T] - \\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{y}^T] \\end{align} \\] If we consider the covariance of the components of \\(\\mathbf{x}\\) with each over, we denote \\(\\mathrm{cov}[\\mathbf{x}] \\equiv \\mathrm{cov}[\\mathbf{x},\\mathbf{x}]\\). Bayesian Probabilities Frequentists view probability as frequencies of random events, while Bayesian view probability as quantification of uncertainty. Given a data set \\(\\mathcal{D} = \\{t_1,...,t_n\\}\\) and model parameters \\(\\mathbf{w}\\), Bayes theorem takes the form \\[ p(\\mathbf{w}|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathcal{D})} \\] allowing us to evaluate uncertainty after \\(\\mathcal{D}\\) is observed. Here \\(p(\\mathbf{w}|\\mathcal{D})\\) is called the posterior probability, \\(p(\\mathcal{D}|\\mathbf{w})\\) can be viewed as a function of \\(\\mathbf{w}\\) called likelihood function, and \\(p(\\mathbf{w})\\) is prior probability where the prior knowledge is included. We can state the theorem in the form \\[ \\mathrm{posterior \\propto likelihood \\times prior} \\] By integrating both sides of the equation above, we have \\[ {p(\\mathcal{D})} = \\int p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})\\ \\mathrm{d}\\mathbf{w} \\] The likelihood function plays a central role in both Bayesian and frequentist paradigms. In a frequentist setting, \\(\\mathbf{w}\\) is being fixed, usually determined by some estimator like maximum likelihood, which is the same as minimizing the negative logarithm of it called error function. For example, we will later see that minimizing \\(E(\\mathbf{w} ) = \\frac{1}{2} \\sum_{n=1}^N [y(x_n ,\\mathbf{w}) - t_n]^2\\) is a form of likelihood maximization. While from the Bayesian viewpoint, there is only a single \\(\\mathcal{D}\\) and uncertainty is expressed through \\(p(\\mathbf{w})\\). Bootstrap is a frequentist way to evaluate accuracy, for example given a set \\(\\mathcal{D} = \\{1,2,3,4,5\\}\\), multi-datasets are generated like \\(\\{4,3,4,1,5\\}\\), \\(\\{1,3,5,5,1\\}\\), which can contain duplicates and evaluation is done on these new datasets. The Gaussian Distribution Univariate Gaussian distribution is defined by \\[ \\mathcal{N} (x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\mathrm{exp}\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\} \\] Note that the reciprocal of the variance in this book is often written as \\(\\beta = 1/\\sigma^2\\) called the precision. Multivariate \\(D\\)-dimensional Gaussian distribution is defined by \\[ \\mathcal{N} (\\mathbf{x}|\\boldsymbol{\\mu}, \\mathbf{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2} |\\mathbf{\\Sigma}|^{1/2}} \\mathrm{exp}\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right\\} \\] Suppose a dataset \\(\\mathtt{x} = (x_1,...,x_N)^T\\) sampled from a univariate Gaussian, i.i.d. between each of them (independently and identically distributed). So the probability of the data set given \\(\\mu\\) and \\(\\sigma^2\\) is \\[ p(\\mathtt x| \\mu , \\sigma^2) = \\prod_{n=1}^N \\mathcal{N}(x_n| \\mu, \\sigma^2) \\] One way to determine the parameters is to maximize the likelihood function above, which can be changed into the logarithm form \\[ \\ln p(\\mathtt x| \\mu , \\sigma^2) = -\\frac{N}{2}\\ln (2\\pi) - N \\ln \\sigma - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\mu)^2 \\] By first differentiating it w.r.t. \\(\\mu\\) we get the solution of \\(\\mu\\) is just the sample mean \\[ \\mu_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^N x_n \\] and then differentiating it w.r.t. \\(\\sigma^2\\) and use the solution of \\(\\mu\\) above we get the solution of \\(\\sigma^2\\) is just the sample variance \\[ \\sigma^2_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^N (x_n-\\mu_\\mathrm{ML})^2 \\] However, in fact, maximum likelihood approach underestimates the variance of the distribution \\[ \\mathbb{E} [\\sigma^2_\\mathrm{ML}] = \\frac{N-1}{N} \\sigma^2 \\] which can be easily proved by using \\(\\mathbb{E}[x_nx_m] = \\mu^2 + I_{nm}\\sigma^2\\), where \\(I_{nm}=1\\) if \\(n=m\\) and \\(I_{nm}=1\\) otherwise. Although the influence of underestimation will be less severe as \\(N\\) grows larger, however, more complex models with many parameters will still suffer. Bias arises in using maximum likelihood to determine the variance of a Gaussian. Curve Fitting Re-visited Previously we solved this problem by minimizing error, now from a probabilistic perspective we can gain some insights into error function, regularization and then a full Bayesian treatment. Now we first use a Gaussian to predict the distribution of data \\[ p(t|x,\\mathbf{w}, \\beta) = \\mathcal{N} (t| y(x,\\mathbf{w}), \\beta^{-1}) \\] Here \\(y(x,\\mathbf{w})= \\sum_{j=0}^M w_j x^j\\) is the polynomial function defined before, and \\(\\beta = 1/\\sigma^2\\) is called the precision of the Gaussian. Using maximum likelihood on the training data \\(\\{\\mathtt{x}, \\mathtt{t}\\}\\), the likelihood function is given by \\[ p(\\mathtt{t}|\\mathtt{x},\\mathbf{w}, \\beta) = \\prod_{n=1}^N\\mathcal{N} (t_n| y(x_n,\\mathbf{w}), \\beta^{-1}) \\] And we have \\[ \\begin{aligned} &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ p(\\mathtt{t}|\\mathtt{x},\\mathbf{w}, \\beta) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ \\ln p(\\mathtt{t}|\\mathtt{x},\\mathbf{w}, \\beta) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ -\\frac{\\beta}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 + \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln (2\\pi) \\\\=\\ &amp;\\mathop{\\arg\\min}_\\mathbf{w} \\ \\frac{1}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 \\end{aligned} \\] We can see that maximizing the log likelihood reach the same optimum as minimizing the sum-of squares error function. Further maximize w.r.t. \\(\\beta\\) we get \\[ \\frac{1}{\\beta_\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^N [y(x_n,\\mathbf{w}_\\mathrm{ML})-t_n]^2 \\] Now we have a probabilistic model expressed in terms of the predictive distribution by \\[ p(t|x,\\mathbf{w}_\\mathrm{ML}, \\beta_\\mathrm{ML}) = \\mathcal{N} (t| y(x,\\mathbf{w}_\\mathrm{ML}), \\beta_\\mathrm{ML}^{-1}) \\] For a more Bayesian approach, we introduce a prior distribution over \\(\\mathbf{w}\\) as Gaussian for simplicity \\[ p(\\mathbf{w}|\\alpha) = \\mathcal{N}(\\mathbf{w}|\\mathbf{0}, \\alpha^{-1}\\mathbf{I}) = \\left(\\frac{\\alpha}{2\\pi}\\right)^{(M+1)/2} \\exp \\left\\{-\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\right\\} \\] Using Bayes theorem, \\[ p(\\mathbf{w}|\\mathtt{x}, \\mathtt{t}, \\alpha, \\beta) = \\frac{p(\\mathtt{t}|\\mathtt{x}, \\mathbf{w}, \\beta)p(\\mathbf{w}|\\alpha)}{p(\\mathtt{t}|\\mathtt{x}, \\beta)} \\propto p(\\mathtt{t}|\\mathtt{x}, \\mathbf{w}, \\beta)p(\\mathbf{w}|\\alpha) \\] To maximize the posterior probability w.r.t. \\(\\mathbf{w}\\), it is equivalent to \\[ \\begin{aligned} &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ p(\\mathbf{w}|\\mathtt{x}, \\mathtt{t}, \\alpha, \\beta) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ \\ln [p(\\mathtt{t}|\\mathtt{x}, \\mathbf{w}, \\beta)p(\\mathbf{w}|\\alpha)] \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ -\\frac{\\beta}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 + \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln (2\\pi) -\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}+ \\frac{M+1}{2}\\ln\\alpha - \\frac{M+1}{2}\\ln(2\\pi) \\\\=\\ &amp;\\mathop{\\arg\\max}_\\mathbf{w} \\ -\\frac{\\beta}{2}\\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 -\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w} \\\\=\\ &amp;\\mathop{\\arg\\min}_\\mathbf{w} \\ \\sum_{n=1}^N [y(x_n,\\mathbf{w})-t_n]^2 +\\frac{\\alpha}{\\beta}\\mathbf{w}^T\\mathbf{w} \\end{aligned} \\] This technique is called maximum posterior (MAP), and we can see that maximizing the posterior is equivalent to minimizing the regularized sum-of-square error with \\(\\lambda = \\alpha/\\beta\\). Bayesian Curve Fitting So far the predictive distribution gives a non-point estimate for \\(t\\), but it is predicted under a point estimate \\(\\mathbf{w}_\\mathrm{ML}\\), or \\(\\mathbf{w}_\\mathrm{MAP}\\). In a fully Bayesian approach, we should integrate all values of \\(\\mathbf{w}\\) \\[ p(t|x,\\mathtt{x},\\mathtt{t}) = \\int p(t|x,\\mathbf{w})p(\\mathbf{w}|\\mathtt{x},\\mathtt{t})\\mathrm{d}\\mathbf{w} \\] Here the precisions \\(\\alpha\\) and \\(\\beta\\) are assumed fixed and known in advance, which will discussed in section 3.3, together with the solution of the integral above.","link":"/old/PRML1-2.html"},{"title":"(PRML Notes) 1.5 Decision Theory","text":"A series of notes taken from Pattern Recognition and Machine Learning. Decision theory allows us to make optimal decisions in situations involving uncertainty. Informally, for classification problem, we are interested in the posterior probability given data which belongs to a class \\(\\mathcal{C}_k\\) \\[ p(\\mathcal{C}_k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C}_k)p(\\mathcal{C}_k)}{p(\\mathbf{x})} \\] Our aim is to minimize the chance of assigning \\(\\mathbf{x}\\) to the wrong class, then intuitively we would choose the class having the higher posterior probability. Actually the intuition is correct, which is shown below. Minimizing the Miss Classification Rate Suppose the input space is divided into regions \\(\\mathcal{R_k}\\), called decision regions, one for each class, so that all points in it are classified to \\(\\mathcal{C_k}\\). The boundaries between decision regions are called decision boundaries or decision surfaces. First consider the binary classification, the probability of a mistake that \\(\\mathbf{x}\\) in \\(\\mathcal{C_1}\\) assigned to \\(\\mathcal{C_2}\\) and vice versa is \\[ \\begin{aligned} p(\\mathrm{mistake}) &amp;= p(\\mathbf{x}\\in \\mathcal{R}_1, \\mathcal{C_2}) + p(\\mathbf{x}\\in \\mathcal{R}_2, \\mathcal{C_1}) \\\\&amp;= \\int_\\mathcal{R_1} p(\\mathbf{x}, \\mathcal{C}_2)\\ \\mathrm{d}\\mathbf{x} + \\int_\\mathcal{R_2} p(\\mathbf{x}, \\mathcal{C}_1)\\ \\mathrm{d}\\mathbf{x} \\end{aligned} \\] For a given value of \\(\\mathbf{x}\\), to minimize \\(p(\\mathrm{mistake})\\), obviously we should assign it to class \\(\\mathcal{C_1}\\) if \\(p(\\mathbf{x}, \\mathcal{C}_1) &gt; p(\\mathbf{x}, \\mathcal{C}_2)\\). And since \\(p(\\mathbf{x}, \\mathcal{C}_k) = p(\\mathcal{C}_k|\\mathbf{x})p(\\mathbf{x})\\), where \\(p(\\mathbf{x})\\) is common for both terms, we can only compare the posterior \\(p(\\mathcal{C}_k|\\mathbf{x})\\), and the minimum \\(p(\\mathrm{mistake})\\) is achieved by assigning \\(\\mathbf{x}\\) to the class with the larger \\(p(\\mathcal{C}_k|\\mathbf{x})\\). The decision boundary is at \\(x_0\\), since when \\(\\hat{x}\\) moves towards \\(x_0\\), the red error area disappears. For the more general case of \\(K\\) classes, the mistake occurs when \\(\\mathbf{x}\\) in class \\(\\mathcal{C_k}\\) is assigned to \\(\\mathcal{C_j}\\) \\[ p(\\mathrm{mistake}) = \\sum_j \\sum_{k\\neq j}\\int_\\mathcal{R_j} p(\\mathbf{x}, \\mathcal{C}_k)\\ \\mathrm{d}\\mathbf{x} \\] It is equivalent and more clear to maximize the probability of being correct \\[ p(\\mathrm{correct}) = 1-p(\\mathrm{mistake}) =\\sum_k \\int_\\mathcal{R_k} p(\\mathbf{x}, \\mathcal{C}_k)\\ \\mathrm{d}\\mathbf{x} \\] Again if aim to minimizing the misclassification rate only, each single \\(\\mathbf{x}\\) should be assigned to the class having the maximum posterior \\(p(\\mathcal{C}_k|\\mathbf{x})\\). Minimizing the Expected Loss Usually some misclassification like diagnosing a patient with cancer as healthy is much more severe than the opposite case and should be punished more. Here we can introduce a loss matrix like \\[ \\begin{array}{@{}r@{}c@{}c@{}c@{}c@{}l@{}} &amp; \\mathrm{cancer} &amp; \\mathrm{normal} \\\\ \\left.\\begin{array} {c} \\mathrm{cancer} \\\\ \\mathrm{normal} \\end{array}\\right( &amp; \\begin{array}{c} 0 \\\\ 1 \\end{array} &amp; \\begin{array}{c} 1000 \\\\ 0 \\end{array} &amp; \\left)\\begin{array}{c} \\\\ \\\\ \\end{array}\\right. \\end{array} \\] Here 1000 denotes a man with cancer diagnosed as normal will be punished 1000 times than a normal man misdiagnosed. Again the mistake occurs when \\(\\mathbf{x}\\) in class \\(\\mathcal{C_k}\\) is assigned to \\(\\mathcal{C_j}\\), and the expected loss is given by \\[ \\mathbb{E}[L]= \\sum_j \\sum_{k}\\int_\\mathcal{R_j} L_{kj}p(\\mathbf{x}, \\mathcal{C}_k)\\ \\mathrm{d}\\mathbf{x} \\] Consider for a single data \\(\\mathbf{x}\\) and choose the region \\(\\mathcal{R_j}\\) it belongs to, we should minimize \\(\\sum_k L_{kj}p(\\mathbf{x}, \\mathcal{C}_k)\\), and as before it is equivalent to minimize \\(\\sum_k L_{kj}p( \\mathcal{C}_k|\\mathbf{x})\\) w.r.t. \\(j\\). For example, if a patient with diagnosis \\(\\mathbf{x}\\) has \\(0.1\\) probability having cancer, i.e. \\(p( \\mathcal{C}_\\mathrm{cancer}|\\mathbf{x})=0.1\\) and \\(p( \\mathcal{C}_\\mathrm{normal}|\\mathbf{x})=0.9\\), then the expected error at this specific point \\(\\mathbf{x}\\) is \\(0*0.1+1*0.9 = 0.9\\) if we classify it as having cancer and \\(1000*0.1+0*0.9=100\\) if we classify it as normal. The Rejection Opinion Usually it will be appropriate to avoid making decision if we are uncertain about class membership, by introducing a threshold \\(\\theta\\) to reject those inputs \\(\\mathbf{x}\\) where \\(\\max_k p( \\mathcal{C}_k|\\mathbf{x}) &lt; \\theta\\). Inference and Decision Now we have broken the classification problem down into two separate stages Inference stage: learning a model for \\(p(\\mathcal{}C_k|\\mathbf{x})\\) from a set of training data Decision stage: making optimal class assignments for new data Alternatively we can combine the two stages and simply learn a discriminant function that maps inputs \\(\\mathbf{x}\\) directly into decisions. In fact, there are three approaches to solve decision problem, in decreasing order of complexity they are Generative model: explicitly or implicitly model the distribution of inputs as well as outputs, by sampling from it is possible to generate synthetic data points in the input space. It first solve the inference problem by determining \\(p(\\mathbf{x}|\\mathcal{C_k})\\) and \\(p(\\mathcal{C_k})\\), then we can infer the posterior and make decision from \\[ p(\\mathcal{C_k}|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C_k})p(\\mathcal{C_k})}{p(\\mathbf{x})} = \\frac{p(\\mathbf{x}|\\mathcal{C_k})p(\\mathcal{C_k})}{\\sum_kp(\\mathbf{x}|\\mathcal{C_k})p(\\mathcal{C_k})} \\] Discriminative model: solve the inference problem directly from learning the posterior \\(p(\\mathcal{C_k}|\\mathbf{x})\\) and then make decision. Discriminative function: combine both the inference and decision stage, learn a function that maps each input directly onto a class label. G model VS D model: Generative model is the most demanding and often need large data to ensure accuracy, but is useful for detecting new data with low probability, known as outlier detection or novelty detection. If we only need to make decisions for classification, we only need the \\(p(\\mathcal{C_k}|\\mathbf{x})\\) in discriminative model as shown below. The class-conditional density (or likelihood function) has no effect on posterior probabilities. Drawbacks of only D function: There are many reasons for wanting to compute the posterior, which can not be accessed in a single discriminative function Minimizing risk: no need to retrain the model if we only modify the loss matrix in decision stage. Reject option: depends on posterior. Compensating for class prior: for better generalization, compute posterior on modified data set and then compensate the effects of modification. (Details shown below) Combining models: combine posterior for small problems which is tackle by a separate model. (Details shown below) Details of \"compensating for class prior\": For example, cancer is rare and there may be only 1 in every 1000 training examples corresponds to cancer, our model can achieve 99.9% accuracy by simply assigning every point to the normal class, which is lack of generalization. One approach is to artificially select equal number of examples from both classes, but we have to compensate for this modification. The compensated posterior takes the form \\[ \\begin{aligned} p(\\mathcal{C_\\mathrm{cancer}}|\\mathbf{x}) &amp;= \\frac{p(\\mathbf{x}|\\mathcal{C_\\mathrm{cancer}})p(\\mathcal{C_\\mathrm{cancer}})}{p(\\mathbf{x})} \\\\&amp;= \\frac{p(\\mathbf{x}|\\mathcal{C_\\mathrm{cancer}})\\hat{p}(\\mathcal{C_\\mathrm{cancer}})}{p(\\mathbf{x})} \\frac{p(\\mathcal{C_\\mathrm{cancer}})}{\\hat{p}(\\mathcal{C_\\mathrm{cancer}})} \\\\&amp;= \\hat{p}(\\mathcal{C_\\mathrm{cancer}}|\\mathbf{x}) \\frac{p(\\mathcal{C_\\mathrm{cancer}})}{\\hat{p}(\\mathcal{C_\\mathrm{cancer}})} \\end{aligned} \\] where \\(\\hat{p}(\\mathcal{C_\\mathrm{cancer}}|\\mathbf{x})\\) is the posterior generated in balanced data set, and both the priors \\(p(\\mathcal{C_\\mathrm{cancer}})\\) and \\(\\hat{p}(\\mathcal{C_\\mathrm{cancer}})\\) can be interpreted as the fractions of points in each class. Details of \"combining models\": If the input spaces, for example, not only include the X-ray images but also blood tests, but now there are models using only one kind of data. Using conditional independent property we can combine these two models \\[ \\begin{aligned} p(\\mathcal{C_k}|\\mathbf{x_I}, \\mathbf{x_B}) &amp;\\propto p(\\mathbf{x_I}, \\mathbf{x_B}|\\mathcal{C_k}) p(\\mathcal{C_k}) \\\\ &amp;= p(\\mathbf{x_I}|\\mathcal{C_k}) p(\\mathbf{x_B}|\\mathcal{C_k}) p(\\mathcal{C_k}) \\\\ &amp;= \\frac{p(\\mathbf{x_I}|\\mathcal{C_k}) p(\\mathcal{C_k})p(\\mathbf{x_B}|\\mathcal{C_k}) p(\\mathcal{C_k})}{p(\\mathcal{C_k})} \\\\ &amp;= \\frac{ p(\\mathcal{C_k}|\\mathbf{x_I})p(\\mathbf{x_I}) p(\\mathcal{C_k}|\\mathbf{x_B})p(\\mathbf{x_B}) }{p(\\mathcal{C_k})} \\\\ &amp;\\propto \\frac{ p(\\mathcal{C_k}|\\mathbf{x_I}) p(\\mathcal{C_k}|\\mathbf{x_B}) }{p(\\mathcal{C_k})} \\end{aligned} \\] And finally we should normalize the new posteriors to ensure they sum to 1. Loss Functions for Regression For regression problem, the decision can be made similarly by minimizing the expected loss \\[ \\mathbb{E}[L] = \\int\\int L(t,y(\\mathbf{x}))p(\\mathbf{x},t) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\] A common choice for loss function in regression problem is \\(L(t,y(\\mathbf{x})) = [y(\\mathbf{x})-t]^2\\), and then minimize the \\(\\mathbb{E}[L]​\\) using the calculus of variations we have \\[ \\frac{\\delta\\mathbb{E}[L]}{\\delta y(\\mathbf{x})} = 2 \\int [y(\\mathbf{x})-t]p(\\mathbf{x},t) \\ \\mathrm{d}t = 0 \\] Then solving \\(y(\\mathbf{x})\\) from it \\[ y(\\mathbf{x}) = \\frac{\\int tp(\\mathbf{x},t)\\ \\mathrm{d}t}{p(\\mathbf{x})} = \\int tp(t|\\mathbf{x})\\ \\mathrm{d}t = \\mathbb{E}[t|\\mathbf{x}] \\] which is the conditional average of \\(t\\) conditioned on \\(\\mathbf{x}\\) and is known as regression function. An alternative way to derive this result is using the knowledge that the optimum is the conditional expectation. This approach shed light on the nature of the regression problem. First expand the squared loss term into \\[ \\begin{aligned} \\ [y(\\mathbf{x})-t]^2 &amp;= [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]+\\mathbb{E}[t|\\mathbf{x}]-t]^2 \\\\ &amp;= [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]^2+2[y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]][\\mathbb{E}[t|\\mathbf{x}]-t]+[\\mathbb{E}[t|\\mathbf{x}]-t]^2 \\end{aligned} \\] Substituting the into the loss function respectively we get for the first term \\[ \\begin{aligned} &amp;\\int \\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]^2p(\\mathbf{x},t) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\\\=&amp; \\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]^2 \\int p(\\mathbf{x},t)\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} \\\\ =&amp; \\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]^2 p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} \\end{aligned} \\] For the second term, which is vanished \\[ \\begin{aligned} &amp;\\int \\int 2[y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]][\\mathbb{E}[t|\\mathbf{x}]-t]p(\\mathbf{x},t) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\\\= &amp; \\ 2\\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]p(\\mathbf{x})\\int[\\mathbb{E}[t|\\mathbf{x}]-t]p(t|\\mathbf{x})\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} \\\\ =&amp;\\ 2\\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]p(\\mathbf{x})*0\\ \\mathrm{d}\\mathbf{x} \\\\=&amp;\\ 0 \\end{aligned} \\] And for the last term \\[ \\begin{aligned} &amp;\\int \\int [\\mathbb{E}[t|\\mathbf{x}]-t]^2p(\\mathbf{x},t) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\\\ =&amp;\\int p(\\mathbf{x})\\int [\\mathbb{E}[t|\\mathbf{x}]-t]^2p(t|\\mathbf{x})\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} %\\\\ =&amp; \\int \\int [\\mathbb{E}^2[t|\\mathbf{x}]-2t\\mathbb{E}[t|\\mathbf{x}]+t^2]p(t|\\mathbf{x})p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t %\\\\ =&amp; \\int \\mathbb{E}^2[t|\\mathbf{x}]p(\\mathbf{x})\\int p(t|\\mathbf{x})\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} + \\int \\mathbb{E}[t|\\mathbf{x}]p(\\mathbf{x}) \\int -2tp(t|\\mathbf{x})\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} + \\int p(\\mathbf{x}) \\int t^2p(t|\\mathbf{x})\\ \\mathrm{d}t \\ \\mathrm{d}\\mathbf{x} %\\\\ =&amp; \\int \\mathbb{E}^2[t|\\mathbf{x}]p(\\mathbf{x})\\ \\mathrm{d}\\mathbf{x} + \\int -2\\mathbb{E}^2[t|\\mathbf{x}]p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} + \\int \\mathbb{E}[t^2|\\mathbf{x}]p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} %\\\\ =&amp; \\int \\left[\\mathbb{E}[t^2|\\mathbf{x}]-\\mathbb{E}^2[t|\\mathbf{x}]\\right]p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} \\\\ =&amp; \\int \\mathrm{var}[t|\\mathbf{x}]p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} \\end{aligned} \\] So finally we have \\[ \\mathbb{E}[L] = \\int [y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]]^2 p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} + \\int \\mathrm{var}[t|\\mathbf{x}]p(\\mathbf{x}) \\ \\mathrm{d}\\mathbf{x} \\] The function \\(y(\\mathbf{x})\\) we seek to determine only enters in the first term called bias, which will vanish if we choose \\(y(\\mathbf{x}) = \\mathbb{E}[t|\\mathbf{x}]\\), and the second term is called noise, which represents the noise during data generation and is irreducible. Similarly, there are three approaches to solve a regression problem, respectively by using: Generative model: learn a joint distribution \\(p(\\mathbf{x}, t)\\) then compute posterior \\(p(t|\\mathbf{x}) = p(\\mathbf{x}, t)/\\int p(\\mathbf{x}, t)\\ \\mathrm{d}t\\), and finally get \\(y(\\mathbf{x}) = \\int t p(t|\\mathbf{x})\\ \\mathrm{d}t\\). Discriminative model: only learn the posterior \\(p(t|\\mathbf{x}) = p(\\mathbf{x}, t)/\\int p(\\mathbf{x}, t)\\ \\mathrm{d}t\\), and then get \\(y(\\mathbf{x}) = \\int t p(t|\\mathbf{x})\\ \\mathrm{d}t\\). Discriminative function: Directly find \\(y(\\mathbf{x})\\) like what we did in multinomial curve fitting before. Sometimes like when \\(p(t|\\mathbf{x})\\) is multimodal, the square loss leads to poor results since \\(t\\) may less likely locates at \\(\\mathbb{E}[t|\\mathbf{x}]\\). We can consider a generalization of the square loss called Minkowski loss \\[ \\mathbb{E}[L_q]=\\int\\int |y(\\mathbf{x})-t|^q p(\\mathbf{x},t) \\ \\mathrm{d}\\mathbf{x}\\ \\mathrm{d}t \\] The minimum of \\(\\mathbb{E}[L_2]\\) is as before the conditional mean, and the minimum of \\(\\mathbb{E}[L_1]\\) is the conditional median, and as \\(q\\rightarrow 0\\), the minimum of \\(\\mathbb{E}[L_q]\\) is the conditional mode.","link":"/old/PRML1-5.html"},{"title":"(PRML Notes) 1.6 Information Theory","text":"A series of notes taken from Pattern Recognition and Machine Learning. When observing a specific value of \\(x\\), the amount of information received can be viewed as the \"degree of surprise\". Higher information is received when a highly improbable value of \\(x\\) has just observed, so the measure of information will depend monotonically on \\(p(x)\\), we denote it as \\(h(x)\\). Entropy of Discrete Distribution For independent variables \\(x\\) and \\(y\\), for which \\(p(x,y) = p(x)p(y)\\), the information should be addictive, i.e. \\(h(x,y) = h(x) + h(y)\\), meaning that the information gained from observing both of them should be the sum of the information gained from each of them separately. Then it can be shown that the relationship between \\(h(x)\\) and \\(p(x)\\) must be logarithm, so we can define \\[ h(x) = -\\log_2p(x) \\] The basis is chosen to be 2 by convention, so that the units of \\(h(x)\\) can be interpreted as bits used when transmitting a specific value of \\(x\\). Following this interpretation, the average amount of bits during one transmission is \\[ \\mathrm{H}[x] = - \\sum_x p(x)\\log_2p(x) \\] Image of \\(y=x\\log_2 x\\), the maximum locates at \\((\\frac{1}{e}, \\frac{1}{e\\ln 2})\\). This important quantity is called the entropy of the random variable \\(x\\). As the noiseless coding theorem stated, the entropy is a lower bound on the number of bits needed to transmit the state of a random variable. For example, Consider a sender wishes to transmit a variable \\(x\\) having 8 possible states \\(\\{a,b,c,d,e,f,g,h\\}\\), for which the respective probabilities are given by \\(\\left( \\frac{1}{2}, \\frac{1}{4},\\frac{1}{8},\\frac{1}{16},\\frac{1}{64},\\frac{1}{64},\\frac{1}{64},\\frac{1}{64}\\right)\\), according to Haffman coding the optimal set of coding string is \\(0\\), \\(10\\), \\(110\\), \\(1110\\), \\(111100\\), \\(111101\\), \\(111110\\), \\(111111\\), hence the average length of code to be transmitted is \\[ \\frac{1}{2} \\times 1 +\\frac{1}{4} \\times 2 +\\frac{1}{8} \\times 3 +\\frac{1}{16} \\times 4 +4 \\times \\frac{1}{64} \\times 6 = 2\\ \\mathrm{bits} \\] which is the same as the entropy of this random variable \\[ \\mathrm{H}[x] = -\\frac{1}{2} \\times \\log_2 \\frac{1}{2} -\\frac{1}{4} \\times \\log_2 \\frac{1}{4} -\\frac{1}{8} \\times \\log_2 \\frac{1}{8} -\\frac{1}{16} \\times \\log_2 \\frac{1}{16} -4\\times\\frac{1}{64} \\times \\log_2 \\frac{1}{64} = 2 \\ \\mathrm{bits} \\] Note that the nonuniform distribution has a smaller entropy than a uniform one, which can be interpreted in terms of disorder. Consider sending another variable having 8 possible states but with equal possibilities, now the entropy is given by \\[ \\mathrm{H}[x] = -8 \\times \\frac{1}{8} \\times \\log_2 \\frac{1}{8} = 3 \\ \\mathrm{bits} \\] More generally, we switch to the use of natural logarithm in entropy in order to follow its much earlier origins in physics, and this will also provide a more convenient link with ideas in later chapters. To understand this view of entropy, consider a set of \\(N\\) identical objects that are to be divided amongst \\(M\\) bins, so that there are \\(n_i\\) objects in the \\(i^\\mathrm{th}\\) bin. The total number of ways allocating, which is also called the multiplicity, is \\[ W = C_N^{n_1}C_{N-n_1}^{n_2}C_{N-n_1-n_2}^{n_3}\\cdots C_{N-n_1-\\cdots-n_{M-1}}^{n_M} = \\frac{N!}{\\prod_in_i!} \\] In physics, the entropy is defined as the logarithm of the multiplicity scaled by an appropriate constant \\[ \\mathrm{H} = \\frac{1}{N}\\ln W = \\frac{1}{N}\\ln N! -\\frac{1}{N}\\sum_i \\ln n_i! \\] Now consider the limit \\(N \\rightarrow \\infty\\) with \\(n_i/N\\) held fixed, applying Stirling approximation, \\(N! \\approx \\sqrt{2\\pi N} (N/e)^N\\), we have \\[ \\ln N! \\approx N\\ln N - N \\] where the \\(O(\\ln N)\\) term is dropped. This gives \\[ \\begin{aligned} \\mathrm{H} &amp;= \\lim_{N\\rightarrow\\infty} \\ln N -1 -\\frac{1}{N} \\sum_i (n_i \\ln n_i -n_i) \\\\ &amp;= \\lim_{N\\rightarrow\\infty}\\ln N -\\frac{1}{N} \\sum_i (n_i \\ln n_i) \\\\ &amp;= \\lim_{N\\rightarrow\\infty}\\frac{1}{N}\\sum_i n_i\\ln N -\\frac{1}{N} \\sum_i (n_i \\ln n_i) \\\\ &amp;= -\\lim_{N\\rightarrow\\infty}\\sum_i \\frac{n_i}{N} \\ln \\frac{n_i}{N} \\\\ &amp;= -\\sum_i p_i \\ln p_i \\end{aligned} \\] Here \\(p_i = \\lim_{N\\rightarrow\\infty}\\frac{n_i}{N}\\) is the probability of an object being assigned to the \\(i^\\mathrm{th}\\) bin. We can interpret the bins as the states \\(x_i\\) of a discrete random variable \\(X\\), where \\(p(X=x_i) = p_i\\), now the entropy is given in the similar form as before \\[ \\mathrm{H}[p] =-\\sum_i p(x_i) \\ln p(x_i) \\] The maximum of the entropy above can be obtained by using Jensen's inequality \\[ \\begin{aligned} \\mathrm{H}[p] &amp;=-\\sum_i p(x_i) \\ln p(x_i) \\\\ &amp;= \\sum_i p(x_i) \\ln \\frac{1}{p(x_i)} \\\\ &amp;\\leq \\ln \\sum_i p(x_i) \\frac{1}{p(x_i)} \\\\ &amp;= \\ln M \\end{aligned} \\] Here \\(M\\) is the total number of states, or bins. The maximum is achieved if and only if \\(p(x_1) = p(x_2)=p(x_M)=1/M\\). Intuitively speaking, the more scatter the distribution is, the larger the entropy is. Entropy of Continuous Distribution The definition of entropy can be extended to include distributions over continuous variables \\(x\\) as follows. First divide \\(x\\) into bins of width \\(\\Delta\\), then according to mean value theorem, in the \\(i^\\mathrm{th}\\) bin there must exist a value \\(x_i\\) such that \\[ \\int_{i\\Delta}^{(i+1)\\Delta} p(x) \\mathrm{d}x = p(x_i)\\Delta \\] Here \\(p(x_i)\\Delta\\) is exactly the probability of \\(x\\) allocated to the \\(i^\\mathrm{th}\\) bin. So now the entropy takes the form \\[ \\begin{aligned} \\mathrm{H}_\\Delta &amp;= - \\sum_i p(x_i)\\Delta \\ln (p(x_i)\\Delta) \\\\ &amp;= - \\sum_i p(x_i)\\Delta \\ln p(x_i)- \\sum_i p(x_i)\\Delta \\ln \\Delta \\\\ &amp;= - \\sum_i p(x_i)\\Delta \\ln p(x_i)- \\ln \\Delta \\end{aligned} \\] If we omit the second term \\(-\\ln\\Delta\\) on r.h.s. and consider the limit \\(\\Delta \\rightarrow 0\\), we have \\[ \\mathrm{H}_\\Delta = - \\int p(x) \\ln p(x) \\mathrm{d}x \\] This is called the differential entropy. The omitted term \\(\\Delta \\rightarrow 0\\) reflects the fact that to specify a continuous variable very precisely requires a large number of bits, and it is irrelevant to \\(p(x)\\), that is why we should omit it. The maximum of the entropy above can be obtained under the three constraints \\[ \\begin{aligned} \\int_{-\\infty}^{\\infty} p(x) \\mathrm{d}x\\ &amp;=\\ 1 \\\\ \\int_{-\\infty}^{\\infty} xp(x) \\mathrm{d}x\\ &amp;=\\ \\mu \\\\ \\int_{-\\infty}^{\\infty} (x-\\mu)^2p(x) \\mathrm{d}x\\ &amp;=\\ \\sigma^2 \\end{aligned} \\] Using Lagrange multiplier we get \\[ -\\int_{-\\infty}^{\\infty} p(x)\\ln p(x) \\mathrm{d}x+ \\lambda_1 \\left(\\int_{-\\infty}^{\\infty} p(x) \\mathrm{d}x-1\\right) + \\lambda_2\\left(\\int_{-\\infty}^{\\infty} xp(x) \\mathrm{d}x - \\mu \\right) + \\lambda_3 \\left(\\int_{-\\infty}^{\\infty} (x-\\mu)^2p(x) \\mathrm{d}x- \\sigma^2\\right) \\] and then maximize the functional w.r.t. \\(p(x)\\) by setting the derivative of this functional to zero, giving \\[ -1 - \\ln p(x) +\\lambda_1 +\\lambda_2 x+ \\lambda_3 (x-\\mu)^2 = 0 \\] Then \\[ p(x) = \\exp \\left\\{ -1 +\\lambda_1 +\\lambda_2 x+ \\lambda_3 (x-\\mu)^2 \\right\\} \\] Solving the last equation together with three constraints (this is somewhat difficult, I don't knowhow to solve actually...) we get \\[ \\begin{aligned} \\lambda_1\\ &amp;=\\ 1-\\frac{1}{2} \\ln (2\\pi \\sigma^2) \\\\ \\lambda_2 \\ &amp;=\\ 0 \\\\ \\lambda_3\\ &amp;=\\ -\\frac{1}{2\\sigma^2} \\end{aligned} \\] We see that \\(p(x)\\) is actually Gaussian \\[ p(x) = \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\left\\{ -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right\\} \\] This is why Gaussian is found widely in nature, which is developing with the trend of entropy increase. We can see that the differential entropy, unlike the discrete entropy, can be negative. For example, the entropy of Gaussian is \\[ \\begin{aligned} \\mathrm{H}[x] &amp;= - \\int_{-\\infty}^\\infty p(x) \\ln p(x) \\mathrm{d}x \\\\ &amp;= - \\int_{-\\infty}^\\infty p(x) \\left( -\\frac{1}{2}\\ln (2\\pi\\sigma^2) -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\\mathrm{d}x \\\\ &amp;= \\frac{1}{2}\\ln (2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\int_{-\\infty}^\\infty p(x) (x-\\mu)^2 \\mathrm{d}x \\\\ &amp;= \\frac{1}{2}\\ln (2\\pi\\sigma^2) +\\frac{1}{2} \\end{aligned} \\] \\(\\mathrm{H}[x]&lt;0\\) for \\(\\sigma^2 &lt; 1/(2\\pi e)\\). We omitted the term \\(-\\ln \\Delta\\) in original entropy expression before, which is an infinite positive term. Conditional Entropy Now consider a joint distribution \\(p(x,y)\\), if a value of \\(x\\) is already known, then the additional information needed to specify the corresponding value of \\(y\\) is given by \\(-\\ln p(y|x)\\), thus the average additional information needed to specify \\(y\\) is \\[ \\mathrm{H}[y|x] = -\\int\\int p(y,x)\\ln p(y|x)\\ \\mathrm{d}y\\ \\mathrm{d}x \\] which is called the conditional entropy of \\(y\\) given \\(x\\). We can see that \\[ \\begin{aligned} \\mathrm{H}[y,x] &amp;= -\\int\\int p(y,x)\\ln p(y,x)\\ \\mathrm{d}y\\ \\mathrm{d}x \\\\ &amp;= -\\int\\int p(y,x)\\ln p(y|x)\\ \\mathrm{d}y\\ \\mathrm{d}x\\ -\\int\\int p(y,x)\\ln p(x)\\ \\mathrm{d}y\\ \\mathrm{d}x \\\\ &amp;= \\mathrm{H}[y|x] + \\mathrm{H}[x] \\end{aligned} \\] which means the information needed to describe \\(x\\) and \\(y\\) is given by the sum of the information needed to describe \\(x\\) alone plus the additional information required to specify \\(y\\) given \\(x\\). KL Divergence Now it's time to relate these idea to pattern recognition. Given an unknown distribution \\(p(x)\\), which has been approximated using distribution \\(q(x)\\), the KL divergence, or the relative entropy is the average addition amount of information required to transmit values of \\(x\\) if we use \\(q(x)\\) instead of \\(p(x)\\) to construct a coding scheme. So under this definition we have \\[ \\begin{aligned} \\mathrm{KL}(p|| q) &amp;=- \\int p(x) \\ln q(x) \\mathrm{d}x - \\left(- \\int p(x) \\ln p(x) \\mathrm{d}x \\right)\\\\\\\\ &amp;=- \\int p(x) \\ln \\left\\{ \\frac{q(x)}{p(x)} \\right\\} \\mathrm{d}x \\\\ \\end{aligned} \\] Note that \\(\\mathrm{KL}(p|| q) \\neq \\mathrm{KL}(q|| p)\\). And since \\(-\\ln x\\) is strictly concave, using Jensen inequality we have \\[ \\mathrm{KL}(p|| q) = - \\int p(x) \\ln \\left\\{ \\frac{q(x)}{p(x)} \\right\\} \\mathrm{d}x \\geq - \\ln\\int p(x) \\left( \\frac{q(x)}{p(x)} \\right)\\mathrm{d}x = -\\ln 1 = 0 \\] which shows that \\(\\mathrm{KL}(p|| q) \\geq 0\\) with equality if and only if \\(p(x)= q(x)\\). Thus we can interpret the KL divergence as a measure of the dissimilarity of the two distributions, and we can show that minimizing the KL divergence is equivalent to maximizing the likelihood function. Suppose we try to approximate \\(p(x)\\) using a parametric distribution \\(q(x|\\theta)\\). Although we don't know \\(p(x)\\) to compute KL divergence directly, we can sample from \\(p(x)\\), \\[ \\begin{aligned} \\mathrm{KL}(p|| q) &amp;= - \\int p(x) \\ln \\left\\{ \\frac{q(x)}{p(x)} \\right\\} \\mathrm{d}x \\\\ &amp;\\simeq - \\frac{1}{N} \\sum_n \\ln \\left\\{ \\frac{q(x_n|\\theta)}{p(x_n)} \\right\\} \\\\ &amp;= - \\frac{1}{N} \\sum_n \\ln q(x_n|\\theta) + \\frac{1}{N} \\sum_n \\ln p(x_n) \\end{aligned} \\] Then \\[ \\mathop{\\arg \\min}_\\theta \\mathrm{KL}(p|| q) = \\mathop{\\arg \\max}_\\theta \\sum_n \\ln q(x_n|\\theta) \\] Mutual Information If the two variables of distribution \\(p(x,y)​\\) are not independent, we can measure how \"close\" they are to being independent by using KL divergence \\[ \\begin{aligned} \\mathrm{I}[x,y] &amp;= \\mathrm{KL}(p(x,y)||p(x)p(y))\\\\ &amp;= - \\int\\int p(x,y) \\ln \\left\\{ \\frac{p(x)p(y)}{p(x,y)} \\right\\} \\ \\mathrm{d}x\\ \\mathrm{d}y \\end{aligned} \\] Here \\(\\mathrm{I}[x,y]\\) is called the mutual information between the variables \\(x\\) and \\(y\\). We can see that \\(\\mathrm{I}[x,y] \\geq 0\\) with equality iff \\(x\\) and \\(y\\) are independent. Note that the fraction in logarithm can be rewritten as \\(p(x)/p(x|y)\\) or \\(p(y)/p(y|x)\\), the mutual information can be related to the conditional entropy through \\[ \\mathrm{I}[x,y] = \\mathrm{H}[x] - \\mathrm{H}[x|y] = \\mathrm{H}[y] - \\mathrm{H}[y|x] \\] From this point of view, the mutual information represents the reduction in uncertainty about \\(x\\) as a consequence of the new observation \\(y\\), or vice versa.","link":"/old/PRML1-6.html"},{"title":"(PRML Notes) 2.1 Binary Variables","text":"A series of notes taken from Pattern Recognition and Machine Learning. Bernoulli Distribution For a random variable \\(x\\) that, for example, describes the outcome of flipping a damaged coin, with the probability of landing heads \\(p(x=1|\\mu)=\\mu\\), and landing tails \\(p(x=0|\\mu)=1-\\mu\\). The Bernoulli distribution is the probability over such \\(x\\) that can be written in the form \\[ \\mathrm{Bern}(x|\\mu) = \\mu^x(1-\\mu)^{1-x} \\] whose mean and variance are given by \\[ \\begin{aligned} \\mathbb{E}[x] &amp;= \\mu \\\\ \\mathrm{var}[x] &amp;= \\mu(1-\\mu) \\end{aligned} \\] Maximum Likelihood Estimation Given a data set \\(\\mathcal{D} = \\{x_1,...,x_N \\}\\) of observed values of \\(x\\), estimating the value of \\(\\mu\\) using MLE is to maximize such log likelihood \\[ \\ln p(\\mathcal{D}|\\mu) = \\sum_{n=1}^N\\ln p(x_n|\\mu) = \\sum_{n=1}^N \\{x_n\\ln \\mu + (1-x_n)\\ln(1-\\mu)\\} \\] Note that the log likelihood function depends on these observations only through their sum \\(\\sum_n x_n\\), which provides an example of a sufficient statistic for the data under this distribution. By setting the derivative w.r.t. \\(\\mu\\) equal to zero, we obtain \\[ \\mu_{\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^N x_n \\] which is also known as the sample mean, meaning that \\(\\mu_{\\mathrm{ML}}\\) is given by the fraction of observations of landing heads. Over-fitting Problem MLE usually over-fits on small data set, e.g. \\(N=3\\) and all results are landing heads, then all the future predictions will give heads, which is unreasonable under common sense. More sensible conclusions can be arrived through the introduction of a prior distribution (e.g. beta distribution) over \\(\\mu​\\). Binomial Distribution If we consider the variable \\(m=x_1+x_2+\\cdots+x_N\\), which denotes the number of observations of \\(x=1\\) (landing heads) over a data set with size \\(N\\), then the distribution over \\(m\\) can be written as \\[ \\mathrm{Bin}(m|N,\\mu) = \\binom{N}{m}\\mu^m(1-\\mu)^{N-m} \\] This is called the Binomial Distribution, and its mean and variance can be easily given using the sum rule of independent variables \\[ \\begin{aligned} \\mathbb{E}[m] &amp;= \\sum_{n=1}^N \\mathbb{E}[x_n] = N\\mu\\\\ \\mathrm{var}[m] &amp;= \\sum_{n=1}^N \\mathrm{var}[x_n] = N\\mu(1-\\mu) \\end{aligned} \\] Beta Distribution The beta distribution is a conjugate prior to binomial distribution, which means that having the same functional form as the binomial distribution so that resulting in a concise posterior. The beta distribution is given by \\[ \\mathrm{Beta}(\\mu | a,b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\mu^{a-1} (1-\\mu)^{b-1} \\] where \\(\\Gamma(x) = \\int_0^\\infty u^{x-1} e^{-u}du\\) is called gamma function, and the coefficient ensure the distribution is normalized \\[ \\int_0^1 \\mu^{a-1} (1-\\mu)^{b-1}d\\mu = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)} \\] Its mean and variance are given by \\[ \\begin{aligned} \\mathbb{E}[\\mu] &amp;= \\frac{a}{a+b}\\\\ \\mathrm{var}[\\mu] &amp;= \\frac{ab}{(a+b)^2(a+b+1)} \\end{aligned} \\] Plots of beta distribution with different hyperparameters. Effective Number of Observations If we use \\(l=N-m\\) to denote the number of observations of \\(x=0\\) (landing tails), then the posterior distribution of \\(\\mu\\) can be obtained by \\[ p(\\mu|m,l,a,b) \\propto p(\\mu|a,b)p(m,l|\\mu,a,b) \\propto \\mu^{m+a-1}\\mu^{l+b-1} \\] By comparing this to the form of binomial distribution, we can interpret the effective number of observations of \\(x=1\\) and \\(x=0\\) in posterior to be \\(m+a-1\\) and \\(l+b-1\\). Actually, the beta distribution as a prior provides a probability distribution over \\(\\mu\\) that having \"virtually\" observed \\(a-1\\) occurrence of \\(x=1\\) and \\(b-1\\) occurrence of \\(x=0\\). Tips: \\(a\\) and \\(b\\) need not be integers. Sequential Update The posterior can act as a prior, when taking observations one at a time and updating the current posterior by multiplying the likelihood for the new observation and then normalize it. Sequential update example for prior with \\(a=b=2\\), after observing one \\(x=1\\) data. The sequential update can be used for Real-time learning, where predictions must be made before seeing all data. Large data sets, because whole data set is not required to be loaded into memory. Relation to MLE The posterior prediction of the next one trial can be written as \\[ p(x=1|\\mathcal{D}) = \\int_0^1p(x=1|\\mu)p(\\mu|\\mathcal{D})d\\mu = \\int_0^1 \\mu p(\\mu|\\mathcal{D})d\\mu = \\mathbb{E}[\\mu|\\mathcal{D}] = \\frac{m+a}{m+a+l+b} \\] In the limit of \\(m,l \\rightarrow \\infty\\), the result reduce to the maximum likelihood result \\(m/(m+l)\\). Actually for a finite dataset, the posterior mean for \\(\\mu\\) can be written in the form \\[ \\frac{m+a}{m+a+l+b} = \\lambda\\frac{a}{a+b} + (1-\\lambda)\\frac{m}{m+l} \\] with \\(\\lambda\\in(0,1)\\). It can be calculated directly that \\(\\lambda = \\frac{a+b}{m+a+l+b} \\in (0,1)\\). Variance of Posterior For the beta distribution, we see that variance of \\(\\mu​\\) goes to zero for \\(a\\rightarrow \\infty​\\) and \\(b\\rightarrow \\infty​\\), In fact, it is a general property for Bayesian learning that the uncertainty represented by the posterior will steadily decrease as more and more data is observed. If we take a frequentist view of Bayesian learning, and consider a general Bayesian inference problem for a parameter \\(\\mathbf{\\theta}\\) where we have observed a dataset \\(\\mathcal{D}\\), described by the joint distribution \\(p(\\mathbf{\\theta}, \\mathcal{D})\\). (Tips: the following \"prior\" refers to the real prior distribution of \\(\\theta\\), rather than one assumed subjectively by Bayesian.) We can show that \\[ \\mathrm{var}_\\mathbf{\\theta}[\\mathbf{\\theta}] = \\mathbb{E}_\\mathcal{D}[\\mathrm{var}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]] + \\mathrm{var}_\\mathcal{D}[\\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]] \\] Since the second term, the posterior mean of \\(\\mathbf{\\theta}​\\) is positive, which means on average (not for a particular \\(\\mathcal{D} ​\\)), the posterior variance of \\(\\mathbf{\\theta}​\\) is smaller than the prior variance. This equation only shows \\(\\mathbb{E}_\\mathcal{D}[\\mathrm{var}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]] &lt; \\mathrm{var}_\\mathbf{\\theta}[\\mathbf{\\theta}]\\), but does not show \\(\\mathrm{var}_\\mathcal{D}[\\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]]\\) will increase as more and more data is observed. Intuitively, the posterior mean of \\(\\mathbf{\\theta}\\)s on different datasets gradually move to the different true values where the observed \\(\\mathcal{D}\\)s generated from, resulting in larger variance over \\(\\mathcal{D}\\). Proof of the above equation First we show that the posterior mean of \\(\\mathbf{\\theta}\\) (\\(\\mathbf{\\theta}^2\\)), averaged over the distribution generating the data, is equal to the prior mean of \\(\\mathbf{\\theta}\\) (\\(\\mathbf{\\theta}^2\\)) \\[ \\begin{align} \\mathbb{E}_{\\mathcal{D}} [ \\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]] &amp;= \\int p(\\mathcal{D}) \\int \\mathbf{\\theta}p(\\mathbf{\\theta}|\\mathcal{D})\\ d\\mathbf{\\theta}\\ d\\mathcal{D} \\\\ &amp;= \\int \\int \\mathbf{\\theta}p(\\mathbf{\\theta},\\mathcal{D})\\ d\\mathcal{D}\\ d\\mathbf{\\theta} \\\\ &amp;= \\int \\mathbf{\\theta}p(\\mathbf{\\theta})\\ d\\mathbf{\\theta} \\\\ &amp;= \\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}]\\\\\\\\ \\mathbb{E}_{\\mathcal{D}} [ \\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}^2|\\mathcal{D}]] &amp;=\\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}^2] , \\end{align} \\] then we have \\[ \\begin{align} \\mathrm{var}_\\mathbf{\\theta}[\\mathbf{\\theta}] &amp;= \\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}^2] - [\\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}]]^2 \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} [ \\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}^2|\\mathcal{D}]] - [\\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]]]^2 \\\\ &amp;= \\mathbb{E}_{\\mathcal{D}} [ \\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}^2|\\mathcal{D}]] - \\mathbb{E}_{\\mathcal{D}}[[\\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]]^2] + \\mathbb{E}_{\\mathcal{D}}[[\\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]]^2] - [\\mathbb{E}_{\\mathcal{D}}[\\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]]]^2 \\\\ &amp;= \\mathbb{E}_\\mathcal{D}[\\mathrm{var}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]] + \\mathrm{var}_\\mathcal{D}[\\mathbb{E}_\\mathbf{\\theta}[\\mathbf{\\theta}|\\mathcal{D}]] \\end{align} \\]","link":"/old/PRML2-1.html"},{"title":"(PRML Notes) 2.3.0 Gaussian Distribution","text":"A series of notes taken from Pattern Recognition and Machine Learning. Introduction In the case of single variable \\(x​\\), the Gaussian distribution can be written in the form \\[ \\mathcal{N}(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right\\} \\] with mean \\(\\mu\\) and variance \\(\\sigma^2\\). For a \\(D\\)-dimensional vector \\(\\mathbf{x}\\), the multivariate Gaussian distribution takes the form \\[ \\mathcal{N}(\\mathbf{x}| \\boldsymbol{\\mu},\\mathbf{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\exp\\left\\{ -\\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) \\right\\} \\] with \\(D\\)-dimensional mean vector \\(\\boldsymbol{\\mu}\\) and \\(D\\times D\\) covariance matrix \\(\\mathbf{\\Sigma}\\). Gaussian distribution arises in many cases, for example Entropy maximization, in section 1.6 When consider the sum of multiple random variables, e.g. binomial distribution tends to Gaussian as \\(N\\rightarrow\\infty\\) according to central limit theorem Coordinate Transformation Transform the original coordinate into a new shifted and rotated coordinates, w.r.t. which the joint multivariate Gaussian can be factorized into a product of independent univariate Gaussian. The functional dependence of the multivariate Gaussian is through the quadratic form \\[ \\Delta^2 = (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) \\] where the quantity \\(\\Delta​\\) is called the Mahalanobis distance from \\(\\boldsymbol{\\mu}​\\) to \\(\\mathbf{x}​\\). Note that the matrix \\(\\mathbf{\\Sigma}​\\) can be taken to be symmetric without loss of generality. Why \\(\\mathbf{\\Sigma}\\) we discussed are always symmetric? Any matrix \\(\\mathbf{M}\\) can be written as sum of a symmetric matrix \\(\\mathbf{M}_S\\) and an anti-symmetric matrix \\(\\mathbf{M}_A\\), where \\(\\mathbf{M}_S= (\\mathbf{M} + \\mathbf{M}^T)/2\\) and \\(\\mathbf{M}_A = (\\mathbf{M}-\\mathbf{M}^T)/2\\). When substitute \\(\\mathbf{\\Sigma}=\\mathbf{\\Sigma}_A+\\mathbf{\\Sigma}_S\\) into \\(\\Delta^2\\), note that \\((\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}_A^{-1}(\\mathbf{x}- \\boldsymbol{\\mu})\\) is a number, we have \\[ \\begin{align} (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}_A^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) &amp;= \\frac{1}{2}(\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}_A^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) + \\frac{1}{2}(\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}_A^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) \\\\ &amp;= \\frac{1}{2}(\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}_A^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) + \\frac{1}{2}\\left[ (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}_A^{-1}(\\mathbf{x}- \\boldsymbol{\\mu})\\right]^T \\\\ &amp;= \\frac{1}{2}(\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}_A^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) - \\frac{1}{2}(\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}_A^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) \\\\ &amp;= 0 \\end{align} \\] The term in involving \\(\\mathbf{\\Sigma}_A\\) vanishes, only the symmetric part \\(\\mathbf{\\Sigma}_S\\) remains. Then by eigendecomposition of the symmetric matrix \\(\\mathbf{\\Sigma}\\) we have \\[ \\mathbf{\\Sigma} = \\mathbf{U}\\Lambda\\mathbf{U}^T = \\sum_{i=1}^D\\lambda_i\\mathbf{u}_i\\mathbf{u}_i^T \\] where \\(\\Lambda=\\mathrm{diag}(\\lambda_1,...,\\lambda_D)\\) with \\(\\lambda_i\\) the eigenvalue of \\(\\mathbf{\\Sigma}\\), and \\(\\mathbf{U}=(\\mathbf{u}_1,...,\\mathbf{u}_D)\\) is an orthogonal matrix with \\(\\mathbf{u}_i\\) the eigenvector of \\(\\lambda_i\\). Thus for \\(\\mathbf{\\Sigma}^{-1}\\) \\[ \\mathbf{\\Sigma}^{-1} = \\mathbf{U}\\Lambda^{-1}\\mathbf{U}^T = \\sum_{i=1}^D\\frac{1}{\\lambda_i}\\mathbf{u}_i\\mathbf{u}_i^T \\] By substituting it to the expression of \\(\\Delta^2\\), we have \\[ \\Delta^2 = (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{U}\\Lambda^{-1}\\mathbf{U}^T(\\mathbf{x}- \\boldsymbol{\\mu}) = \\sum_{i=1}^{D} \\frac{y_i^2}{\\lambda_i} =||\\Lambda^{-1/2} \\mathbf{y}||_2 \\] where \\(\\mathbf{y}=\\mathbf{U}^T(\\mathbf{x}- \\boldsymbol{\\mu}) = (y_1,...,y_D)^T\\), each \\(y_i = \\mathbf{u}_i^T (\\mathbf{x}- \\boldsymbol{\\mu})\\) represents the projection length of \\(\\mathbf{x}- \\boldsymbol{\\mu}\\) on the \\(i\\)th eigenvector \\(\\mathbf{u}_i​\\). Now we have transformed the original \\(\\mathbf{x}\\) coordinate to a more concise coordinate system \\(\\mathbf{y}\\). The Gaussian density will be constant on surface where \\(\\Delta^2\\) is constant. These surfaces according to different constants represent ellipsoids if all the \\(\\lambda_i\\) are positive, centering at \\(\\boldsymbol{\\mu}\\) and axes orienting along \\(\\mathbf{u}_i\\), with scaling factors in the directions of the axes given by \\(\\lambda_i^{1/2}​\\). Coordinate Transformation. In the new \\(\\mathbf{y}\\) coordinate system, the Gaussian takes the form \\[ \\begin{align} p(\\mathbf{y}) &amp;= p(\\mathbf{x}) |\\mathbf{J}| &amp;&amp;\\scriptstyle{(\\text{where }\\mathbf{J}=\\frac{d\\mathbf{x}}{d\\mathbf{y}} \\text{, and since }\\frac{d\\mathbf{y}}{d\\mathbf{x}}=\\mathbf{U}^T \\text{, we have }\\mathbf{J}=\\mathbf{U}^{-T}=\\mathbf{U})} \\\\ &amp;=p(\\mathbf{x}) &amp;&amp;\\scriptstyle{( |\\mathbf{J}|^2=|\\mathbf{U}|^2=|\\mathbf{U}||\\mathbf{U}|^T = |\\mathbf{U}\\mathbf{U}^T|=|\\mathbf{I}| = 1\\text{, thus } |\\mathbf{J}|=1)} \\\\ &amp;= \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\exp\\left\\{ - \\sum_{i=1}^{D} \\frac{y_i^2}{2\\lambda_i} \\right\\}&amp; &amp;\\scriptstyle{(\\text{substituting by } \\Delta^2 =\\sum_{i=1}^{D} y_i^2/\\lambda_i )} \\\\ &amp;= \\prod_{i=1}^D\\frac{1}{\\sqrt{2\\pi\\lambda_i}} \\exp\\left\\{ -\\frac{y_i^2}{2\\lambda_i} \\right\\} &amp;&amp; \\scriptstyle{(|\\mathbf{\\Sigma}| = |\\mathbf{U}||\\Lambda||\\mathbf{U}^T| = |\\Lambda|=\\lambda_1\\lambda_2\\cdots\\lambda_D)} \\end{align} \\] so till now, we transform the original multivariate Gaussian into the product of \\(D\\) independent univariate Gaussian. Mean and Covariance The mean of Gaussian can be easily derived \\[ \\begin{align} \\mathbb{E}[\\mathbf{x}] &amp;= \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\int\\exp\\left\\{ -\\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) \\right\\} \\ \\mathbf{x}\\ d\\mathbf{x} \\\\ &amp;= \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\int\\exp\\left\\{ -\\frac{1}{2} \\mathbf{z}^T\\mathbf{\\Sigma}^{-1}\\mathbf{z}\\right\\} \\ (\\mathbf{z}+\\boldsymbol{\\mu})\\ d\\mathbf{z} &amp;&amp; \\scriptstyle{(\\text{define } \\mathbf{z}=\\mathbf{x}- \\boldsymbol{\\mu})} \\\\ &amp;= \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\int\\exp\\left\\{ -\\frac{1}{2} \\mathbf{z}^T\\mathbf{\\Sigma}^{-1}\\mathbf{z}\\right\\} \\ \\boldsymbol{\\mu}\\ d\\mathbf{z} &amp;&amp; \\scriptstyle{(\\text{for any even function }f(x),\\ \\int xf(x)dx=0)} \\\\ &amp;= \\boldsymbol{\\mu} &amp;&amp; \\scriptstyle{(\\text{using the fact that any distribution is normalized.})} \\end{align} \\] To derive \\(\\mathrm{cov}[\\mathbf{x}]\\), we need first derive \\(\\mathbb{E}[\\mathbf{xx}^T]\\), somewhat tedious \\[ \\begin{aligned} \\mathbb{E}[\\mathbf{x}\\mathbf{x}^T] &amp;= \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\int \\exp\\left\\{ -\\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) \\right\\} \\mathbf{x}\\mathbf{x}^T \\mathrm{d}\\mathbf{x}\\\\ &amp;= \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\int \\exp\\left\\{ -\\frac{1}{2} \\mathbf{z}^T\\mathbf{\\Sigma}^{-1}\\mathbf{z}\\right\\} (\\mathbf{z}+\\boldsymbol{\\mu})(\\mathbf{z}+\\boldsymbol{\\mu})^T \\mathrm{d}\\mathbf{z}\\ \\ \\ \\ \\scriptstyle{(\\text{let }\\mathbf{z}=\\mathbf{x}-\\boldsymbol{\\mu})} \\\\ &amp;= \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\int \\exp\\left\\{ -\\frac{1}{2} \\mathbf{z}^T\\mathbf{\\Sigma}^{-1}\\mathbf{z}\\right\\} (\\mathbf{z}\\mathbf{z}^T + \\boldsymbol{\\mu}\\mathbf{z}^T +\\mathbf{z}\\boldsymbol{\\mu}^T +\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T) \\mathrm{d}\\mathbf{z}\\\\ &amp;= \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\int \\exp\\left\\{ -\\frac{1}{2} \\mathbf{z}^T\\mathbf{\\Sigma}^{-1}\\mathbf{z}\\right\\} \\mathbf{z}\\mathbf{z}^T \\mathrm{d}\\mathbf{z} \\\\ &amp;\\ \\ \\ \\ \\scriptstyle{(\\text{use the fact that Gaussian is normalized and } \\exp\\{ -\\frac{1}{2} \\mathbf{z}^T\\mathbf{\\Sigma}^{-1}\\mathbf{z}\\} \\text{ is an even function for any } z_i)} \\\\ &amp;= \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\int \\exp\\left\\{ -\\frac{1}{2} \\sum_{k=1}^{D} \\frac{1}{\\lambda_k}\\mathbf{z}^T\\mathbf{u}_k \\mathbf{u}_k^T\\mathbf{z}\\right\\} \\mathbf{z}\\mathbf{z}^T \\mathrm{d}\\mathbf{z} \\ \\ \\ \\ \\scriptstyle{( \\mathbf{\\Sigma}^{-1} = \\sum_{k=1}^D\\frac{1}{\\lambda_k}\\mathbf{u}_k\\mathbf{u}_k^T)} \\\\ &amp;= \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\int \\exp\\left\\{ - \\sum_{k=1}^{D} \\frac{y_k^2}{2\\lambda_k} \\right\\} \\mathbf{U}\\mathbf{y}\\mathbf{y}^T\\mathbf{U}^T\\left||\\mathbf{U}|\\right|\\mathrm{d}\\mathbf{y} \\\\ &amp; \\ \\ \\ \\ \\scriptstyle{(\\text{let }y_k=\\mathbf{u}_k^T\\mathbf{z}\\text{, i.e. }\\mathbf{y}=\\mathbf{U}^T}\\mathbf{z}\\text{, where }\\mathbf{U}=(\\mathbf{u}_1,...,\\mathbf{u}_D)\\text{, and we have known that } |\\mathbf{U}|=|\\mathbf{U}^T|=1) \\\\ &amp;= \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\sum_{i=1}^{D} \\sum_{j=1}^{D}\\left( \\mathbf{u}_i \\mathbf{u}_j^T\\int \\exp\\left\\{ \\sum_{k=1}^{D} \\frac{y_k^2}{2\\lambda_k} \\right\\} y_i y_j \\mathrm{d}\\mathbf{y}\\right) \\\\ &amp;= \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\sum_{i=1}^{D} \\left( \\mathbf{u}_i \\mathbf{u}_i^T\\int \\exp\\left\\{ - \\sum_{k=1}^{D} \\frac{y_k^2}{2\\lambda_k} \\right\\} y_i^2 \\mathrm{d}\\mathbf{y}\\right) \\\\ &amp; \\ \\ \\ \\ \\scriptstyle{(\\text{note that }\\int \\exp\\left\\{ -\\frac{y_i^2}{2\\lambda_i} \\right\\} y_i\\mathrm{d}y_i = 0, \\text{ so when } i\\neq j,\\ \\int \\exp\\left\\{ - \\sum_{k=1}^{D} \\frac{y_k^2}{2\\lambda_k} \\right\\} y_i y_j \\mathrm{d}\\mathbf{y}= 0)} \\\\ &amp;= \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\sum_{i=1}^{D} \\left( \\mathbf{u}_i \\mathbf{u}_i^T\\int \\exp\\left\\{ - \\frac{y_i^2}{2\\lambda_i} \\right\\} y_i^2\\ \\mathrm{d}y_i \\prod_{k\\neq i} \\int \\exp\\left\\{ - \\frac{y_k^2}{2\\lambda_k} \\right\\} \\mathrm{d}y_k \\right) \\\\ &amp;= \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\sum_{i=1}^{D} \\left( \\mathbf{u}_i \\mathbf{u}_i^T \\frac{1}{(2\\pi\\lambda_i)^{1/2}} \\int \\exp\\left\\{ - \\frac{y_i^2}{2\\lambda_i} \\right\\} y_i^2\\ \\mathrm{d}y_i \\prod_{k\\neq i} \\frac{1}{(2\\pi\\lambda_k)^{1/2}} \\int \\exp\\left\\{ - \\frac{y_k^2}{2\\lambda_k} \\right\\} \\mathrm{d}y_k \\right) \\\\ &amp;\\ \\ \\ \\ \\scriptstyle{(\\text{use the fact that Gaussian is normalized, and for } y_i\\sim\\mathcal{N}(0, \\lambda_i)\\text{, we have } \\mathbb{E}[y_i^2] = \\lambda_i)} \\\\ &amp;= \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\sum_{i=1}^{D} \\lambda_i \\mathbf{u}_i \\mathbf{u}_i^T \\\\ &amp;= \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\mathbf{\\Sigma}\\end{aligned} \\] Then the covariance can be easily obtained by \\[ \\text{cov}[\\mathbf{x}] = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^T] - \\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{x}^T] = \\mathbb{E}[\\mathbf{x}\\mathbf{x}^T] - \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T = \\mathbf{\\Sigma} \\] Limitation of Gaussian Although the Gaussian distribution is widely used as a density model, it suffers from some significant limitations Hard to compute \\(\\mathbf{\\Sigma}^{-1}\\). Possible solution is to restrict \\(\\mathbf{\\Sigma}\\) to be diagonal (\\(\\mathbf{\\Sigma}=\\mathrm{diag}(\\sigma^2_i)\\)) or even isotropic (\\(\\mathbf{\\Sigma}=\\sigma^2\\mathbf{I}\\)). Unfortunately, these methods greatly restrict the form of the probability density and limit its ability to capture interesting correlations in the data. Intrinsically unimodal. Thus the Gaussian distribution can be both too flexible, in the sense of having too many parameters so that hard to compute \\(\\mathbf{\\Sigma}^{-1}​\\), while also being too limited in the range of distributions that it can adequately represent. Fortunately, in later chapters, the introduction of latent variables, also called hidden variables or unobserved variables, allows both of these problems to be addressed. Gaussian with covariance matrix in (a) general form, (b) diagonal form, and (c) isotropic form.","link":"/old/PRML2-3-0.html"},{"title":"(PRML Notes) 2.3.1-3 Conditional and Marginal Gaussian","text":"A series of notes taken from Pattern Recognition and Machine Learning. If two sets of variables are jointly Gaussian, then the conditional distribution of one conditioned on the other is again Gaussian, and the marginal of either set is also Gaussian. Conditional Gaussian If we partition \\(\\mathbf{x}\\) into two disjoint subsets \\(\\mathbf{x}_a\\) and \\(\\mathbf{x}_b\\), w.l.o.g. we can take \\(\\mathbf{x}_a\\) to be the first \\(M\\) components of \\(\\mathbf{x}\\), then correspondingly we have \\[ \\mathbf{x} = \\left(\\matrix{\\mathbf{x}_a\\\\\\mathbf{x}_b}\\right) \\ \\ \\boldsymbol{\\mu} = \\left(\\matrix{\\boldsymbol{\\mu}_a\\\\\\boldsymbol{\\mu}_b}\\right) \\ \\ \\mathbf{\\Sigma} = \\left(\\matrix{\\mathbf{\\Sigma}_{aa}&amp;\\mathbf{\\Sigma}_{ab}\\\\\\mathbf{\\Sigma}_{ba} &amp; \\mathbf{\\Sigma}_{bb}}\\right) \\ \\ \\mathbf{\\Lambda} = \\left(\\matrix{\\mathbf{\\Lambda}_{aa}&amp;\\mathbf{\\Lambda}_{ab}\\\\\\mathbf{\\Lambda}_{ba} &amp; \\mathbf{\\Lambda}_{bb}}\\right) \\] where \\(\\mathbf{\\Lambda}\\equiv \\mathbf{\\Sigma}^{-1}\\) is known as the precision matrix. Note that \\(\\mathbf{\\Sigma}_{ba} = \\mathbf{\\Sigma}_{ab}^T\\) and \\(\\mathbf{\\Lambda}_{ba} = \\mathbf{\\Lambda}_{ba}^T\\) by the fact that the inverse of a symmetric matrix is also symmetric. Also note that in most cases \\(\\mathbf{\\Lambda}_{aa} \\neq \\mathbf{\\Sigma}_{aa}\\). The conditional distribution can be evaluated from \\(p(\\mathbf{x}_a|\\mathbf{x}_b) = p(\\mathbf{x}_a, \\mathbf{x}_b)/p(\\mathbf{x}_b)\\) with \\(\\mathbf{x}_b\\) fixed. It can be easily inferred that \\(p(\\mathbf{x}_a|\\mathbf{x}_b)\\) remains to be Gaussian since the quadratic form in the exponent of \\(p(\\mathbf{x}_a, \\mathbf{x}_b)\\) remains to be quadratic w.r.t. \\(\\mathbf{x}_a\\) in \\(p(\\mathbf{x}_a|\\mathbf{x}_b)\\) \\[ \\begin{align} -\\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}- \\boldsymbol{\\mu})=&amp;-\\frac{1}{2} (\\mathbf{x}_a- \\boldsymbol{\\mu}_a)^T\\mathbf{\\Lambda}_{aa}(\\mathbf{x}_a- \\boldsymbol{\\mu}_a) -\\frac{1}{2} (\\mathbf{x}_a- \\boldsymbol{\\mu}_a)^T\\mathbf{\\Lambda}_{ab}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b) \\\\ &amp;-\\frac{1}{2} (\\mathbf{x}_b- \\boldsymbol{\\mu}_b)^T\\mathbf{\\Lambda}_{ba}(\\mathbf{x}_a- \\boldsymbol{\\mu}_a) -\\frac{1}{2} (\\mathbf{x}_b- \\boldsymbol{\\mu}_b)^T\\mathbf{\\Lambda}_{bb}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b) \\\\ =&amp; -\\frac{1}{2} \\mathbf{x}_a^T\\mathbf{\\Lambda}_{aa}\\mathbf{x}_a + \\mathbf{x}_a^T[\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a -\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b)] + \\text{const} \\end{align} \\] By comparison to the standard quadratic form \\[ -\\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}- \\boldsymbol{\\mu}) = -\\frac{1}{2} \\mathbf{x}^T\\mathbf{\\Sigma}^{-1}\\mathbf{x} + \\mathbf{x}^T\\mathbf{\\Sigma}^{-1}\\boldsymbol{\\mu} + \\text{const} \\] We get \\[ \\color{red}{ \\begin{align} \\mathbf{\\Sigma}_{a|b} &amp;= \\mathbf{\\Lambda}_{aa}^{-1} \\\\ \\boldsymbol{\\mu}_{a|b} &amp; \\color{}= \\mathbf{\\Sigma}_{a|b}[\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a -\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b)] = \\boldsymbol{\\mu}_a -\\mathbf{\\Lambda}_{aa}^{-1}\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b) \\end{align} } \\] We can see that \\(\\mathbf{\\Sigma}_{a|b}​\\) is independent of \\(\\mathbf{x}_b​\\), while \\(\\boldsymbol{\\mu}_{a|b}​\\) is a linear function of \\(\\mathbf{x}_b​\\), which represents an example of a linear-Gaussian model. To represent these results in terms of the partitioned covariance matrix, first consider that \\[ {\\left(\\matrix{ \\mathbf{A}&amp;\\mathbf{B}\\\\\\mathbf{C}&amp;\\mathbf{D} }\\right)^{-1}} = {\\left(\\matrix{ \\mathbf{M}&amp;\\mathbf{-MBD^{-1}}\\\\-\\mathbf{D}^{-1}\\mathbf{CM}&amp;\\mathbf{D}^{-1}+\\mathbf{D}^{-1}\\mathbf{CMBD}^{-1} }\\right)}\\tag{1} \\] where \\(\\mathbf{M} = (\\mathbf{A} - \\mathbf{BD}^{-1}\\mathbf{C})^{-1}\\). Thus from \\[ {\\left(\\matrix{ \\mathbf{\\Sigma}_{aa}&amp;\\mathbf{\\Sigma}_{ab}\\\\\\mathbf{\\Sigma}_{ba}&amp;\\mathbf{\\Sigma}_{bb} }\\right)^{-1}} = \\left(\\matrix{\\mathbf{\\Lambda}_{aa}&amp;\\mathbf{\\Lambda}_{ab}\\\\\\mathbf{\\Lambda}_{ba} &amp; \\mathbf{\\Lambda}_{bb}}\\right) \\] we can derive that \\[ \\begin{align} \\mathbf{\\Lambda}_{aa} &amp;= (\\mathbf{\\Sigma}_{aa} - \\mathbf{\\Sigma}_{ab}\\mathbf{\\Sigma}_{bb}^{-1}\\mathbf{\\Sigma}_{ba})^{-1} \\\\ \\mathbf{\\Lambda}_{ab} &amp;= -(\\mathbf{\\Sigma}_{aa} - \\mathbf{\\Sigma}_{ab}\\mathbf{\\Sigma}_{bb}^{-1}\\mathbf{\\Sigma}_{ba})^{-1}\\mathbf{\\Sigma}_{ab}\\mathbf{\\Sigma}_{bb}^{-1} \\end{align} \\] Replace them into mean and covariance we have \\[ \\begin{align} \\mathbf{\\Sigma}_{a|b} &amp;= \\mathbf{\\Sigma}_{aa} - \\mathbf{\\Sigma}_{ab}\\mathbf{\\Sigma}_{bb}^{-1}\\mathbf{\\Sigma}_{ba} \\\\ \\boldsymbol{\\mu}_{a|b} &amp;= \\boldsymbol{\\mu}_a +\\mathbf{\\Sigma}_{ab}\\mathbf{\\Sigma}_{bb}^{-1}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b) \\end{align} \\] Marginal Gaussian In the textbook, the author uses integration to derive marginal Gaussian distribution. However, with the help of the conditional Gaussian that we have derived before, the derivation can be done in a simpler way by using \\(p(\\mathbf{x}_b)= p(\\mathbf{x}_a, \\mathbf{x}_b)/p(\\mathbf{x}_a|\\mathbf{x}_b)​\\) Remember that the quadratic form in the exponent of \\(p(\\mathbf{x}_a, \\mathbf{x}_b)​\\) takes the form \\[ \\begin{align} -\\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}- \\boldsymbol{\\mu})=&amp;-\\frac{1}{2} (\\mathbf{x}_a- \\boldsymbol{\\mu}_a)^T\\mathbf{\\Lambda}_{aa}(\\mathbf{x}_a- \\boldsymbol{\\mu}_a) -\\frac{1}{2} (\\mathbf{x}_a- \\boldsymbol{\\mu}_a)^T\\mathbf{\\Lambda}_{ab}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b) \\\\ &amp;-\\frac{1}{2} (\\mathbf{x}_b- \\boldsymbol{\\mu}_b)^T\\mathbf{\\Lambda}_{ba}(\\mathbf{x}_a- \\boldsymbol{\\mu}_a) \\color{blue}{-\\frac{1}{2} (\\mathbf{x}_b- \\boldsymbol{\\mu}_b)^T\\mathbf{\\Lambda}_{bb}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b)} \\end{align} \\] While the the quadratic form in the exponent of \\(p(\\mathbf{x}_a|\\mathbf{x}_b)​\\) takes the form \\[ \\begin{align} &amp;-\\frac{1}{2} (\\mathbf{x}_a- \\boldsymbol{\\mu}_a +\\mathbf{\\Lambda}_{aa}^{-1}\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b) )^T\\mathbf{\\Lambda}_{aa}(\\mathbf{x}_a- \\boldsymbol{\\mu}_a +\\mathbf{\\Lambda}_{aa}^{-1}\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b)) \\\\ =&amp;-\\frac{1}{2} (\\mathbf{x}_a- \\boldsymbol{\\mu}_a)^T\\mathbf{\\Lambda}_{aa}(\\mathbf{x}_a- \\boldsymbol{\\mu}_a) -\\frac{1}{2} (\\mathbf{x}_a- \\boldsymbol{\\mu}_a)^T\\mathbf{\\Lambda}_{ab}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b) \\\\ &amp;-\\frac{1}{2} (\\mathbf{x}_b- \\boldsymbol{\\mu}_b)^T\\mathbf{\\Lambda}_{ba}(\\mathbf{x}_a- \\boldsymbol{\\mu}_a) \\color{blue}{-\\frac{1}{2} (\\mathbf{x}_b- \\boldsymbol{\\mu}_b)^T\\mathbf{\\Lambda}_{ba}\\mathbf{\\Lambda}_{aa}^{-1}\\mathbf{\\Lambda}_{ab}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b)} \\end{align} \\] We see that the exponents only differ in the last term. So the exponent of \\(p(\\mathbf{x}_b)= p(\\mathbf{x}_a, \\mathbf{x}_b)/p(\\mathbf{x}_a|\\mathbf{x}_b)\\) remains quadratic and will be \\[ -\\frac{1}{2} (\\mathbf{x}_b- \\boldsymbol{\\mu}_b)^T(\\mathbf{\\Lambda}_{bb}-\\mathbf{\\Lambda}_{ba}\\mathbf{\\Lambda}_{aa}^{-1}\\mathbf{\\Lambda}_{ab})(\\mathbf{x}_b- \\boldsymbol{\\mu}_b) = -\\frac{1}{2} (\\mathbf{x}_b- \\boldsymbol{\\mu}_b)^T\\mathbf{\\Sigma}_{bb}^{-1}(\\mathbf{x}_b- \\boldsymbol{\\mu}_b) \\] where we use the fact \\(\\mathbf{\\Sigma}_{bb} = (\\mathbf{\\Lambda}_{bb}-\\mathbf{\\Lambda}_{ba}\\mathbf{\\Lambda}_{aa}^{-1}\\mathbf{\\Lambda}_{ab})^{-1}​\\). So the marginal distribution \\(p(\\mathbf{x}_b)​\\) is also Gaussian that takes a very simple form where \\[ \\color{red}{ \\begin{align} \\mathbb{E}[\\mathbf{x}_b] &amp;= \\boldsymbol{\\mu}_b \\\\ \\mathrm{cov}[\\mathbf{x}_b] &amp;= \\boldsymbol{\\Sigma}_{bb} \\end{align} } \\] red: conditional, blue: marginal. Bayes' Theorem for Gaussian Vairables Having found the marginal \\(p(\\mathbf{x})​\\) as a prior and the conditional \\(p(\\mathbf{y|x})​\\) as likelihood, how can we infer the posterior \\(p(\\mathbf{x|y})​\\) together with \\(p(\\mathbf{y})​\\)? Assume the marginal and the conditional distributions take the form \\[ p(\\mathbf{x}) =\\mathcal{N} (\\mathbf{x}|\\boldsymbol{\\mu}, \\mathbf{\\Lambda}^{-1}) \\\\ p(\\mathbf{y|x})= \\mathcal{N} (\\mathbf{y}|\\mathbf{Ax+b}, \\mathbf{L}^{-1}) \\] where \\(\\mathbf{\\Lambda}​\\), \\(\\mathbf{L}​\\) are precision matrices, and we used the fact that \\(\\boldsymbol{\\mu}_{\\mathbf{y|x}}​\\) is a linear function of \\(\\mathbf{x}​\\). To derive the joint distribution \\(p(\\mathbf{x},\\mathbf{y})​\\), consider the log of it \\[ \\begin{align} \\ln p(\\mathbf{z}) &amp;= \\ln p(\\mathbf{x}) + \\ln p(\\mathbf{y|x}) \\\\ &amp;= -\\frac{1}{2} (\\mathbf{x}- \\boldsymbol{\\mu})^T\\mathbf{\\Lambda}(\\mathbf{x}- \\boldsymbol{\\mu}) - \\frac{1}{2} (\\mathbf{y}-\\mathbf{Ax-b})^T\\mathbf{L}(\\mathbf{y}-\\mathbf{Ax-b}) + \\text{const} \\\\ &amp;= \\underbrace{\\left(-\\frac{1}{2} \\mathbf{x}^T\\mathbf{\\Lambda}\\mathbf{x}-\\frac{1}{2} \\mathbf{y}^T\\mathbf{L}\\mathbf{y}-\\frac{1}{2} \\mathbf{x}^T\\mathbf{A}^T\\mathbf{LA}\\mathbf{x}+ \\mathbf{y}^T\\mathbf{LA}\\mathbf{x}\\right)}_\\text{quadratic term} + \\underbrace{\\left(\\mathbf{x}^T\\mathbf{\\Lambda}\\boldsymbol{\\mu} + \\mathbf{y}^T\\mathbf{L}\\mathbf{b} - \\mathbf{x}^T\\mathbf{A}^T\\mathbf{L}\\mathbf{b}\\right)}_\\text{linear term} \\\\ &amp;= -\\frac{1}{2} \\mathbf{z}^T \\left(\\matrix{\\mathbf{\\Lambda}+\\mathbf{A}^T\\mathbf{LA} &amp; -\\mathbf{A}^T\\mathbf{L} \\\\ -\\mathbf{L}\\mathbf{A} &amp; \\mathbf{L}}\\right) \\mathbf{z} + \\mathbf{z}^T \\left(\\matrix{\\mathbf{\\Lambda}\\boldsymbol{\\mu} - \\mathbf{A}^T\\mathbf{L}\\mathbf{b} \\\\ \\mathbf{L}\\mathbf{b}}\\right) \\\\ &amp;\\equiv -\\frac{1}{2} \\mathbf{z}^T \\mathbf{R} \\mathbf{z} + \\mathbf{z}^T \\left(\\matrix{\\mathbf{\\Lambda}\\boldsymbol{\\mu} - \\mathbf{A}^T\\mathbf{L}\\mathbf{b} \\\\ \\mathbf{L}\\mathbf{b}}\\right) \\end{align} \\] where we define \\(\\mathbf{z}\\equiv(\\mathbf{x \\ \\ y})^T\\). Then we get \\[ \\begin{align} \\mathrm{cov}[\\mathbf{z}] &amp;= \\mathbf{R}^{-1} = \\left(\\matrix{\\mathbf{\\Lambda}^{-1} &amp; \\mathbf{\\Lambda}^{-1}\\mathbf{A}^T \\\\ \\mathbf{A}\\mathbf{\\Lambda}^{-1} &amp; \\mathbf{L}^{-1}+\\mathbf{A}\\mathbf{\\Lambda}^{-1}\\mathbf{A}^T}\\right) &amp;&amp;\\scriptstyle{\\text{(using equation (1))}} \\\\ \\mathbb{E}[\\mathbf{z}] &amp;= \\mathbf{R}^{-1}\\left(\\matrix{\\mathbf{\\Lambda}\\boldsymbol{\\mu} - \\mathbf{A}^T\\mathbf{L}\\mathbf{b} \\\\ \\mathbf{L}\\mathbf{b}}\\right) = \\left(\\matrix{\\boldsymbol{\\mu} \\\\ \\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b}}\\right) \\end{align} \\] Then it's easy to derive the mean and covariance of the Gaussian \\(p(\\mathbf{y})​\\) according to the result from previous sections \\[ \\color{red}{ \\begin{align} \\mathbb{E}[\\mathbf{y}] &amp;= \\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b} \\\\ \\mathrm{cov}[\\mathbf{y}] &amp;= \\mathbf{L}^{-1}+\\mathbf{A}\\mathbf{\\Lambda}^{-1}\\mathbf{A}^T \\end{align} } \\] and for \\(p(\\mathbf{x|y})\\) \\[ \\color{red}{ \\begin{align} \\mathbb{E}[\\mathbf{x|y}] &amp;= (\\mathbf{\\Lambda}+\\mathbf{A}^T\\mathbf{LA})^{-1}[\\mathbf{A}^T\\mathbf{L}(\\mathbf{y}-\\mathbf{b})+\\mathbf{\\Lambda}\\boldsymbol{\\mu}] \\\\ \\mathrm{cov}[\\mathbf{x|y}] &amp;= (\\mathbf{\\Lambda}+\\mathbf{A}^T\\mathbf{LA})^{-1} \\end{align} } \\]","link":"/old/PRML2-3-1~3.html"},{"title":"(PRML Notes) 2.3.4-5 Frequentist Estimate for Gaussian","text":"A series of notes taken from Pattern Recognition and Machine Learning. Maximum Likelihood for Gaussian Given a dataset \\(\\mathbf{X} = (\\mathbf{x}_1,...,\\mathbf{N})^T\\) where \\(\\mathbf{x_n}\\) are drawn independently from a multivariate Gaussian. To derive MLE, the log likelihood of the dataset is given by \\[ \\begin{align} \\ln p(\\mathbf{X}|\\boldsymbol{\\mu},\\mathbf{\\Sigma}) &amp;= \\ln \\prod_{n=1}^{N}\\frac{1}{(2\\pi)^{D/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\exp\\left\\{ -\\frac{1}{2} (\\mathbf{x}_n- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_n- \\boldsymbol{\\mu}) \\right\\} \\\\ &amp;= -\\frac{ND}{2}\\ln (2\\pi) - \\frac{N}{2}\\ln|\\mathbf{\\Sigma}| - \\frac{1}{2}\\sum_{n=1}^N (\\mathbf{x}_n- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_n- \\boldsymbol{\\mu}) \\end{align} \\] First take the derivative w.r.t. \\(\\boldsymbol{\\mu}\\) \\[ \\frac{\\partial}{\\partial\\boldsymbol{\\mu}} \\ln p(\\mathbf{X}|\\boldsymbol{\\mu},\\mathbf{\\Sigma}) = \\sum_{n=1}^N \\mathbf{\\Sigma}^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}) \\] By setting it to zero we get \\[ \\color{red}{\\boldsymbol{\\mu}_{\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^N \\mathbf{x}_n} \\] Some useful matrix derivative that will be used in deriving \\(\\mathbf{\\Sigma}_{\\mathrm{ML}}\\): \\[ \\frac{\\partial |\\mathbf{X}|}{\\partial \\mathbf{X}} = |\\mathbf{X}| \\mathbf{X}^{-T} \\\\ \\frac{\\partial \\mathbf{a}^T\\mathbf{Xb}}{\\partial \\mathbf{X}} = \\mathbf{a} \\mathbf{b}^T \\\\ \\frac{\\partial \\mathbf{a}^T\\mathbf{X^{-1}b}}{\\partial \\mathbf{X}} = -\\mathbf{X}^{-T}\\mathbf{a} \\mathbf{b}^T\\mathbf{X}^{-T} \\] Next take the derivative w.r.t. \\(\\mathbf{\\Sigma}​\\) \\[ \\frac{\\partial}{\\partial\\mathbf{\\Sigma}} \\ln p(\\mathbf{X}|\\boldsymbol{\\mu},\\mathbf{\\Sigma}) = - \\frac{N}{2}\\mathbf{\\Sigma}^{-T} + \\frac{1}{2} \\sum_{n=1}^{N} \\mathbf{\\Sigma}^{-T}(\\mathbf{x}_n- \\boldsymbol{\\mu})(\\mathbf{x}_n- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-T} \\] By setting it to zero we get \\[ \\begin{align} &amp;&amp;\\frac{N}{2}\\mathbf{\\Sigma}^{-T} &amp;= \\frac{1}{2} \\sum_{n=1}^{N} \\mathbf{\\Sigma}^{-T}(\\mathbf{x}_n- \\boldsymbol{\\mu})(\\mathbf{x}_n- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-T} \\\\ \\Rightarrow &amp;&amp;N &amp;= \\sum_{n=1}^{N} \\mathbf{\\Sigma}^{-T}(\\mathbf{x}_n- \\boldsymbol{\\mu})(\\mathbf{x}_n- \\boldsymbol{\\mu})^T \\\\ \\Rightarrow &amp;&amp;\\mathbf{\\Sigma}^T &amp;= \\frac{1}{N}\\sum_{n=1}^{N} (\\mathbf{x}_n- \\boldsymbol{\\mu})(\\mathbf{x}_n- \\boldsymbol{\\mu})^T = \\mathbf{\\Sigma} \\end{align} \\] Note that the solution of \\(\\boldsymbol{\\mu}_{\\mathrm{ML}}​\\) is independent of \\(\\boldsymbol{\\mathbf{\\Sigma}}_{\\mathrm{ML}}​\\), so we can first evaluate \\(\\boldsymbol{\\mu}_{\\mathrm{ML}}​\\) and then use this to get \\[ \\color{red}{\\mathbf{\\Sigma}_{\\mathrm{ML}} = \\frac{1}{N}\\sum_{n=1}^{N} (\\mathbf{x}_n- \\boldsymbol{\\mu}_{\\mathrm{ML}})(\\mathbf{x}_n- \\boldsymbol{\\mu}_{\\mathrm{ML}})^T} \\] Also note that we ignored the symmetric constraint on \\(\\mathbf{\\Sigma}\\) during derivation while the resulting solution is indeed symmetric as required. If we further evaluate the expectations of MLE results under the true distribution we can get \\[ \\begin{align} \\mathbb{E}[\\boldsymbol{\\mu}_{\\mathrm{ML}}] &amp;= \\boldsymbol{\\mu} &amp;&amp; \\text{(unbiased)} \\\\ \\mathbb{E}[\\mathbf{\\Sigma}_{\\mathrm{ML}}] &amp;= \\frac{N-1}{N} \\mathbf{\\Sigma} &amp;&amp; \\text{(biased)} \\end{align} \\] \\(\\mathbb{E}[\\boldsymbol{\\mu}_{\\mathrm{ML}}]\\) can be calculated straightforward. As for \\(\\mathbb{E}[\\mathbf{\\Sigma}_{\\mathrm{ML}}]​\\) \\[ \\begin{align} \\mathbb{E}[\\mathbf{\\Sigma}_{\\mathrm{ML}}] &amp;= \\frac{1}{N}\\sum_{n=1}^{N} \\mathbb{E}[(\\mathbf{x}_n- \\boldsymbol{\\mu}_{\\mathrm{ML}})(\\mathbf{x}_n- \\boldsymbol{\\mu}_{\\mathrm{ML}})^T] \\\\ &amp;= \\frac{1}{N}\\sum_{n=1}^{N}\\mathbb{E}\\left[ \\left(\\mathbf{x}_n- \\frac{1}{N}\\sum_{k=1}^N \\mathbf{x}_k\\right)\\left(\\mathbf{x}_n- \\frac{1}{N}\\sum_{k=1}^N \\mathbf{x}_k\\right)^T \\right] \\\\ &amp;= \\frac{1}{N^3}\\sum_{n=1}^{N}\\mathbb{E}\\left[ \\left(N\\mathbf{x}_n-\\sum_{k=1}^N \\mathbf{x}_k\\right)\\left(N\\mathbf{x}_n- \\sum_{k=1}^N \\mathbf{x}_k\\right)^T \\right] \\\\ &amp;= \\frac{1}{N^3}\\sum_{n=1}^{N}\\mathbb{E}\\left[ \\left((N-1)\\mathbf{x}_n-\\sum_{k\\neq n} \\mathbf{x}_k\\right)\\left((N-1)\\mathbf{x}_n-\\sum_{k\\neq n} \\mathbf{x}_k\\right)^T \\right] \\\\ &amp;= \\frac{1}{N^3}\\sum_{n=1}^{N}\\mathbb{E}\\left[(N-1)^2\\mathbf{x}_n\\mathbf{x}_n^T -(N-1)\\sum_{k\\neq n} \\mathbf{x}_n\\mathbf{x}_k^T -(N-1)\\sum_{k\\neq n} \\mathbf{x}_k\\mathbf{x}_n^T + \\left(\\sum_{k\\neq n} \\mathbf{x}_k\\right)\\left(\\sum_{k\\neq n} \\mathbf{x}_k^T\\right) \\right] \\\\ &amp; \\ \\ \\ \\ \\scriptstyle{(\\text{Recall that }\\mathbb{E}[\\mathbf{x}_i\\mathbf{x}_j^T] = \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\mathbf{\\Sigma},\\ \\forall \\ i\\neq j, \\text{ and }\\mathbb{E}[\\mathbf{x}_i\\mathbf{x}_j^T] = \\boldsymbol{\\mu}\\boldsymbol{\\mu}^T,\\ \\text{if} \\ i= j)} \\\\ &amp;= \\frac{1}{N^3}\\sum_{n=1}^{N}\\left[(N-1)^2(\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\mathbf{\\Sigma}) -2(N-1)^2\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + \\left((N-1)^2\\boldsymbol{\\mu}\\boldsymbol{\\mu}^T + (N-1)\\mathbf{\\Sigma}\\right)\\right] \\\\ &amp;= \\frac{1}{N^3}\\sum_{n=1}^{N}N(N-1)\\mathbf{\\Sigma} \\\\ &amp;= \\frac{N-1}{N}\\mathbf{\\Sigma} \\end{align} \\] We can correct the bias of \\(\\mathbb{E}[\\mathbf{\\Sigma}_{\\mathrm{ML}}]​\\) by using \\(\\tilde{\\mathbf{\\Sigma}}​\\) instead \\[ \\tilde{\\mathbf{\\Sigma}} = \\frac{1}{N-1}\\sum_{n=1}^{N} (\\mathbf{x}_n- \\boldsymbol{\\mu}_{\\mathrm{ML}})(\\mathbf{x}_n- \\boldsymbol{\\mu}_{\\mathrm{ML}})^T \\] Sequential Estimation Sequential estimation allows data points to be processed one at a time and then discarded. It is important for on-line applications. A Naive Approach \\[ \\begin{align} \\boldsymbol{\\mu}_{\\mathrm{ML}}^{(N)} &amp;= \\frac{1}{N}\\sum_{n=1}^N \\mathbf{x}_n \\\\ &amp;= \\frac{1}{N}\\mathbf{x}_N + \\frac{1}{N}\\sum_{n=1}^{N-1} \\mathbf{x}_n \\\\ &amp;= \\frac{1}{N}\\mathbf{x}_N + \\frac{N-1}{N}\\boldsymbol{\\mu}_{\\mathrm{ML}}^{(N-1)} \\\\ &amp;= \\boldsymbol{\\mu}_{\\mathrm{ML}}^{(N-1)} + \\frac{1}{N}(\\mathbf{x}_N -\\boldsymbol{\\mu}_{\\mathrm{ML}}^{(N-1)}) \\end{align} \\] where \\(\\boldsymbol{\\mu}_{\\mathrm{ML}}^{(N)}\\) is the MLE for \\(\\boldsymbol{\\mu}_{\\mathrm{ML}}\\) based on \\(N\\) observations. However, we will not always be able to derive such a sequential algorithm by this route. Robbins-Monro Algorithm Here comes a general formulation of sequential learning. Consider a pair of random variables \\(\\theta\\) and \\(z\\) governed by a joint distribution \\(p(z, \\theta)\\). And a deterministic function \\(f(\\theta)\\), which is called a regression function, is defined by \\[ f(\\theta) \\equiv \\mathbb{E}[z|\\theta] = \\int zp(z|\\theta)\\ dz \\] Our goal is to find the root \\(\\theta^*\\) satisfying \\(f(\\theta^*)=0\\), i.e. \\(\\mathbb{E}[z|\\theta^*]=0\\), and we need to find a sequential estimation scheme for \\(\\theta^*\\) when the values of \\(z\\) are observed one at a time, instead of modeling the \\(f\\) directly then working out the root when a large dataset is obtained. First we shall assume that The conditional variance of \\(z\\) is finite: \\(\\mathrm{var}[z|\\theta]\\equiv\\mathbb{E}[(z-f)|\\theta]&lt;\\infty\\) w.l.o.g. \\(f(\\theta)&gt;0\\) for \\(\\theta &gt; \\theta^*\\) and \\(f(\\theta)&lt;0\\) for \\(\\theta &lt; \\theta^*\\), as in the following figure Then the Robbins-Monro algorithm defines a sequence estimate of \\(\\theta^*\\) given by \\[ \\theta^{(N)} = \\theta^{(N-1)} - a_{N-1}z(\\theta^{(N-1)}) \\] where \\(z(\\theta^{(N)})\\) is an observed value of \\(z\\) when \\(\\theta\\) takes the value \\(\\theta^{(N)}\\). Intuitively, as illustrated in the above figure, when the \\(\\theta^{(N-1)}\\) is estimated to be larger than \\(\\theta^*\\), then the corresponding \\(p(z|\\theta)\\) will assign higher probability to those \\(z(\\theta^{(N-1)})&gt;0\\), which results in \\(\\theta^{(N)}\\) shifting to \\(\\theta^*\\). The coefficients \\(\\{a_N\\}\\) represents a sequence of positive numbers that satisfy the conditions \\[ \\begin{align} \\lim_{N\\rightarrow\\infty} a_N &amp;= 0 \\\\ \\sum_{N=1}^\\infty a_N &amp;= \\infty \\\\ \\sum_{N=1}^\\infty a_N^2 &amp;&lt; \\infty \\end{align} \\] a common example is \\(a_N = 1/N\\). \\(a_N\\) is just the same concept as learning rate in deep learning literal. And we will see that the upcoming Robbins-Monro algorithm is just like the gradient ascent method that maximizing the log likelihood. Apply Robbins-Monro Algorithm to General MLE We can apply Robbins-Monro algorithm to general MLE in this form \\[ \\theta^{(N)} = \\theta^{(N-1)} - a_{N-1}\\color{blue}{\\frac{\\partial}{\\partial\\theta^{(N-1)}}\\left[ -\\ln p(x_N|\\theta^{(N-1)}) \\right]} \\] where we aim to find a maximum likelihood solution \\(\\theta_\\mathrm{ML}\\) for \\(p(x|\\theta)\\), which we assume to be unimodal. To see why, note that here we define \\(z​\\) to be the derivative of the negative log likelihood \\[ z \\equiv \\frac{\\partial}{\\partial\\theta}\\left[ -\\ln p(x|\\theta)\\right] \\] By sampling we know that now the \\(\\theta^*\\) that satisfying \\(f(\\theta^*)=0\\) is just the stationary point for MLE \\[ f(\\theta) = \\mathbb{E}[z|\\theta] = \\lim_{N\\rightarrow\\infty}\\frac{1}{N} \\sum_{n=1}^N \\frac{\\partial}{\\partial\\theta}\\left[ -\\ln p(x|\\theta)\\right] = \\frac{\\partial}{\\partial\\theta}\\left[\\lim_{N\\rightarrow\\infty}-\\frac{1}{N} \\sum_{n=1}^N \\ln p(x_n|\\theta)\\right] \\] The negative keeps the property that \\(f(\\theta)&gt;0\\) for \\(\\theta &gt; \\theta^*\\) and \\(f(\\theta)&lt;0\\) for \\(\\theta &lt; \\theta^*\\) Intuitively, from the expression \\(\\theta^{(N)} = \\theta^{(N-1)} + a_{N-1}\\frac{\\partial}{\\partial\\theta^{(N-1)}}\\left[\\ln p(x_N|\\theta^{(N-1)}) \\right]\\) we know that for every step \\(\\theta^{(N)}\\) is improved according to the gradient that maximizes \\(p(x_N|\\theta^{(N-1)})\\). Case Study: Apply to Univariate Gaussian For univariate Gaussian distribution with fixed variance, we have \\[ z = \\frac{\\partial}{\\partial\\mu_\\mathrm{ML}}\\left[ -\\ln \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{ -\\frac{(x-\\mu_\\mathrm{ML})^2}{2\\sigma^2} \\right\\}\\right] = -\\frac{x-\\mu_\\mathrm{ML}}{\\sigma^2} \\] then the sequential estimate rule becomes \\[ \\theta^{(N)} = \\theta^{(N-1)} + a_{N-1}\\frac{x-\\mu_\\mathrm{ML}}{\\sigma^2} \\] If we choose \\(a_{N-1} = \\sigma^2/N\\), we can obtain the same univariate form of the naive sequential estimate approach.","link":"/old/PRML2-3-4~5.html"},{"title":"(PRML Notes) 2.3.6 Bayesian Inference for Gaussian","text":"A series of notes taken from Pattern Recognition and Machine Learning. Now for univariate Gaussian, we use \\(\\mathtt{x} = \\{x_1,...,x_N\\}​\\) to denote the \\(N​\\) observations, and \\(\\mathbf{X}=\\{\\mathbf{x}_1,...,\\mathbf{x}_N\\}​\\) for the multivariate Gaussian. For convenience, the likelihood functions of the observed data given mean and variance are written below \\[ p(\\mathtt{x}|\\mu,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{N/2}}\\exp\\left\\{ -\\sum_{n=1}^N\\frac{(x_n-\\mu)^2}{2\\sigma^2} \\right\\} \\] \\[ p(\\mathbf{X}| \\boldsymbol{\\mu},\\mathbf{\\Sigma}) = \\frac{1}{(2\\pi)^{ND/2}} \\frac{1}{|\\mathbf{\\Sigma}|^{N/2}} \\exp\\left\\{ -\\frac{1}{2} \\sum_{n=1}^{N}(\\mathbf{x}_n- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_n- \\boldsymbol{\\mu}) \\right\\} \\] In this section a Bayesian treatment that mainly focuses on univariate case by introducing prior will be developed. Infer Mean with Known Variance Univariate Case For this case the likelihood function can be viewed as a function of \\(\\mu\\), which takes the form of a quadratic form in \\(\\mu\\) \\[ p(\\mathtt{x}|\\mu) = \\frac{1}{(2\\pi\\sigma^2)^{N/2}}\\exp\\left\\{ -\\sum_{n=1}^N\\frac{(x_n-\\mu)^2}{2\\sigma^2} \\right\\} \\] So the conjugate prior we choose should also be a Gaussian \\[ p(\\mu) = \\mathcal{N} (\\mu|\\mu_0,\\sigma_0^2) \\] The posterior therefore becomes \\[ \\begin{align} p(\\mu|\\mathtt{x}) &amp;= p(\\mu)p(\\mathtt{x}|\\mu) \\\\ &amp;\\propto \\exp \\left\\{ -\\sum_{n=1}^N\\frac{(x_n-\\mu)^2}{2\\sigma^2}-\\frac{(\\mu-\\mu_0)^2}{2\\sigma_0^2} \\right\\} \\\\ &amp;\\propto \\exp \\left\\{ -\\frac{1}{2\\sigma^2}N\\mu^2+\\frac{1}{\\sigma^2}N\\mu_\\mathrm{ML}\\mu -\\frac{1}{2\\sigma_0^2}\\mu^2 +\\frac{1}{\\sigma_0^2}\\mu_0\\mu\\right\\} &amp;&amp; \\scriptstyle{(\\mu_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^N x_n)} \\\\ &amp;= \\exp \\left\\{ -\\frac{1}{2}\\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right)\\mu^2 +\\left(\\frac{N\\mu_\\mathrm{ML}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right)\\mu \\right\\} \\\\ &amp;\\propto \\exp \\left\\{ -\\frac{1}{2}\\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right)\\left(\\mu - \\frac{\\frac{N\\mu_\\mathrm{ML}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2} }{\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}}\\right)^2 \\right\\} \\\\ &amp;= \\exp \\left\\{ -\\frac{1}{2}\\left(\\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2}\\right)\\left(\\mu - \\frac{N\\sigma_0^2\\mu_\\mathrm{ML}+\\sigma^2\\mu_0}{N\\sigma_0^2+\\sigma^2} \\right)^2 \\right\\} \\end{align} \\] namely, \\(p(\\mu|\\mathtt{x}) = \\mathcal{N}(\\mu|\\mu_N,\\sigma_N^2)\\), where \\[ \\begin{align} \\mu_N &amp;= \\frac{N\\sigma_0^2}{N\\sigma_0^2+\\sigma^2}\\mu_\\mathrm{ML} +\\frac{\\sigma^2}{N\\sigma_0^2+\\sigma^2}\\mu_0 \\\\ \\frac{1}{\\sigma_N^2} &amp;= \\frac{N}{\\sigma^2}+\\frac{1}{\\sigma_0^2} \\end{align} \\] Again we can verify the relation between Bayesian view and MLE, that as \\(N\\) goes from \\(0\\) to infinity, \\(\\mu_N\\) changes from the prior mean \\(\\mu_0\\) to MLE mean \\(\\mu_\\mathrm{ML}\\), and the same holds for the precision which goes from \\(1/\\sigma_0^2\\) to infinity. Multivariate Case Consider a \\(D​\\)-dimensional Gaussian random variable \\(\\mathbf{x}​\\) with distribution \\(p(\\mathbf{x}| \\boldsymbol{\\mu},\\mathbf{\\Sigma})​\\), where the covariance is known and we wish to infer the mean \\(\\boldsymbol{\\mu}​\\) from a set of observations \\(\\mathbf{X}=\\{\\mathbf{x}_1,...,\\mathbf{x}_N\\}​\\). Since the likelihood function again takes a quadratic form of \\(\\boldsymbol\\mu​\\) in exponent, the conjugate prior should also be multivariate Gaussian, and we define it as \\(p(\\boldsymbol\\mu) = p(\\mathbf{\\boldsymbol\\mu}| \\boldsymbol{\\mu}_0,\\mathbf{\\Sigma}_0)​\\). Then the corresponding posterior takes the form \\[ \\begin{align} p(\\boldsymbol{\\mu}|\\mathbf{X}) &amp;= p(\\boldsymbol{\\mu})p(\\mathbf{X}|\\boldsymbol{\\mu}) \\\\ &amp;\\propto \\exp \\left\\{ -\\frac{1}{2} \\sum_{n=1}^{N}(\\mathbf{x}_n- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}_n- \\boldsymbol{\\mu}) -\\frac{1}{2} (\\boldsymbol{\\mu}_0- \\boldsymbol{\\mu})^T\\mathbf{\\Sigma}_0^{-1}(\\boldsymbol{\\mu}_0- \\boldsymbol{\\mu}) \\right\\} \\\\ &amp;\\propto \\exp \\left\\{ -\\frac{1}{2} \\boldsymbol{\\mu}^T (N\\mathbf{\\Sigma^{-1}}+\\mathbf{\\Sigma_0^{-1}}) \\boldsymbol{\\mu} + \\left( N\\boldsymbol{\\mu}_\\mathrm{ML}^T\\mathbf{\\Sigma}^{-1}+ \\boldsymbol{\\mu}_0^T\\mathbf{\\Sigma}_0^{-1}\\right) \\boldsymbol{\\mu} \\right\\} &amp;&amp; \\scriptstyle{(\\boldsymbol\\mu_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^N \\mathbf{x}_n)} \\end{align} \\] Then by comparing to the standard form we can get the mean and covariance of \\(p(\\boldsymbol{\\mu}|\\mathbf{X})\\) \\[ \\begin{align} \\boldsymbol{\\mu}_N &amp;= (N\\mathbf{\\Sigma}^{-1}+\\mathbf{\\Sigma}_0^{-1})^{-1} \\left( N\\mathbf{\\Sigma}^{-1}\\boldsymbol{\\mu}_\\mathrm{ML}+ \\mathbf{\\Sigma}_0^{-1}\\boldsymbol{\\mu}_0\\right) \\\\ \\mathbf{\\Sigma}_N^{-1} &amp;= N\\mathbf{\\Sigma}^{-1}+\\mathbf{\\Sigma}_0^{-1} \\end{align} \\] Bayesian Sequential Estimation Again like in section 2.1, the posterior can act as a prior, and here comes a univariate Gaussian version of Bayesian sequential update to infer mean when variance is known \\[ p(\\mu|\\mathtt{x}) \\propto \\left[ p(\\mu) \\prod_{n=1}^{N-1}p(x_n|\\mu) \\right] p(x_N|\\mu) \\] Infer Precision with Known Mean Univariate Case In this case, instead of infer variance directly, it is more convenient to infer the precision \\(\\lambda\\equiv 1/\\sigma^2​\\). Observe that the likelihood w.r.t \\(\\lambda\\) now takes the form \\[ p(\\mathtt{x}|\\lambda) \\propto \\lambda^{N/2} \\exp\\left\\{ -\\frac{\\lambda}{2}\\sum_{n=1}^N (x_n-\\mu)^2 \\right\\} \\] Its conjugate prior is the Gamma distribution which is defined by \\[ \\mathrm{Gam}(\\lambda|a,b) = \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1}\\exp(-b\\lambda) \\] The gamma distribution has a finite integral if \\(a&gt;0\\) and the distribution itself is finite if \\(a\\geq 1\\). Its mean and variance and given by \\[ \\begin{align} \\mathbb{E}[\\lambda] &amp;= \\frac{a}{b} \\\\ \\mathrm{var}[\\lambda] &amp;= \\frac{a}{b^2} \\end{align} \\] Gamma distribution degenerates to exponential distribution when \\(a=1​\\). So intuitively to help memorization, we can view the mean and variance of Gamma distribution as a result of superposition of \\(a​\\) exponential distribution with parameter \\(b​\\). Consider a prior \\(p(\\lambda)=\\mathrm{Gam}(\\lambda|a_0,b_0)​\\), the posterior takes the form \\[ \\begin{align} p(\\lambda|\\mathtt{x}) &amp;=p(\\lambda)p(\\mathtt{x}|\\lambda) \\\\ &amp;\\propto \\lambda^{N/2+a_0-1} \\exp\\left\\{-b_0\\lambda -\\frac{\\lambda}{2}\\sum_{n=1}^N (x_n-\\mu)^2 \\right\\} \\\\ &amp;\\propto \\lambda^{N/2+a_0-1} \\exp\\left\\{-b_0\\lambda -\\frac{N\\sigma_\\mathrm{ML}^2}{2}\\lambda \\right\\} &amp;&amp;\\scriptstyle{(\\sigma_\\mathrm{ML}^2 = \\frac{1}{N}\\sum_{n=1}^N (x_n-\\mu)^2)} \\end{align} \\] Hence \\(p(\\lambda|\\mathtt{x})\\) is a Gamma distribution with \\[ \\begin{align} a_N &amp;= a_0+\\frac{N}{2} \\\\ b_N &amp;= b_0 +\\frac{N}{2}\\sigma_\\mathrm{ML}^2 \\end{align} \\] We can interpret \\(a_0\\) in the prior in terms of \\(2a_0\\) \"effective\" prior observations, which have variance \\(b_0/a_0\\). It is also possible to define a conjugate prior over the variance rather than precision, this conjugate prior is called the inverse gamma distribution. Multivariate Case For a multivariate Gaussian \\(p(\\mathbf{x}| \\boldsymbol{\\mu},\\mathbf{\\Lambda}^{-1})​\\), the conjugate prior for the precision \\(\\Lambda​\\), assuming the mean is known, is the Wishart distribution given by \\[ \\mathcal{W}(\\mathbf{\\Lambda}|\\mathbf{W},\\nu) = B|\\mathbf{\\Lambda}|^{(\\nu-D-1)/2} \\exp \\left( -\\frac{1}{2}\\mathrm{Tr}(\\mathbf{W}^{-1}\\mathbf{A}) \\right) \\] where \\(\\nu\\) is the number of degrees of freedom of the distribution, \\(\\mathbf{W}\\) is a \\(D\\times D\\) scale matrix, \\(B\\) is the normalization constant. The conjugate prior for covariance matrix is called the inverse Wishart distribution. Infer Mean and Precision Univariate Case For both unknown \\(\\mu\\) and \\(\\lambda\\), the likelihood now takes the form \\[ \\begin{align} p(\\mathtt{x}|\\mu, \\lambda) &amp;\\propto \\lambda^{N/2} \\exp\\left\\{ -\\frac{\\lambda}{2}\\sum_{n=1}^N (x_n-\\mu)^2 \\right\\} \\\\ &amp;= \\lambda^{N/2} \\exp\\left\\{ -\\frac{N}{2}\\lambda\\mu^2 \\right\\} \\exp\\left\\{ \\lambda\\mu\\sum_{n=1}^N x_n \\right\\}\\exp\\left\\{ -\\frac{\\lambda}{2}\\sum_{n=1}^N x_n^2 \\right\\} \\end{align} \\] So the conjugate prior should take the form \\[ \\begin{align} p(\\mu,\\lambda) &amp;\\propto \\lambda^{\\beta/2} \\exp\\left\\{ -\\frac{\\beta}{2}\\lambda\\mu^2 \\right\\} \\exp( c\\lambda\\mu)\\exp( -d\\lambda) \\\\ &amp; = \\lambda^{\\beta/2} \\exp\\left\\{ -\\frac{\\beta}{2}\\lambda\\mu^2 + c\\lambda\\mu -d\\lambda \\right\\} \\\\ &amp; = \\lambda^{\\beta/2} \\exp\\left\\{ -\\frac{\\beta\\lambda}{2}(\\mu-\\frac{c}{\\beta})^2 -(d-\\frac{c^2}{2\\beta})\\lambda \\right\\} \\\\ &amp; = \\exp\\left\\{ -\\frac{\\beta\\lambda}{2}(\\mu-\\frac{c}{\\beta})^2 \\right\\} \\lambda^{\\beta/2} \\exp\\left\\{ -(d-\\frac{c^2}{2\\beta})\\lambda \\right\\} \\\\ &amp;\\propto \\mathcal{N}\\left(\\mu\\middle|\\frac{c}{\\beta},(\\beta\\lambda)^{-1}\\right) \\mathrm{Gam}\\left(\\lambda\\middle|\\frac{\\beta}{2}+1,d-\\frac{c^2}{2\\beta}\\right) \\end{align} \\] This prior is called the Gaussian-gamma distribution, and we can regard it as a product of a Gamma \\(p(\\lambda)​\\) and a Gaussian \\(p(\\mu|\\lambda)​\\). Note that \\(p(\\lambda)​\\) and \\(p(\\mu|\\lambda)​\\) are not independent, since the precision of \\(p(\\mu|\\lambda)​\\) is a linear function of \\(\\lambda​\\). Multivariate Case The prior is known as the Gaussian-Wishart distribution \\[ p(\\boldsymbol{\\mu}, \\mathbf{\\Lambda}|\\boldsymbol{\\mu}_0, \\beta,\\mathbf{W},\\nu) = \\boldsymbol{\\mu}\\mathcal{N}(\\boldsymbol{\\mu}|\\boldsymbol{\\mu}_0,(\\beta \\mathbf{\\Lambda})^{-1})\\ \\mathcal{W}(\\mathbf{\\Lambda}|\\mathbf{W},\\nu) \\] Summary Known Infer Univariate Prior Multivariate Prior \\(\\sigma^2\\) \\(\\mu\\) Gaussian Gaussian \\(\\mu\\) \\(\\lambda\\) Gamma Wishart \\(\\mu\\) \\(\\sigma^2\\) inverse-Gamma inverse-Wishart \\(\\mu,\\lambda\\) Gaussian-gamma Gaussian-Wishart","link":"/old/PRML2-3-6.html"},{"title":"(PRML Notes) 2.3.7-9 Miscellaneous of Gaussian","text":"A series of notes taken from Pattern Recognition and Machine Learning. Student's t-distribution Recall that for a univariate Gaussian \\(\\mathcal{N}(x|\\mu,\\tau^{-1})\\), its conjugate prior when \\(\\mu\\) is known and \\(\\tau^{-1}\\) is unknown is a Gamma distribution \\(\\mathrm{Gam}(\\tau|a,b)\\). Student's t-distribution is the result where we integrate out the precision of the posterior \\[ \\begin{align} p(x|\\mu,a,b) &amp;= \\int_0^\\infty \\mathcal{N}(x|\\mu,\\tau^{-1})\\mathrm{Gam}(\\tau|a,b) \\ d\\tau \\\\ &amp;= \\int_0^\\infty \\left( \\frac{\\tau}{2\\pi} \\right)^{1/2} \\exp \\left\\{ \\frac{\\tau}{2}(x-\\mu)^2 \\right\\} \\frac{b^a \\tau^{a-1}}{\\Gamma(a)} \\exp\\{ -b\\tau \\} \\ d\\tau \\\\ &amp;= \\frac{b^a}{\\Gamma(a) \\sqrt{2\\pi} } \\int_0^\\infty \\tau^{a-1/2} \\exp\\left\\{ -\\left[\\frac{(x-\\mu)^2}{2}+b\\right]\\tau \\right\\} \\ d\\tau \\\\ &amp;= \\frac{b^a}{\\Gamma(a) \\sqrt{2\\pi} }\\left[\\frac{(x-\\mu)^2}{2}+b\\right]^{-a-1/2} \\int_0^\\infty z^{a-1/2} e^{-z} \\ dz &amp;\\scriptstyle{(\\text{Let }z=\\left[\\frac{(x-\\mu)^2}{2}+b\\right]\\tau)} \\\\ &amp;= \\frac{\\Gamma(a+1/2)}{\\Gamma(a)}\\frac{b^a}{\\sqrt{2\\pi} }\\left[\\frac{(x-\\mu)^2}{2}+b\\right]^{-a-1/2} \\\\ &amp;= \\frac{\\Gamma(a+1/2)}{\\Gamma(a)}\\left(\\frac{1}{2\\pi b}\\right)^{1/2}\\left[1+\\frac{(x-\\mu)^2}{2b}\\right]^{-a-1/2} \\\\ &amp;= \\frac{\\Gamma(\\nu/2+1/2)}{\\Gamma(\\nu/2)} \\left(\\frac{\\lambda}{\\pi\\nu}\\right)^{1/2} \\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\nu/2-1/2} &amp;\\scriptstyle{(\\text{Let }\\nu=2a,\\ \\lambda=a/b)} \\\\ \\\\ &amp;\\equiv \\mathrm{St}(x|\\mu,\\lambda,\\nu) \\end{align} \\] The parameter \\(\\nu\\) is called the degrees of freedom. For the particular case of \\(\\nu=1\\), the t-distribution reduces to the Cauchy distribution. And in the limit \\(\\nu \\rightarrow \\infty\\), recall that we can interpret \\(a\\) in prior in terms of \\(2a\\) \"effective\" prior observations, so in this case we are quite sure about the precision of Gaussian with infinite observations, the t-distribution will becomes a Gaussian \\(\\mathcal{N}(x|\\mu,\\lambda^{-1})\\), this can be verified as follow \\[ \\begin{align} \\lim_{\\nu\\rightarrow\\infty} \\mathrm{St}(x|\\mu,\\lambda,\\nu) &amp;\\propto \\lim_{\\nu\\rightarrow\\infty} \\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\nu/2-1/2} \\\\ &amp;= \\lim_{\\nu\\rightarrow\\infty} \\exp\\left\\{-\\frac{\\nu+1}{2} \\ln\\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]\\right\\} \\\\ &amp;= \\lim_{\\nu\\rightarrow\\infty} \\exp\\left\\{-\\frac{\\nu+1}{2} \\left[\\frac{\\lambda(x-\\mu)^2}{\\nu} + o(\\nu^{-1})\\right]\\right\\} \\\\ &amp;= \\lim_{\\nu\\rightarrow\\infty} \\exp\\left\\{-\\frac{\\lambda(x-\\mu)^2}{2} + o(1)\\right\\} \\\\ &amp;= \\exp\\left\\{-\\frac{\\lambda(x-\\mu)^2}{2}\\right\\} \\end{align} \\] The parameter \\(\\lambda\\) is thus called the precision of the t-distribution, though it is only equal to the inverse of the variance under the case \\(\\nu \\rightarrow \\infty\\). Plot of Student's t-distribution for \\(\\mu = 0\\) and \\(\\lambda = 1\\) for various values of \\(\\nu\\) Robustness We can see that t-distribution has longer 'tails' than a Gaussian, which gives the t-distribution an important property called robustness, meaning that it is much less sensitive than the Gaussian to the presence of a few outliers. This is because the t-distribution can be viewed as a infinite mixture of Gaussian with different variance. The MLE solution for the t-distribution can be found using EM algorithm. Illustration of the robustness of Student's t-distribution compared to a Gaussian. Purple is the histogram distribution of 30 data drawn from a Gaussian and 3 additional drawn for (b). The red curve is MLE for t-distribution and green for Gaussian. Robustness is also an important property for regression problems. Comparing to the least squares approach to regression which does not exhibit robustness (since it corresponds to maximum likelihood under a conditional Gaussian distribution as mentioned before), a more heavy-tailed t-distribution can obtain a more robust model. Multivariate t-distribution (To be continued...) Periodic Variables Here the text introduces this variable model in the motivation of removing the \"dependence of the origin of the angular coordinate\", which is different from my note. Gaussian are inappropriate as density models for periodic variables such as the wind direction. Handling such variables rely on choosing some direction as origin, and then representing all the variables in the range \\([0,2\\pi)​\\). However, problem comes here, for example, two variables \\(\\theta_0=359^\\circ​\\) and \\(\\theta_1=361^\\circ​\\) with origin chosen at \\(0^\\circ​\\) will first be changed into \\(359^\\circ​\\) and \\(1^\\circ​\\) which have mean \\(180^\\circ​\\), while with origin chosen at \\(2^\\circ​\\) the two variables will be changed into \\(357^\\circ​\\) and \\(359^\\circ​\\) where the mean suddenly changes to \\(358^\\circ​\\). This is because under such angular coordinate the resulting representation of angular is not continuous w.r.t the choice of origin. This problem can be solved by describing the univariate angles instead by two-dimensional vectors, so that we don't need to transform all the variables into the range \\([0,2\\pi)\\). Concretely, the original set of observations on the Cartesian coordinate \\(\\mathcal{D} =\\{\\theta_1,...,\\theta_N\\}\\) is now changed into \\(\\mathcal{D}=\\{(\\cos\\theta_1,\\sin\\theta_1),...,(\\cos\\theta_N,\\sin\\theta_N)\\}\\), and the sample mean can now be viewed as \\((\\frac{1}{N}\\sum_n \\cos \\theta_n, \\frac{1}{N}\\sum_n\\sin\\theta_n)\\), and \\[ \\bar{\\theta} = \\arctan\\left\\{ \\frac{\\sum_n\\sin\\theta_n}{\\sum_n\\cos\\theta_n} \\right\\} \\] which is continuous w.r.t the choice of origin, since now \\(\\theta_n\\in \\mathbb{R}​\\). Actually this result corresponds to the MLE for von Mises distribution over such periodic variable. Illustration of the representation of different \\(\\theta_n\\) as 2-D vectors Von Mises Distribution A periodic distribution (with period \\(2\\pi\\) for example) is not necessarily and not possibly be normalized, but must satisfy the three conditions \\[ \\begin{align} p(\\theta) \\ &amp;\\geq \\ 0 \\\\ \\int_0^{2\\pi} p(\\theta)\\ d\\theta \\ &amp;= \\ 1 \\\\ p(\\theta+2\\pi) \\ &amp;= \\ p(\\theta) \\end{align} \\] And we can obtain a Gaussian-like distribution that satisfies these properties as follows. Consider a two variable Gaussian \\[ p_\\mathrm{Cart}(x_1,x_2) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left\\{ -\\frac{(x_1-\\mu_1)^2+(x_2-\\mu_2)^2}{2\\sigma^2} \\right\\} \\] The von Mises distribution can be derived by a two-dimensional Gaussian conditioning on the unit circle. When we only consider the value of this distribution along a circle of radius \\(r=1\\), and transform the from of this distribution from Cartesian coordinate \\((x_1,x_2)\\) to polar coordinate \\((r,\\theta)\\), we have \\[ \\begin{align} p(\\theta,r=1) &amp;= p_\\mathrm{Cart}(\\cos\\theta,\\sin\\theta) \\\\ &amp;= \\frac{1}{2\\pi\\sigma^2} \\exp\\left\\{ -\\frac{(\\cos\\theta-r_0\\cos\\theta_0)^2+(\\sin\\theta-r_0\\sin\\theta_0)^2}{2\\sigma^2} \\right\\} \\\\ &amp;\\propto \\exp\\left\\{ -\\frac{1+r_0^2-2r_0\\cos\\theta\\cos\\theta_0 - 2r_0\\sin\\theta\\sin\\theta_0}{2\\sigma^2} \\right\\} \\\\ &amp;\\propto \\exp\\left\\{ \\frac{r_0}{\\sigma^2} \\cos(\\theta-\\theta_0) \\right\\} \\\\ &amp;= \\exp\\left\\{ m \\cos(\\theta-\\theta_0) \\right\\} \\end{align} \\] where we have defined \\((\\mu_1,\\mu_2) = (r_0\\cos\\theta_0,r_0\\sin\\theta_0)\\) and \\(m=r_0/\\sigma^2\\). Finally by normalization we obtain the von Mises Distribution over \\(\\theta\\) along the unit circle \\[ p(\\theta|\\theta_0,m,r=1) = \\frac{1}{2\\pi I_0(m)} \\exp\\{ m\\cos(\\theta-\\theta_0) \\} \\] here \\(m​\\) is known as the concentration parameter, which is analogous to the precision for Gaussian. For large \\(m​\\), the distribution becomes approximately Gaussian peaked at \\(\\theta_0​\\). And we have defined \\[ I_0(m) = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\exp\\{ m\\cos\\theta\\}\\ d\\theta \\] The von Mises distribution shown as a Cartesian plot and a polar plot. It is easy to obtain the MLE for \\(\\theta_0\\) and \\(m\\). Consider first the log likelihood given by \\[ \\ln p(\\mathcal{D}|\\theta_0,m) = -N\\ln(2\\pi) - N \\ln I_0(m) + m\\sum_{n=1}^N \\cos(\\theta_n-\\theta_0) \\] Setting the derivative w.r.t. \\(\\theta_0\\) to zero gives \\[ \\begin{align} \\sum_{n=1}^N \\sin(\\theta_n-\\theta_0^\\mathrm{ML}) &amp;= 0 \\\\ \\sum_{n=1}^N \\sin\\theta_n\\cos\\theta_0^\\mathrm{ML} &amp;= \\sum_{n=1}^N \\cos\\theta_n\\sin\\theta_0^\\mathrm{ML} \\\\ \\tan\\theta_0^\\mathrm{ML} &amp;= \\frac{\\sum_{n=1}^N \\sin\\theta_n}{\\sum_{n=1}^N \\cos\\theta_n} \\\\ \\theta_0^\\mathrm{ML} &amp;= \\arctan\\left\\{\\frac{\\sum_{n=1}^N \\sin\\theta_n}{\\sum_{n=1}^N \\cos\\theta_n}\\right\\} \\end{align} \\] which coincides with the mean we defined in the previous section. Similarly setting the derivative w.r.t. \\(m\\) to zero gives \\[ \\frac{I_1(m_\\mathrm{ML})}{I_0(m_\\mathrm{ML})} = \\frac{1}{N}\\sum_{n=1}^N \\cos(\\theta_n-\\theta_0^\\mathrm{ML}) \\] where \\(I_1(m) = I&#39;_0(m)​\\). There are some other alternative techniques for the construction of periodic distributions mentioned in text, which are either more complex or suffer from significant limitations. Mixtures of Gaussians A simple Gaussian distribution is unable to capture multi-modal structure, whereas a linear superposition of multiple Gaussians can approximate any continuous density to arbitrary accuracy, which is called a mixture of Gaussians \\[ p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\] The parameter \\(\\pi_k​\\) is called the mixing coefficients and must satisfy \\[ \\sum_{k=1}^K \\pi_k \\ = \\ 1 \\\\ 0 \\le \\pi_k \\le 1 \\] to make \\(p(\\mathbf{x})\\) become probability. We can view \\(\\pi_k=p(k)\\) as the prior of picking the \\(k\\)th component and \\(\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) = p(\\mathbf{x}|k)\\) to be the likelihood of \\(\\mathbf{x}\\) conditioned on \\(k\\). The posteriors here play an important role in many problems, and is also known as responsibilities, given by \\[ \\begin{align} \\gamma_k(\\mathbf{x}) &amp;\\equiv p(k|\\mathbf{x}) \\\\&amp;= \\frac{p(k)p(\\mathbf{x}|k)}{\\sum_l p(l)p(\\mathbf{x}|l)} \\\\&amp;= \\frac{\\pi_k\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k)}{\\sum_l \\pi_l\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_l, \\mathbf{\\Sigma}_l)} \\end{align} \\] The MLE for the parameters in mixtures of Gaussian no longer have a closed-form solution since the presence of the summation over \\(k\\) inside the logarithm in the log likelihood \\[ \\ln p(\\mathbf{X}|\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = \\sum_{n=1}^N \\ln\\left\\{ \\sum_{k=1}^K\\pi_k\\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\mathbf{\\Sigma}_k) \\right\\} \\] A powerful framework for maximizing the likelihood is called expectation maximization, which will be discussed later.","link":"/old/PRML2-3-7~9.html"},{"title":"(PRML Notes) 2.4 The Exponential Family","text":"A series of notes taken from Pattern Recognition and Machine Learning. The exponential family is defined by \\[ p(\\mathbf{x} | \\boldsymbol{\\eta})=h(\\mathbf{x}) g(\\boldsymbol{\\eta}) \\exp \\left\\{\\boldsymbol{\\eta}^{\\mathrm{T}} \\mathbf{u}(\\mathbf{x})\\right\\} \\] here \\(\\boldsymbol{\\eta}\\) is called the natural parameters of the distribution, \\(g(\\boldsymbol{\\eta})\\) ensures the normalization. Now we will see that all the distributions we have seen so far are actually members in this family. Bernoulli Distribution \\[ \\begin{aligned} \\operatorname{Bern}(x | \\mu) &amp;= \\exp \\{x \\ln \\mu+(1-x) \\ln (1-\\mu)\\} \\\\ &amp;=(1-\\mu) \\exp \\left\\{\\ln \\left(\\frac{\\mu}{1-\\mu}\\right) x\\right\\} \\end{aligned} \\] and we define \\[ \\eta=\\ln \\left(\\frac{\\mu}{1-\\mu}\\right) \\] then we have \\[ \\mu = \\sigma(\\eta)=\\frac{1}{1+\\exp (-\\eta)} \\] where \\(\\sigma\\) is called the logistic sigmoid function (we can regard it as allowing to define \"probability in \\(\\mathbb{R}\\)\" and then transforming into a proper range \\([0,1]\\)). With the property that \\(1-\\sigma(\\eta)=\\sigma(-\\eta)\\), now we can represent the Bernoulli distribution in the form \\[ p(x | \\eta)=\\sigma(-\\eta) \\exp (\\eta x) \\] by compariing we see that Bernoulli distribution takes the form of exponential family where \\[ \\begin{array}{l}{u(x)=x} \\\\ {h(x)=1} \\\\ {g(\\eta)=\\sigma(-\\eta)}\\end{array} \\] Multinomial Distribution \\[ \\mathrm{Multi}(\\mathbf{x} | \\boldsymbol{\\mu})=\\prod_{k=1}^{M} \\mu_{k}^{x_{k}}=\\exp \\left\\{\\sum_{k=1}^{M} x_{k} \\ln \\mu_{k}\\right\\} = \\exp \\left(\\boldsymbol{\\eta}^{\\mathrm{T}} \\mathbf{x}\\right) \\] where \\(\\mathbf{x}=\\left(x_{1}, \\dots, x_{N}\\right)^{\\mathrm{T}}\\), and we have defined \\(\\boldsymbol{\\eta}=\\left(\\eta_{1}, \\ldots, \\eta_{M}\\right)^{\\mathrm{T}}\\) in which \\(\\eta_{k}=\\ln \\mu_{k}\\). By comparing we see that multinomial distribution takes the form of exponential family where \\[ \\begin{aligned} \\mathbf{u}(\\mathbf{x}) &amp;=\\mathbf{x} \\\\ h(\\mathbf{x}) &amp;=1 \\\\ g(\\boldsymbol{\\eta}) &amp;=1 \\end{aligned} \\] Note that in this form \\(\\eta_k\\) are not independent due to the constraint \\(\\sum_{k=1}^{M} \\mu_{k}=1\\). To make it keep the form with Bernoulli distribution, we can rewrite the distribution with this constraint removed by \\[ \\begin{array}{l}{\\mathrm{Multi}(\\mathbf{x} | \\boldsymbol{\\mu}) \\\\ = \\exp \\left\\{\\sum_{k=1}^{M} x_{k} \\ln \\mu_{k}\\right\\}} \\\\ {=\\exp \\left\\{\\sum_{k=1}^{M-1} x_{k} \\ln \\mu_{k}+\\left(1-\\sum_{k=1}^{M-1} x_{k}\\right) \\ln \\left(1-\\sum_{k=1}^{M-1} \\mu_{k}\\right)\\right\\}} \\\\ {=\\exp \\left\\{\\sum_{k=1}^{M-1} x_{k} \\ln \\left(\\frac{\\mu_{k}}{1-\\sum_{j=1}^{M-1} \\mu_{j}}\\right)+\\ln \\left(1-\\sum_{k=1}^{M-1} \\mu_{k}\\right)\\right\\}}\\end{array} \\] now we define \\[ \\eta_{k}=\\ln \\left(\\frac{\\mu_{k}}{1-\\sum_{j} \\mu_{j}}\\right) \\] and taking exponent of each side and summing both sides we have \\[ \\sum_k\\exp(\\eta_{k})=\\frac{\\sum_j\\mu_{j}}{1-\\sum_{j} \\mu_{j}} \\ \\Rightarrow \\ \\sum_j\\mu_{j} = \\frac{\\sum_k\\exp(\\eta_{k})}{1+\\sum_k\\exp(\\eta_{k})} \\] then we get \\[ \\mu_k = \\exp(\\eta_k)(1-\\sum_{j} \\mu_{j})=\\frac{\\exp \\left(\\eta_{k}\\right)}{1+\\sum_{j} \\exp \\left(\\eta_{j}\\right)} \\] This function is called the softmax function (a multivariate version of sigmoid function). Now we can represent the multinomial distribution in the form \\[ p(\\mathbf{x} | \\boldsymbol{\\eta})= \\left(1-\\sum_{k=1}^{M-1} \\mu_{k}\\right)\\exp \\left(\\boldsymbol{\\eta}^{\\mathrm{T}} \\mathbf{x}\\right) = \\left(1+\\sum_{k=1}^{M-1} \\exp \\left(\\eta_{k}\\right)\\right)^{-1} \\exp \\left(\\boldsymbol{\\eta}^{\\mathrm{T}} \\mathbf{x}\\right) \\] by comparing \\[ \\begin{array}{l}{\\mathbf{u}(\\mathbf{x})=\\mathbf{x}} \\\\ {h(\\mathbf{x})=1} \\\\ {g(\\boldsymbol{\\eta})=\\left(1+\\sum_{k=1}^{M-1} \\exp \\left(\\eta_{k}\\right)\\right)^{-1}}\\end{array} \\] Gaussian Distribution \\[ \\begin{aligned} p(x | \\mu, \\sigma^{2}) &amp;=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{1 / 2}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}}(x-\\mu)^{2}\\right\\} \\\\ &amp;=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{1 / 2}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}} x^{2}+\\frac{\\mu}{\\sigma^{2}} x-\\frac{1}{2 \\sigma^{2}} \\mu^{2}\\right\\} \\end{aligned} \\] by comparing we have \\[ \\begin{aligned} \\boldsymbol{\\eta} &amp;=\\left( \\begin{array}{c}{\\mu / \\sigma^{2}} \\\\ {-1 / 2 \\sigma^{2}}\\end{array}\\right) \\\\ \\mathbf{u}(x) &amp;=\\left( \\begin{array}{c}{x} \\\\ {x^2}\\end{array}\\right) \\\\ h(x) &amp;=(2 \\pi)^{-1 / 2} \\\\ g(\\boldsymbol{\\eta}) &amp;=\\left(-2 \\eta_{2}\\right)^{1 / 2} \\exp \\left(\\frac{\\eta_{1}^{2}}{4 \\eta_{2}}\\right) \\end{aligned} \\] Note that we write \\(\\mathbf{u}(x)\\) in the form of combination of different orders, which helps infer different order moments. Maximum Likelihood and Sufficient Statistics From the fact that exponential family is normalized we have \\[ g(\\boldsymbol{\\eta}) \\int h(\\boldsymbol{x}) \\exp \\left\\{\\boldsymbol{\\eta}^{T} \\boldsymbol{u}(\\boldsymbol{x})\\right\\} \\mathrm{d} \\boldsymbol{x}=1 \\] then by taking the gradient of both sides w.r.t. \\(\\boldsymbol{\\eta}\\) we have \\[ \\begin{align} &amp;\\nabla g(\\boldsymbol{\\eta}) \\int h(\\mathbf{x}) \\exp \\left\\{\\boldsymbol{\\eta}^{\\mathrm{T}} \\mathbf{u}(\\mathbf{x})\\right\\} \\mathrm{d} \\mathbf{x} + g(\\boldsymbol{\\eta}) \\int h(\\mathbf{x}) \\exp \\left\\{\\boldsymbol{\\eta}^{\\mathrm{T}} \\mathbf{u}(\\mathbf{x})\\right\\} \\mathbf{u}(\\mathbf{x}) \\mathrm{d} \\mathbf{x}=0 \\\\ \\Rightarrow&amp; -\\frac{1}{g(\\boldsymbol{\\eta})} \\nabla g(\\boldsymbol{\\eta})=g(\\boldsymbol{\\eta}) \\int h(\\mathbf{x}) \\exp \\left\\{\\boldsymbol{\\eta}^{\\mathrm{T}} \\mathbf{u}(\\mathbf{x})\\right\\} \\mathbf{u}(\\mathbf{x}) \\mathrm{d} \\mathbf{x}=\\mathbb{E}[\\mathbf{u}(\\mathbf{x})] \\\\ \\Rightarrow&amp; -\\nabla \\ln g(\\boldsymbol{\\eta})=\\mathbb{E}[\\mathbf{u}(\\mathbf{x})] \\end{align} \\] Similarly we can find higher order moments by simply taking higher order differentiation. Now consider finding the MLE under a set of i.i.d. data denoted by \\(\\mathbf{X}=\\left\\{\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n}\\right\\}\\), then the log likelihood is \\[ p(\\mathbf{X} | \\boldsymbol{\\eta})=\\left(\\prod_{n=1}^{N} h\\left(\\mathbf{x}_{n}\\right)\\right) g(\\boldsymbol{\\eta})^{N} \\exp \\left\\{\\boldsymbol{\\eta}^{\\mathrm{T}} \\sum_{n=1}^{N} \\mathbf{u}\\left(\\mathbf{x}_{n}\\right)\\right\\} \\] setting the gradient of \\(\\ln p(\\mathbf{X} | \\boldsymbol{\\eta})\\) w.r.t. \\(\\boldsymbol{\\eta}\\) to zero we get \\[ -\\nabla \\ln g\\left(\\boldsymbol{\\eta}_{\\mathrm{ML}}\\right)=\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{u}\\left(\\mathbf{x}_{n}\\right) \\] under the limit \\(N \\rightarrow \\infty​\\), we have \\(\\mathbb{E}[\\mathbf{u}(\\mathbf{x})] = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{u}\\left(\\mathbf{x}_{n}\\right)​\\), so in this limit \\(\\boldsymbol{\\eta}_{\\mathrm{ML}}​\\) will be equal to the true value \\(\\boldsymbol{\\eta}​\\). Note that the solution of MLE depends on data only through \\(\\sum_{n} \\mathbf{u}\\left(\\mathbf{x}_{n}\\right)\\), which is called the sufficient statistic. For example, to find out \\(\\boldsymbol{\\eta}_{\\mathrm{ML}}\\) for Gaussian, we only need to keep the sum of \\(\\left\\{x_{n}\\right\\}\\) and \\(\\left\\{x_{n}^2\\right\\}\\). Conjugate Priors For exponential family, a conjugate prior can be written in the form \\[ p(\\boldsymbol{\\eta} | \\boldsymbol\\chi, \\nu)=f(\\boldsymbol{\\chi},\\nu) g(\\boldsymbol{\\eta})^{\\nu} \\exp \\left\\{\\nu \\boldsymbol{\\eta}^{\\mathrm{T}} \\boldsymbol{\\chi}\\right\\} \\] and we see that the resulting posterior takes the same form as the prior \\[ p(\\boldsymbol{\\eta} | \\mathbf{X}, \\boldsymbol\\chi, \\nu) \\propto g(\\boldsymbol{\\eta})^{\\nu+N} \\exp \\left\\{\\boldsymbol{\\eta}^{\\mathrm{T}}\\left(\\sum_{n=1}^{N} \\mathbf{u}\\left(\\mathbf{x}_{n}\\right)+\\nu \\boldsymbol{\\chi}\\right)\\right\\} \\] Note that from this form we can generally interpret the parameter \\(\\nu\\) as an effective number of pseudo-observations in the prior, and each of which contribute a value for the sufficient statistic \\(\\mathbf{u}(\\mathbf{x})\\) given by \\(\\boldsymbol\\chi\\). Noninformative Priors Noninformative Priors are intended to have little influence on the posterior distribution as possible, \"letting the data speak for themselves\". We would like to choose a uniform distribution as a prior over a finite parameter space. However, there are two problems arise: Improper prior over infinite parameter space. However, this doesn't matter too much, since an improper prior can also result in a proper posterior. For example, for an improper prior \\(p(\\mu)=1, -\\infty&lt;\\mu&lt;\\infty\\) and likelihood \\(p(x) = \\mathcal{N}(\\mu,1)\\), the posterior is proper \\[ \\begin{aligned} p(\\mu | x) &amp;=\\frac{p(x | \\mu) p(\\mu)}{\\int_{-\\infty}^{\\infty} p(x | \\mu) p(\\mu) d \\mu} \\\\ &amp;=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left\\{-\\frac{1}{2}(\\mu-x)^{2}\\right\\} \\end{aligned} \\] The uniform distribution is not invariant under reparameterization. For example, for the same \\(p(\\mu)\\) as before, after transformation \\(\\mu=\\eta^2\\), we have \\(p_\\eta(\\eta)=2\\eta p_\\mu(\\mu) =2\\eta\\), no longer uniform, so can we still say that it's noninformative? To solve the second problem, we must take care to use an appropriate representation for the parameters. Two examples are shown below. Location Parameter Suppose \\(p(x|\\mu)\\) takes the form \\(f(x-\\mu)\\), such density functions compose the translation invariant family, and \\(\\mu\\) is known as location parameter. Now if we move \\(x\\) to become \\(x&#39;=x+c\\), to make the density function still a member of translation invariant family, we should take \\(\\mu&#39;=\\mu+c\\), and we see that \\(p(x&#39;|\\mu&#39;)\\) takes the same form \\(f(x&#39;-\\mu&#39;)\\). Since they are both members of translation invariant family, \\(p(x|\\mu)\\) and \\(p(x|\\mu&#39;)\\) are equally likely to be the real likelihood, \\(p(\\mu)\\) and \\(p(\\mu&#39;)\\) should take the same noninformative distribution form, i.e. \\(p(\\mu) = p(\\mu&#39;)\\). Note that \\[ p(\\mu) = p(\\mu&#39;) \\left|\\frac{\\mathrm{d} \\mu&#39;}{\\mathrm{d} \\mu}\\right| = p(\\mu&#39;) = p(\\mu+c) \\] First let \\(\\mu=-c​\\), we have \\(p(-c) = p(0)=\\mathrm{const}​\\), and note that \\(c​\\) can be arbitrary, so we know that the noninformative distribution \\(p(\\mu)​\\) is constant. Scale Parameter Similarly, suppose \\(p(x|\\sigma)​\\) takes the form \\(\\frac{1}{\\sigma}f(\\frac{x}{\\sigma})​\\), such density functions compose the scale invariant family, and \\(\\sigma​\\) is known as scale parameter. Now if we scale \\(x​\\) to become \\(x&#39;=cx​\\), we see that \\(p(x&#39;|\\sigma&#39;) =p(x|\\sigma)/c = \\frac{1}{c\\sigma}f(\\frac{cx}{c\\sigma}) = \\frac{1}{\\sigma&#39;}f(\\frac{x&#39;}{\\sigma&#39;})​\\), where we take \\(\\sigma&#39;=c\\sigma​\\). Since they are both members of scale invariant family, \\(p(x|\\sigma)\\) and \\(p(x|\\sigma&#39;)\\) are equally likely to be the real likelihood, \\(p(\\sigma)\\) and \\(p(\\sigma&#39;)\\) should take the same noninformative distribution form, i.e. \\(p(\\sigma) = p(\\sigma&#39;)\\). Note that \\[ p(\\sigma) = p(\\sigma&#39;) \\left|\\frac{\\mathrm{d} \\sigma&#39;}{\\mathrm{d} \\sigma}\\right| = cp(\\sigma&#39;) = cp(c\\sigma) \\] First let \\(\\sigma=1/c\\), we have \\(p(1/c) = cp(1)\\), and note that \\(c\\) can be arbitrary, so we know that \\(p(1/\\sigma)\\propto \\sigma\\), i.e. \\(p(\\sigma) \\propto \\sigma^{-1}\\). Actually, there is no objective and unique prior that represents ignorance instead noninformative priors are chosen by public agreement much like units of length and weight. See here.","link":"/old/PRML2-4.html"},{"title":"(PRML Notes) 3.1 Linear Basis Function Models","text":"A series of notes taken from Pattern Recognition and Machine Learning. A simplest regression model for \\(D\\)-dimensional input variable \\(\\mathbf{x}=\\left(x_{1}, \\dots, x_{D}\\right)^{\\mathrm{T}}\\) is \\[ y(\\mathbf{x}, \\mathbf{w})=w_{0}+w_{1} x_{1}+\\ldots+w_{D} x_{D} \\] The key property of this model is that it is a linear function of parameters \\(w_{0}, \\dots, w_{D}\\). This model can be extended to linear combination of fixed nonlinear functions of \\(\\mathbf{x}\\) \\[ y(\\mathbf{x}, \\mathbf{w})=w_{0}+\\sum_{j=1}^{M-1} w_{j} \\phi_{j}(\\mathbf{x}) \\] where \\(\\phi_{j}(\\mathbf{x})\\) is known as basis functions. It is convenient to define an additional dummy \\(\\phi_{0}(\\mathbf{x})=1\\) for bias term so that \\[ y(\\mathbf{x}, \\mathbf{w})=\\sum_{j=0}^{M-1} w_{j} \\phi_{j}(\\mathbf{x})=\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}(\\mathbf{x}) \\] There are many choices for basis functions, for example, in previous section we have seen the polynomial basis function \\[ \\phi_{j}(x)=x^{j} \\] Other examples are Gaussian basis function \\[ \\phi_{j}(x)=\\exp \\left\\{-\\frac{\\left(x-\\mu_{j}\\right)^{2}}{2 s^{2}}\\right\\} \\] and the sigmoidal basis function, where \\(\\sigma(a)=1/(1+\\exp (-a))\\) \\[ \\phi_{j}(x)=\\sigma\\left(\\frac{x-\\mu_{j}}{s}\\right) \\] Equivalently, a general linear combination of logistic sigmoid functions is equivalent to a general linear combination of \\(\\mathrm{tanh}\\) functions since \\[ \\tanh (a)= \\frac{e^a-e^{-a}}{e^a+e^{-a}} = \\frac{1-e^{-2a}}{1+e^{-2a}} = \\frac{2}{1+e^{-2a}}-1=2 \\sigma(2a)-1 \\] Another possible choice of basis function is the Fourier basis, which leads to an expansion in sinusoidal functions which represents a specific frequency has infinite spacial extent. While the functions localized in both space and frequency are known as wavelets. Maximum Likelihood and Least Squares As before the target variable \\(t\\) is assumed to be given by \\[ t=y(\\mathbf{x}, \\mathbf{w})+\\epsilon \\] where \\(\\epsilon \\sim \\mathcal{N}(0,\\beta^{-1})\\). Thus we have \\[ p(t | \\mathbf{x}, \\mathbf{w}, \\beta)=\\mathcal{N}(t | y(\\mathbf{x}, \\mathbf{w}), \\beta^{-1}) \\] Recall that if we assume a squared loss, the optimal prediction for \\(\\mathbf{x}\\) will be given by the conditional mean of \\(t\\), so \\[ \\text{optimal prediction for }\\mathbf{x} = \\mathbb{E}[t | \\mathbf{x}]=\\int t p(t | \\mathbf{x}) \\mathrm{d} t=y(\\mathbf{x}, \\mathbf{w}) \\] Now for a data set \\(\\mathbf{X}=\\left\\{\\mathbf{x}_{1}, \\dots, \\mathbf{x}_{N}\\right\\}\\) drawn i.i.d., with corresponding targets \\(\\boldsymbol{\\mathsf{t}}= (t_{1}, \\dots, t_{N})^{\\mathrm{T}}\\), and we have the following likelihood \\[ p(\\boldsymbol{\\mathsf{t}} | \\mathbf{w}, \\beta)=\\prod_{n=1}^{N} \\mathcal{N}\\left(t_{n} | \\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right), \\beta^{-1}\\right) \\] Note that from now on \\(\\mathbf{x}​\\) will be dropped from conditioning variables since in supervised learning we are not seeking to model the distribution of \\(\\mathbf{x}​\\). Taking logarithm we have \\[ \\begin{aligned} \\ln p(\\boldsymbol{\\mathsf{t}} | \\mathbf{w}, \\beta) &amp;=\\sum_{n=1}^{N} \\ln \\mathcal{N}\\left(t_{n} | \\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right), \\beta^{-1}\\right) \\\\ &amp;=\\frac{N}{2} \\ln \\beta-\\frac{N}{2} \\ln (2 \\pi)-\\beta E_{D}(\\mathbf{w}) \\end{aligned} \\] where \\(E_{D}(\\mathbf{w})=\\frac{1}{2} \\sum_{n=1}^{N}\\left\\{t_{n}-\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\right\\}^{2} = \\frac{1}{2} \\| \\boldsymbol{\\mathsf{t}} - \\mathbf{\\Phi w} \\|^2​\\), here \\(\\mathbf{\\Phi}​\\) is an \\(N\\times M​\\) matrix, called the design matrix \\[ \\boldsymbol{\\Phi}=\\left( \\begin{array}{cccc}{\\phi_{0}\\left(\\mathbf{x}_{1}\\right)} &amp; {\\phi_{1}\\left(\\mathbf{x}_{1}\\right)} &amp; {\\cdots} &amp; {\\phi_{M-1}\\left(\\mathbf{x}_{1}\\right)} \\\\ {\\phi_{0}\\left(\\mathbf{x}_{2}\\right)} &amp; {\\phi_{1}\\left(\\mathbf{x}_{2}\\right)} &amp; {\\cdots} &amp; {\\phi_{M-1}\\left(\\mathbf{x}_{2}\\right)} \\\\ {\\vdots} &amp; {\\vdots} &amp; {\\ddots} &amp; {\\vdots} \\\\ {\\phi_{0}\\left(\\mathbf{x}_{N}\\right)} &amp; {\\phi_{1}\\left(\\mathbf{x}_{N}\\right)} &amp; {\\cdots} &amp; {\\phi_{M-1}\\left(\\mathbf{x}_{N}\\right)}\\end{array}\\right) \\] To find out \\(\\mathbf{w}_{\\mathrm{ML}}​\\), set gradient to zero \\[ \\nabla_\\mathbf{w} \\ln p(\\boldsymbol{\\mathsf{t}} | \\mathbf{w}, \\beta) = \\nabla_\\mathbf{w} \\beta E_{D}(\\mathbf{w}) = \\frac{\\beta}{2} \\mathbf{\\Phi}^\\mathrm{T} (\\boldsymbol{\\mathsf{t}} - \\mathbf{\\Phi w}) = 0 \\] we have \\[ \\mathbf{w}_{\\mathrm{ML}}=\\left(\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\] which are known as the normal equations for the least squares problem, and the quantity \\(\\mathbf{\\Phi}^{\\dagger} \\equiv\\left(\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^{\\mathrm{T}}\\) is known as the Moore-Penrose pseudo-inverse of the matrix \\(\\mathbf{\\Phi}\\). To find out \\(\\beta_{\\mathrm{ML}}​\\), set gradient to zero \\[ \\nabla_\\mathbf{\\beta} \\ln p(\\boldsymbol{\\mathsf{t}} | \\mathbf{w}, \\beta) = \\frac{N}{2\\beta} - E_{D}(\\mathbf{w}) = 0 \\] we have \\[ \\frac{1}{\\beta_{\\mathrm{ML}}} = \\frac{1}{N}\\| \\boldsymbol{\\mathsf{t}} - \\mathbf{\\Phi w} \\|^2 \\] Geometry of Least Squares Consider \\(N​\\)-dimensional space whose axes are given by \\(t_n​\\), and \\(\\boldsymbol{\\mathsf{t}}=\\left(t_{1}, \\ldots, t_{N}\\right)^{\\mathrm{T}}​\\) is a vector in this space, then the \\(M​\\) vectors \\(\\boldsymbol{\\varphi}_j = (\\phi_j(\\mathbf{x}_1),...,\\phi_j(\\mathbf{x}_N))^\\mathrm{T},\\ j=0,...,M-1​\\) will span a subspace \\(\\mathcal{S}=\\mathrm{Span} (\\boldsymbol{\\varphi}_1,...,\\boldsymbol{\\varphi}_{M-1})​\\). And define \\(\\boldsymbol{\\mathsf{y}} = (y\\left(\\mathbf{x}_{1}, \\mathbf{w}\\right),...,y\\left(\\mathbf{x}_{N}, \\mathbf{w}\\right))^\\mathrm{T}​\\), it can lie anywhere in \\(\\mathcal{S}\\). Intuitively, since in least square solution \\(\\boldsymbol{\\mathsf{y}}​\\) is the closest vector to \\(\\boldsymbol{\\mathsf{t}}​\\) in \\(\\mathcal{S}​\\), so the solution corresponds to the orthogonal projection of \\(\\boldsymbol{\\mathsf{t}}​\\) onto \\(\\mathcal{S}​\\). Geometrical interpretation of the least-squares solution Sequential Learning For an error function describe in the sum over data points \\(E=\\sum_{n} E_{n}\\), we an apply an on-line learning (sequential learning) algorithm called stochastic gradient descent to update \\(\\mathbf{w}\\) \\[ \\mathbf{w}^{(\\tau+1)}=\\mathbf{w}^{(\\tau)}-\\eta \\nabla E_{n} \\] where \\(\\tau\\) denotes the iteration number, and \\(\\eta\\) the learning rate. For example, for the least square error, it can be easily verified that this gives \\[ \\mathbf{w}^{(\\tau+1)}=\\mathbf{w}^{(\\tau)}+\\eta\\boldsymbol{\\phi}(\\mathbf{x}_n)\\left(t_{n}-\\mathbf{w}^{(\\tau) \\mathrm{T}} \\boldsymbol{\\phi}(\\mathbf{x}_n)\\right) \\] which is known as least-mean-squares or the LMS algorithm. Regularized Least Squares Recall that the general form of error function with regularization is \\[ E_{D}(\\mathbf{w})+\\lambda E_{W}(\\mathbf{w}) \\] For sum-of-square error with a quadratic regularizer (known as weight decay) takes the form \\[ \\frac{1}{2} \\| \\boldsymbol{\\mathsf{t}} - \\mathbf{\\Phi w} \\|^2 +\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 \\] Now the solution of minimization of \\(\\mathbf{w}\\) is \\[ \\mathbf{w}=\\left(\\lambda \\mathbf{I}+\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{\\mathrm{T}} \\mathbf{t} \\] Unlike \\(\\left(\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi}\\right)^{-1}​\\), this time the inverse term always exists, since for any non-zero vector \\(\\mathbf{v}​\\), it is definite \\[ \\mathbf{v}^\\mathrm{T}\\left(\\lambda \\mathbf{I}+\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi}\\right)\\mathbf{v} = \\lambda\\mathbf{v}^\\mathrm{T}\\mathbf{v} + \\|\\mathbf{\\Phi}\\mathbf{v}\\|^2 &gt; 0 \\] A more general regularizer takes the form \\[ \\frac{1}{2} \\| \\boldsymbol{\\mathsf{t}} - \\mathbf{\\Phi w} \\|^2 +\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^q \\] where \\(\\|\\mathbf{w}\\|^q = \\sum_{j=1}^{M}\\left|w_{j}\\right|^{q}​\\). Contours of the regularization term The case of \\(q=1\\) is known as lasso. It has the property that if \\(\\lambda\\) is sufficiently large, some of the \\(w_j\\) becomes zero, so it is often used when we believe some features are irrelevant to the target. To see why, just note that for lasso the minimization problem is the same as \\[ \\min \\ \\frac{1}{2} \\| \\boldsymbol{\\mathsf{t}} - \\mathbf{\\Phi w} \\|^2 \\\\ \\mathrm{s.t.} \\sum_{j=1}^{M}\\left|w_{j}\\right| \\leqslant \\eta \\] related by using Lagrange multipliers. When \\(\\lambda\\) becomes large, it is easy to see that \\(w_j\\) becomes small, then so is the corresponding \\(\\eta\\), and from geometry intuition as the figure below we know some \\(w_j\\) will become \\(0​\\). Plot of the contours of the unregularized error function along with the constraint region for the quadratic regularizer \\(q = 2\\) on the left and lasso on the right. Note that some dimensions of \\(\\mathbf{w}^*\\) vanishes only when \\(\\eta\\) is small. Conclusion: Regularization allows complex models to be trained on data sets of limited size without severe over-fitting, essentially by limiting the effective model complexity. However, the problem of determining the optimal model complexity is then shifted from one of finding the appropriate number of basis functions to one of determining a suitable value of the regularization coefficient. Multiple Outputs For multiple targets to predict, we denote the targets to be a vector \\(\\mathbf{t}\\), now we suppose the conditional distribution of the target vector takes the form \\[ p(\\mathbf{t} | \\mathbf{x}, \\mathbf{W}, \\mathbf{\\Lambda}^{-1})=\\mathcal{N}(\\mathbf{t} | \\mathbf{W}^{\\mathrm{T}} \\boldsymbol{\\phi}(\\mathbf{x}), \\mathbf{\\Lambda}^{-1}) \\] where \\(\\mathbf{W}​\\) is an \\(M\\times K​\\) weight matrix. If we have a set of \\(N​\\) observed targets denoted by an \\(N\\times K​\\) matrix \\(\\mathbf{T} = (\\mathbf{t}_{1}\\ \\mathbf{t}_{2} \\dots \\mathbf{t}_{N})^\\mathrm{T}​\\), and the corresponding inputs denoted by an \\(N\\times M​\\) matrix \\(\\mathbf{X} = (\\mathbf{x}_{1}\\ \\mathbf{x}_{2} \\dots \\mathbf{x}_{N})^\\mathrm{T}​\\), and transform it into the \\(N\\times M​\\) design matrix \\(\\mathbf{\\Phi}​\\), then the log likelihood function is then given by \\[ \\begin{aligned} &amp;\\ln p(\\mathbf{T} | \\mathbf{X}, \\mathbf{W},\\mathbf{\\Lambda}^{-1}) \\\\ =&amp;\\sum_{n=1}^{N} \\ln \\mathcal{N}(\\mathbf{t}_n | \\mathbf{W}^{\\mathrm{T}} \\boldsymbol{\\phi}(\\mathbf{x}_n), \\mathbf{\\Lambda}^{-1}) \\\\ =&amp; - \\frac{N K}{2} \\ln \\left(2 \\pi\\right) + \\frac{N }{2} \\ln |\\mathbf{\\Lambda}| -\\frac{1}{2} \\sum_{n=1}^{N}\\left(\\mathbf{t}_{n}-\\mathbf{W}^{\\mathrm{T}} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\right)^{\\mathrm{T}} \\boldsymbol{\\Lambda}\\left(\\mathbf{t}_{n}-\\mathbf{W}^{\\mathrm{T}} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\right) \\end{aligned} \\] It can be verified that by setting the gradient w.r.t. \\(\\mathbf{W}\\), we have \\[ \\mathbf{W}_{\\mathrm{ML}}=\\left(\\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{\\mathrm{T}} \\mathbf{T} \\] If we examine for each target \\(t_k\\), we will result in the same form as single target case \\[ \\mathbf{w}_{k}=\\left(\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{\\mathrm{T}} \\mathbf{t}_{k} \\] Proof of \\(\\mathbf{W}_{\\mathrm{ML}}=\\left(\\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{\\mathrm{T}} \\mathbf{T}\\): \\[ \\begin{aligned} &amp;\\frac{\\partial}{\\partial{\\mathbf{W}}}\\ln p(\\mathbf{T} | \\mathbf{X}, \\mathbf{W},\\mathbf{\\Lambda}^{-1}) \\\\ =&amp; - \\frac{1}{2} \\frac{\\partial}{\\partial{\\mathbf{W}}}\\sum_{n=1}^{N}\\left(\\mathbf{t}_{n}-\\mathbf{W}^{\\mathrm{T}} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\right)^{\\mathrm{T}} \\boldsymbol{\\Lambda}\\left(\\mathbf{t}_{n}-\\mathbf{W}^{\\mathrm{T}} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\right) \\\\ =&amp; - \\frac{1}{2} \\frac{\\partial}{\\partial{\\mathbf{W}}} \\mathrm{Tr}\\left( \\left(\\mathbf{T}- \\boldsymbol{\\Phi}\\mathbf{W}\\right) \\boldsymbol{\\Lambda}\\left(\\mathbf{T}- \\boldsymbol{\\Phi}\\mathbf{W}\\right)^{\\mathrm{T}} \\right) \\\\ =&amp; \\boldsymbol{\\Phi}^\\mathrm{T} \\left(\\mathbf{T}- \\boldsymbol{\\Phi}\\mathbf{W}\\right) \\boldsymbol{\\Lambda} \\end{aligned} \\] in the last step we use the formula \\(\\frac{\\partial}{\\partial \\mathbf{X}} \\operatorname{Tr}\\left(\\mathbf{X} \\mathbf{B} \\mathbf{X}^{T}\\right)=\\mathbf{X} \\mathbf{B}^{T}+\\mathbf{X} \\mathbf{B}\\). Then just set it to zero we can get the result.","link":"/old/PRML3-1.html"},{"title":"(PRML Notes) 3.3 Bayesian Linear Regression","text":"A series of notes taken from Pattern Recognition and Machine Learning. This section turns to a Bayesian view of linear regression, which avoids the over-fitting problem of MLE and also leads to automatic methods of determining the model complexity using a single training data set alone. Parameter Distribution For the moment we assume the precision \\(\\beta\\) in data distribution \\(p(\\boldsymbol{\\mathsf{t}}|\\mathbf{w}, \\beta) = \\mathcal{N}\\left(\\boldsymbol{\\mathsf{t}}| \\boldsymbol{\\Phi}\\mathbf{w}, \\beta^{-1}\\mathbf{I}\\right)\\) is a known constant. Then from a Bayesian view a corresponding conjugate prior takes the form \\[ p(\\mathbf{w})=\\mathcal{N}(\\mathbf{w} | \\mathbf{m}_{0}, \\mathbf{S}_{0}) \\] The posterior \\(p(\\mathbf{w} | \\boldsymbol{\\mathsf{t}})=\\mathcal{N}(\\mathbf{w} | \\mathbf{m}_{N}, \\mathbf{S}_{N})​\\) can be directly got by using the results in section 2.3.3 where \\[ \\begin{aligned} \\mathbf{m}_{N} &amp;=\\mathbf{S}_{N}\\left(\\mathbf{S}_{0}^{-1} \\mathbf{m}_{0}+\\beta \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}\\right) \\\\ \\mathbf{S}_{N}^{-1} &amp;=\\mathbf{S}_{0}^{-1}+\\beta \\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\end{aligned} \\] Note that for a Gaussian, its mode coincides with its mean, so \\(\\mathbf{w}_{\\mathrm{MAP}}=\\mathbf{m}_{N}\\). If the prior is noninformative, namely \\(\\mathbf{S}_{0}=\\alpha^{-1} \\mathbf{I}​\\) with \\(\\alpha \\rightarrow 0​\\), we can see that \\[ \\begin{align} \\mathbf{m}_{N} &amp;=\\left(\\mathbf{S}_{0}^{-1}+\\beta \\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\right)^{-1}\\left(\\mathbf{S}_{0}^{-1} \\mathbf{m}_{0}+\\beta \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}\\right) \\\\&amp;=\\left(\\beta \\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\right)^{-1}\\beta \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\\\&amp;=\\left(\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\right)^{-1}\\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\end{align} \\] which reduces to the MLE solution \\(\\mathbf{w}_\\mathrm{ML}​\\) without regularization. Later in this chapter, the prior is simplified to be \\(p(\\mathbf{w} | \\alpha)=\\mathcal{N}(\\mathbf{w} | \\mathbf{0}, \\alpha^{-1} \\mathbf{I})​\\), and we can see that now \\[ \\begin{align} \\mathbf{m}_{N} &amp;=\\left(\\mathbf{S}_{0}^{-1}+\\beta \\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\right)^{-1}\\left(\\mathbf{S}_{0}^{-1} \\mathbf{m}_{0}+\\beta \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}\\right) \\\\&amp;=\\left(\\alpha \\mathbf{I}+\\beta \\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\right)^{-1}\\beta \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\\\&amp;=\\left(\\frac{\\alpha}{\\beta} \\mathbf{I}+ \\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\right)^{-1} \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\end{align} \\] which is just the same as the result of minimizing the regularized error \\(\\frac{1}{2} \\| \\boldsymbol{\\mathsf{t}} - \\mathbf{\\Phi w} \\|^2 +\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2\\), with \\(\\lambda = \\alpha/\\beta\\). If we generalize the prior to be \\[ p(\\mathbf{w} | \\alpha)=\\left[\\frac{q}{2}\\left(\\frac{\\alpha}{2}\\right)^{1 / q} \\frac{1}{\\Gamma(1 / q)}\\right]^{M} \\exp \\left(-\\frac{\\alpha}{2} \\sum_{j=1}^{M}\\left|w_{j}\\right|^{q}\\right) \\] where \\(q=2\\) is just the Gaussian, then under this prior the solution of \\(\\mathbf{w}_{\\mathrm{MAP}}\\) is just the same as minimizing the generalized regularized error \\(\\frac{1}{2} \\| \\boldsymbol{\\mathsf{t}} - \\mathbf{\\Phi w} \\|^2 +\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^q\\). Furthermore, like before, it can be shown that if data points arrive sequentially, then the posterior distribution can be updated sequentially as a new prior for subsequent data point. Predictive Distribution To make prediction of \\(t​\\) for new \\(\\mathbf{x}​\\), we use the predictive distribution defined by \\[ p(t | \\boldsymbol{\\mathsf{t}}, \\alpha, \\beta)=\\int p(t | \\mathbf{w}, \\beta) p(\\mathbf{w} | \\boldsymbol{\\mathsf{t}}, \\alpha, \\beta) \\mathrm{d} \\mathbf{w} \\] where the posterior \\(p(\\mathbf{w} | \\boldsymbol{\\mathsf{t}}, \\alpha, \\beta)=\\mathcal{N}(\\mathbf{w} | \\mathbf{m}_{N}, \\mathbf{S}_{N})​\\) is defined as before, also the likelihood \\(p(t | \\mathbf{w}, \\beta) = \\mathcal{N} (t|\\mathbf{w}^\\mathrm{T}\\phi(\\mathbf{x}), \\beta^{-1})​\\). Again using the results in section 2.3.3, we get \\[ p(t | \\mathbf{x}, \\boldsymbol{\\mathsf{t}}, \\alpha, \\beta)=\\mathcal{N}(t | \\mathbf{m}_{N}^{\\mathrm{T}} \\boldsymbol{\\phi}(\\mathbf{x}), \\sigma_{N}^{2}(\\mathbf{x})) \\] where the variance is given by \\(\\sigma_{N}^{2}(\\mathbf{x})=\\frac{1}{\\beta}+\\phi(\\mathbf{x})^{\\mathrm{T}} \\mathbf{S}_{N} \\phi(\\mathbf{x})​\\). In the variance, the first term represents the noise on data, and the second term reflects the uncertainty w.r.t. \\(\\mathbf{w}\\). These two terms are addictive since the likelihood of new data and posterior after observing old data are independent Gaussian. Illustration of predictive distribution after observing more and more data, modeled with Gaussian basis. Sampled curves from posterior after observing more and more data. We see that the predictive uncertainty depends on \\(x\\) and is smallest in the neighborhood of the data points, since we use a localized basis function. Also note that the level of uncertainty decreases as more data points are observed. Note that since \\(\\sigma_{N}^{2}(\\mathbf{x})=\\frac{1}{\\beta}+\\phi(\\mathbf{x})^{\\mathrm{T}} \\mathbf{S}_{N} \\phi(\\mathbf{x})\\), if we use a localized basis like Gaussian, then in regions away from basis centers, \\(\\phi(\\mathbf{x})\\) will go to zero, resulting in very confident predictions when do extrapolation. Equivalent Kernel Recall that the posterior \\(p(\\mathbf{w} | \\boldsymbol{\\mathsf{t}})=\\mathcal{N}(\\mathbf{w} | \\mathbf{m}_{N}, \\mathbf{S}_{N})\\). And for the simplified prior \\(p(\\mathbf{w} | \\alpha)=\\mathcal{N}(\\mathbf{w} | \\mathbf{0}, \\alpha^{-1} \\mathbf{I})\\), the posterior mean (mode) \\(\\mathbf{m}_N = \\mathbf{w}_\\mathrm{MAP}= \\beta \\mathbf{S}_{N} \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}\\). So we can rewrite the predicted target of \\(\\mathbf{x}\\) as \\[ t = \\mathbf{w}_\\mathrm{MAP}^{\\mathrm{T}} \\boldsymbol{\\phi}(\\mathbf{x})=\\beta \\boldsymbol{\\phi}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{S}_{N} \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}=\\sum_{n=1}^{N} \\beta \\boldsymbol{\\phi}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{S}_{N} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right) t_{n} \\equiv \\sum_{n=1}^{N} k\\left(\\mathbf{x}, \\mathbf{x}_{n}\\right) t_{n} \\] where \\(k\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\beta\\boldsymbol{\\phi}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{S}_{N} \\boldsymbol{\\phi}\\left(\\mathbf{x}&#39;\\right)​\\) is known as smoother matrix or equivalent kernel. Regression functions like this that make predictions by taking linear combinations of the training set target values are known as linear smoothers. Note that the equivalent kernels have localization property that the prediction of target on \\(x\\) is formed by a weighted combination of the target values in which data points close to \\(x\\) are given higher weight than points further removed from \\(x\\). This property not only holds for localized basis function like Gaussian, but also for the nonlocal polynomial and sigmoidal basis functions. Equivalent kernel \\(k(x,x&#39;)\\) for Gaussian basis functions, shown as a 2D plot of \\(x\\) versus \\(x&#39;\\), with 3 slices for different \\(x\\) on the left. Plot of \\(k(0,x&#39;)\\), left corresponds to the polynomial basis functions, and right to sigmoidal basis functions. Intuitively, \\(k\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)\\) represents how much a point \\(\\mathbf{x}^{\\prime}\\) on the training set should respond to \\(\\mathbf{x}\\). Further insight can be obtained by considering the covariance \\[ \\begin{aligned} \\operatorname{cov}\\left[y(\\mathbf{x}), y\\left(\\mathbf{x}^{\\prime}\\right)\\right] &amp;=\\operatorname{cov}\\left[\\boldsymbol{\\phi}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{w}, \\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\left(\\mathbf{x}^{\\prime}\\right)\\right] = \\boldsymbol{\\phi}(\\mathbf{x})^{\\mathrm{T}}\\operatorname{cov}\\left[ \\mathbf{w}, \\mathbf{w}^{\\mathrm{T}} \\right]\\boldsymbol{\\phi}\\left(\\mathbf{x}^{\\prime}\\right)\\\\ &amp;=\\boldsymbol{\\phi}(\\mathbf{x})^{\\mathrm{T}} \\mathbf{S}_{N} \\boldsymbol{\\phi}\\left(\\mathbf{x}^{\\prime}\\right)=\\beta^{-1} k\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right) \\end{aligned} \\] We see that the predictive mean at nearby points will be highly correlated. Equivalent kernels can be used as an alternative approach to regression, that instead of introducing a set of basis functions, we can instead define a localized kernel directly to make predictions for new input vectors. This leads to a framework for regression call Gaussian processes. There is a pleasing result that for all \\(\\mathbf{x}\\) \\[ \\sum_{n=1}^{N} k\\left(\\mathbf{x}, \\mathbf{x}_{n}\\right)=1 \\] which can be intuitively proved that if the training data can be fitted exactly, then the summation is equivalent to the predicted target on \\(\\mathbf{x}\\), i.e. \\(\\sum_{n=1}^{N} k\\left(\\mathbf{x}, \\mathbf{x}_{n}\\right)t_n\\), where \\(t_n =1,\\ \\forall n\\). Since the model is well fitted, so all predictions will be one. Note that the kernel function can be negative as well as positive. Note that equivalent kernel satisfies an general property for kernel functions that it can be expressed in the form of inner product \\[ k(\\mathbf{x}, \\mathbf{z})=\\boldsymbol{\\psi}(\\mathbf{x})^{\\mathrm{T}} \\boldsymbol{\\psi}(\\mathbf{z}) \\] where \\(\\boldsymbol{\\psi}(\\mathbf{x})=\\beta^{1 / 2} \\mathbf{S}_{N}^{1 / 2} \\boldsymbol{\\phi}(\\mathbf{x})\\).","link":"/old/PRML3-3.html"},{"title":"(PRML Notes) 4.2 Probabilistic Generative Model","text":"A series of notes taken from Pattern Recognition and Machine Learning. This section turns to a probabilistic view of classification and show how models with linear decision boundaries arise from generative approach with simple assumptions about the distribution of the data. For the case of two class, we would like to transform the posterior to take the form \\[ \\begin{aligned} p\\left(\\mathcal{C}_{1} | \\mathbf{x}\\right) &amp;=\\frac{p\\left(\\mathbf{x} | \\mathcal{C}_{1}\\right) p\\left(\\mathcal{C}_{1}\\right)}{p\\left(\\mathbf{x} | \\mathcal{C}_{1}\\right) p\\left(\\mathcal{C}_{1}\\right)+p\\left(\\mathbf{x} | \\mathcal{C}_{2}\\right) p\\left(\\mathcal{C}_{2}\\right)} \\\\ &amp;=\\frac{1}{1+\\exp (-a)}=\\sigma(a) \\end{aligned} \\] where we have defined \\[ a\\equiv \\ln \\frac{p\\left(\\mathbf{x} | \\mathcal{C}_{1}\\right) p\\left(\\mathcal{C}_{1}\\right)}{p\\left(\\mathbf{x} | \\mathcal{C}_{2}\\right) p\\left(\\mathcal{C}_{2}\\right)} \\] Here \\(\\sigma(a)=1/(1+\\exp (-a))\\) is the logistic sigmoid function, which means S-shaped. It maps the whole real axis into a finite interval, and satisfy the simple property \\(\\sigma(-a)=1-\\sigma(a)\\). Its inverse \\(a=\\ln \\left(\\frac{\\sigma}{1-\\sigma}\\right)\\) is known as the logit function. At this time simply rewritten the posterior probabilities seems meaningless, but if \\(\\mathbf{a}(\\mathbf{x})\\) takes a simple functional form, e.g. linear in \\(\\mathbf{x}\\), the resulting decision boundary will become simple as linear. For \\(K&gt;2​\\) classes, we can change the representation to take the form of softmax function \\[ \\begin{aligned} p\\left(\\mathcal{C}_{k} | \\mathbf{x}\\right) &amp;=\\frac{p\\left(\\mathbf{x} | \\mathcal{C}_{k}\\right) p\\left(\\mathcal{C}_{k}\\right)}{\\sum_{j} p\\left(\\mathbf{x} | \\mathcal{C}_{j}\\right) p\\left(\\mathcal{C}_{j}\\right)} \\\\ &amp;=\\frac{\\exp \\left(a_{k}\\right)}{\\sum_{j} \\exp \\left(a_{j}\\right)} \\end{aligned} \\] where we can define \\(a_{k}=C\\ln \\big(p\\left(\\mathbf{x} | \\mathcal{C}_{k}\\right) p\\left(\\mathcal{C}_{k}\\right)\\big)\\), (\\(C\\) can be any constant, since canceled in fraction). The term \"softmax\" means it is a smoothed version of the \"max\" function, because if \\(a_{k} \\gg a_{j},\\ \\forall k\\neq j\\), we have \\(p\\left(\\mathcal{C}_{k} | \\mathbf{x}\\right) \\simeq 1\\), and \\(p\\left(\\mathcal{C}_{j} | \\mathbf{x}\\right) \\simeq 0\\). Continuous Inputs Formulation Now we assume the likelihood \\(p(\\mathbf{x}|\\mathcal{C}_{k})\\) of continuous input \\(\\mathbf{x}\\) is Gaussian, and all classes share the same covariance matrix, so we have \\[ p\\left(\\mathbf{x} | \\mathcal{C}_{k}\\right)=\\frac{1}{(2 \\pi)^{D / 2}} \\frac{1}{|\\mathbf{\\Sigma}|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{k}\\right)\\right\\} \\] For \\(K=2\\) classes, to represent the posterior in the form of sigmoid function, first find out the expression of \\(a\\) \\[ \\begin{align} a&amp;= \\ln \\frac{p\\left(\\mathbf{x} | \\mathcal{C}_{1}\\right) p\\left(\\mathcal{C}_{1}\\right)}{p\\left(\\mathbf{x} | \\mathcal{C}_{2}\\right) p\\left(\\mathcal{C}_{2}\\right)} \\\\ &amp;= \\ln \\frac{p\\left(\\mathbf{x} | \\mathcal{C}_{1}\\right) }{p\\left(\\mathbf{x} | \\mathcal{C}_{2}\\right) } + \\ln \\frac{ p\\left(\\mathcal{C}_{1}\\right)}{p\\left(\\mathcal{C}_{2}\\right)} \\\\ &amp;= -\\frac{1}{2} \\left(\\mathbf{x}-\\boldsymbol{\\mu}_{1}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{1}\\right) - \\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{2}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{2}\\right) + \\ln \\frac{ p\\left(\\mathcal{C}_{1}\\right)}{p\\left(\\mathcal{C}_{2}\\right)} \\\\ &amp;= \\mathbf{x}^\\mathrm{T}\\boldsymbol{\\Sigma}^{-1}\\left(\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{2}\\right) -\\frac{1}{2} \\boldsymbol{\\mu}_{1}^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}+\\frac{1}{2} \\boldsymbol{\\mu}_{2}^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{2} + \\ln \\frac{ p\\left(\\mathcal{C}_{1}\\right)}{p\\left(\\mathcal{C}_{2}\\right)} \\end{align} \\] So the posterior takes the form that linear in \\(\\mathbf{x}\\), resulting in a linear decision boundary \\[ p\\left(\\mathcal{C}_{1} | \\mathbf{x}\\right)=\\sigma\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}+w_{0}\\right) \\] where \\[ \\begin{aligned} \\mathbf{w} &amp;=\\mathbf{\\Sigma}^{-1}\\left(\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{2}\\right) \\\\ w_{0} &amp;=-\\frac{1}{2} \\boldsymbol{\\mu}_{1}^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}+\\frac{1}{2} \\boldsymbol{\\mu}_{2}^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{2}+\\ln \\frac{p\\left(\\mathcal{C}_{2}\\right)}{p\\left(\\mathcal{C}_{2}\\right)} \\end{aligned} \\] For \\(K&gt;2\\) classes, we can change the posterior to the form of softmax function \\[ \\begin{aligned} p\\left(\\mathcal{C}_{k} | \\mathbf{x}\\right) &amp;= \\frac{p\\left(\\mathbf{x} | \\mathcal{C}_{k}\\right) p\\left(\\mathcal{C}_{k}\\right)}{\\sum_{j} p\\left(\\mathbf{x} | \\mathcal{C}_{j}\\right) p\\left(\\mathcal{C}_{j}\\right)} \\\\&amp;=\\frac{\\exp \\left\\{-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{k}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{k}\\right)\\right\\} p\\left(\\mathcal{C}_{k}\\right)}{\\sum_{j} \\exp \\left\\{-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{j}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_{j}\\right)\\right\\} p\\left(\\mathcal{C}_{j}\\right)} \\\\&amp;=\\frac{\\exp \\left\\{\\mathbf{x}^\\mathrm{T}\\mathbf{\\Sigma}^{-1}\\boldsymbol{\\mu}_k -\\frac{1}{2} \\boldsymbol{\\mu}_{k}^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{k}\\right\\} p\\left(\\mathcal{C}_{k}\\right)} {\\sum_{j} \\exp \\left\\{\\mathbf{x}^\\mathrm{T}\\mathbf{\\Sigma}^{-1}\\boldsymbol{\\mu}_j -\\frac{1}{2} \\boldsymbol{\\mu}_{j}^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{j}\\right\\} p\\left(\\mathcal{C}_{j}\\right)} \\\\ &amp;=\\frac{\\exp \\left(a_{k}\\right)}{\\sum_{j} \\exp \\left(a_{j}\\right)} \\end{aligned} \\] note that we have defined \\[ a_{k}=\\mathbf{w}_{k}^{\\mathrm{T}} \\mathbf{x}+w_{k 0} \\] where \\[ \\begin{aligned} \\mathbf{w}_{k} &amp;=\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{k} \\\\ w_{k 0} &amp;=-\\frac{1}{2} \\boldsymbol{\\mu}_{k}^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{k}+\\ln p\\left(\\mathcal{C}_{k}\\right) \\end{aligned} \\] So again we have a generalized linear model. Note that if we allow \\(p\\left(\\mathbf{x} | \\mathcal{C}_{k}\\right)\\) to have its own covariance matrix \\(\\boldsymbol{\\Sigma}_{k}\\), then the resulting decision boundary will be quadratic. Maximum Likelihood Solution Consider for the case \\(K=2\\) classes, with a data set \\(\\left\\{\\mathbf{x}_{n}, t_{n}\\right\\}\\), where \\(n=1, \\dots, N\\), and we use \\(t_n=1\\) to denote class \\(\\mathcal{C}_1\\) and \\(t_n=0\\) to denote class \\(\\mathcal{C}_2\\). And we also denote the prior \\(p\\left(\\mathcal{C}_{1}\\right)=\\pi\\) and so that \\(p\\left(\\mathcal{C}_{2}\\right)=1-\\pi\\). For a data point from class \\(\\mathcal{C}_1\\) or \\(\\mathcal{C}_2\\) we have \\[ p\\left(\\mathbf{x}_{n}, \\mathcal{C}_{1}\\right)=p\\left(\\mathcal{C}_{1}\\right) p\\left(\\mathbf{x}_{n} | \\mathcal{C}_{1}\\right)=\\pi \\mathcal{N}\\left(\\mathbf{x}_{n} | \\boldsymbol{\\mu}_{1}, \\mathbf{\\Sigma}\\right) \\\\ p\\left(\\mathbf{x}_{n}, \\mathcal{C}_{2}\\right)=p\\left(\\mathcal{C}_{2}\\right) p\\left(\\mathbf{x}_{n} | \\mathcal{C}_{2}\\right)=(1-\\pi) \\mathcal{N}\\left(\\mathbf{x}_{n} | \\boldsymbol{\\mu}_{2}, \\mathbf{\\Sigma}\\right) \\] So the likelihood function of this dataset is given by \\[ \\begin{align} p\\left(\\boldsymbol{\\mathsf{t}},\\mathbf{X} \\right) &amp;= \\prod_{n=1}^N p\\left(t_n,\\mathbf{x}_n \\right) \\\\ &amp;= \\prod_{n=1}^N \\big[ p\\left(t_n,\\mathbf{x}_n,\\mathcal{C}_1 \\right) + p\\left(t_n,\\mathbf{x}_n,\\mathcal{C}_2 \\right) \\big] \\\\ &amp;= \\prod_{n=1}^N \\big[ p(\\mathcal{C}_1)p(t_n|\\mathcal{C}_1)p(\\mathbf{x}_n|\\mathcal{C}_1) + p(\\mathcal{C}_2)p(t_n|\\mathcal{C}_2)p(\\mathbf{x}_n|\\mathcal{C}_2) \\big] \\\\ &amp;= \\prod_{n=1}^N \\big[ \\pi t_n \\mathcal{N}\\left(\\mathbf{x}_{n} | \\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}\\right) + (1-\\pi)(1-t_n) \\mathcal{N}\\left(\\mathbf{x}_{n} | \\boldsymbol{\\mu}_{2}, \\boldsymbol{\\Sigma}\\right) \\big] \\\\&amp;=\\prod_{n=1}^{N}\\left[\\pi \\mathcal{N}\\left(\\mathbf{x}_{n} | \\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}\\right)\\right]^{t_{n}}\\left[(1-\\pi) \\mathcal{N}\\left(\\mathbf{x}_{n} | \\boldsymbol{\\mu}_{2}, \\boldsymbol{\\Sigma}\\right)\\right]^{1-t_{n}} \\end{align} \\] First to find the MLE of \\(\\pi\\), note that the terms in \\(\\ln p\\left(\\boldsymbol{\\mathsf{t}},\\mathbf{X} \\right)\\) that depend on \\(\\pi\\) are \\[ \\sum_{n=1}^{N}\\left\\{t_{n} \\ln \\pi+\\left(1-t_{n}\\right) \\ln (1-\\pi)\\right\\} \\] Set the derivative of it w.r.t. \\(\\pi​\\) we get \\[ \\begin{align} \\sum_{n=1}^N \\left\\{ \\frac{t_n}{\\pi} - \\frac{1-t_n}{1-\\pi} \\right\\} &amp;= 0 \\\\ \\sum_{n=1}^N \\frac{t_n}{\\pi} &amp;= \\sum_{n=1}^N\\frac{1-t_n}{1-\\pi} \\\\ \\frac{N_1}{\\pi} &amp;= \\frac{N_2}{1-\\pi} \\\\ N_1-N_1\\pi &amp;= N_2\\pi \\\\ \\pi &amp;= \\frac{N_1}{N} \\end{align} \\] So the prior of each class is given by the fraction of the training set points assigned to that class. Next consider the MLE of \\(\\boldsymbol{\\mu}_{1}​\\), note that the terms in \\(\\ln p\\left(\\boldsymbol{\\mathsf{t}},\\mathbf{X} \\right)​\\) that depend on \\(\\boldsymbol{\\mu}_{1}​\\) are \\[ \\sum_{n=1}^{N} t_{n} \\ln \\mathcal{N}\\left(\\mathbf{x}_{n} | \\boldsymbol{\\mu}_{1}, \\boldsymbol{\\Sigma}\\right)=-\\frac{1}{2} \\sum_{n=1}^{N} t_{n}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right)+\\mathrm{const} \\] Set the derivative of it w.r.t. \\(\\boldsymbol{\\mu}_{1}​\\) we get \\[ \\begin{align} \\sum_{n=1}^{N} t_{n}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right) &amp;= 0 \\\\ \\sum_{n=1}^{N} t_{n}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right) &amp;= 0 \\\\ \\sum_{n=1}^{N} t_{n}\\mathbf{x}_{n} &amp;= \\sum_{n=1}^{N} t_{n}\\boldsymbol{\\mu}_{1} \\\\ \\sum_{n=1}^{N} t_{n}\\mathbf{x}_{n} &amp;= N_1\\boldsymbol{\\mu}_{1} \\\\ \\boldsymbol{\\mu}_{1} &amp;= \\frac{1}{N_1}\\sum_{n=1}^{N} t_{n}\\mathbf{x}_{n} \\end{align} \\] which is the mean of data points in class \\(\\mathcal{C}_1\\). Similarly the MLE of \\(\\boldsymbol{\\mu}_{2}\\) is given by the mean of data points in class \\(\\mathcal{C}_2\\). \\[ \\boldsymbol{\\mu}_{2}=\\frac{1}{N_{2}} \\sum_{n=1}^{N}\\left(1-t_{n}\\right) \\mathbf{x}_{n} \\] Finally, consider the MLE of \\(\\mathbf{\\Sigma}\\), note that the terms in \\(\\ln p\\left(\\boldsymbol{\\mathsf{t}},\\mathbf{X} \\right)\\) that depend on \\(\\boldsymbol{\\mu}_{1}\\) are \\[ \\begin{align} &amp; -\\frac{1}{2} \\sum_{n=1}^{N} t_{n} \\ln |\\mathbf{\\Sigma}|-\\frac{1}{2} \\sum_{n=1}^{N} t_{n}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right) {-\\frac{1}{2} \\sum_{n=1}^{N}\\left(1-t_{n}\\right) \\ln |\\boldsymbol{\\Sigma}|-\\frac{1}{2} \\sum_{n=1}^{N}\\left(1-t_{n}\\right)\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{2}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{2}\\right)} \\\\ =&amp; -\\frac{N}{2} \\ln |\\boldsymbol{\\Sigma}| -\\frac{1}{2} \\sum_{n \\in \\mathcal{C}_{1}}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right) -\\frac{1}{2} \\sum_{n \\in \\mathcal{C}_{2}}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{2}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{2}\\right) \\\\ =&amp; -\\frac{N}{2} \\ln |\\boldsymbol{\\Sigma}| -\\frac{1}{2} \\operatorname{Tr}\\left\\{\\sum_{n \\in \\mathcal{C}_{1}}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right)\\right\\} -\\frac{1}{2} \\operatorname{Tr}\\left\\{\\sum_{n \\in \\mathcal{C}_{2}}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{2}\\right)^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{2}\\right)\\right\\} \\\\ =&amp; -\\frac{N}{2} \\ln |\\boldsymbol{\\Sigma}| -\\frac{1}{2} \\operatorname{Tr}\\left\\{\\sum_{n \\in \\mathcal{C}_{1}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right)\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right)^{\\mathrm{T}} \\right\\} -\\frac{1}{2} \\operatorname{Tr}\\left\\{\\sum_{n \\in \\mathcal{C}_{2}} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{2}\\right)\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{2}\\right)^{\\mathrm{T}} \\right\\} \\\\ =&amp; -\\frac{N}{2} \\ln |\\boldsymbol{\\Sigma}| -\\frac{1}{2} \\operatorname{Tr}\\left\\{ N_1\\boldsymbol{\\Sigma}^{-1}\\mathbf{S}_1 \\right\\} -\\frac{1}{2} \\operatorname{Tr}\\left\\{N_2\\boldsymbol{\\Sigma}^{-1}\\mathbf{S}_2 \\right\\} \\\\ =&amp;-\\frac{N}{2} \\ln |\\boldsymbol{\\Sigma}|-\\frac{N}{2} \\operatorname{Tr}\\left\\{\\boldsymbol{\\Sigma}^{-1} \\mathbf{S}\\right\\} \\end{align} \\] where we have defined \\[ \\begin{aligned} \\mathbf{S} &amp;=\\frac{N_{1}}{N} \\mathbf{S}_{1}+\\frac{N_{2}}{N} \\mathbf{S}_{2} \\\\ \\mathbf{S}_{1} &amp;=\\frac{1}{N_{1}} \\sum_{n \\in \\mathcal{C}_{1}}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right)\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{1}\\right)^{\\mathrm{T}} \\\\ \\mathbf{S}_{2} &amp;=\\frac{1}{N_{2}} \\sum_{n \\in \\mathcal{C}_{2}}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{2}\\right)\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu}_{2}\\right)^{\\mathrm{T}} \\end{aligned} \\] Similar result can be easily extended to the \\(K&gt;2\\) classes problem. Discrete Inputs For simplicity, we look at \\(D\\)-dimensional binary input values \\(x_{i} \\in\\{0,1\\}\\). A general distribution of \\(p(\\mathbf{x}|\\mathcal{C}_k)\\) requires a table of \\(2^D-1\\) independent variables to represent, which is tedious. A common approach is to use Naive Bayes, where features are treated independent conditioned on \\(\\mathcal{C}_k\\), so that \\[ p\\left(\\mathbf{x} | \\mathcal{C}_{k}\\right)=p\\left(x_1 | \\mathcal{C}_{k}\\right)\\cdots p\\left(x_D | \\mathcal{C}_{k}\\right) = \\prod_{i=1}^{D} \\mu_{k i}^{x_{i}}\\left(1-\\mu_{k i}\\right)^{1-x_{i}} \\] where \\(\\mu_{ki} = p(x_i=1|\\mathcal{C}_k)\\) is parameter to be determined. With this approach, the discriminant function is again linear in \\(\\mathbf{x}\\) since in softmax function we can take \\[ a_k = \\ln \\big(p\\left(\\mathbf{x} | \\mathcal{C}_{k}\\right) p\\left(\\mathcal{C}_{k}\\right)\\big)=\\sum_{i=1}^{D}\\left\\{x_{i} \\ln \\mu_{k i}+\\left(1-x_{i}\\right) \\ln \\left(1-\\mu_{k i}\\right)\\right\\}+\\ln p\\left(\\mathcal{C}_{k}\\right) \\] Exponential Family Actually there is a more general result that can be obtained by assuming that the class-conditional densities \\(p(x|\\mathcal{C}_k)\\) are members of the exponential family of distributions with the restriction that \\(\\mathbf{u}(\\mathbf{x})=\\mathbf{x}\\), so the densities take the form \\[ p\\left(\\mathbf{x} | \\boldsymbol{\\lambda}_{k}\\right)=h(\\mathbf{x}) g\\left(\\boldsymbol{\\lambda}_{k}\\right) \\exp \\left\\{\\boldsymbol{\\lambda}_{k}^{\\mathrm{T}} \\mathbf{x}\\right\\} \\] For \\(K=2\\) classes problem, change the posterior into the sigmoid function we have \\[ a= \\ln \\frac{p\\left(\\mathbf{x} | \\mathcal{C}_{1}\\right) p\\left(\\mathcal{C}_{1}\\right)}{p\\left(\\mathbf{x} | \\mathcal{C}_{2}\\right) p\\left(\\mathcal{C}_{2}\\right)}=\\left(\\boldsymbol{\\lambda}_{1}-\\boldsymbol{\\lambda}_{2}\\right)^{\\mathrm{T}} \\mathbf{x}+\\ln g\\left(\\boldsymbol{\\lambda}_{1}\\right)-\\ln g\\left(\\boldsymbol{\\lambda}_{2}\\right)+\\ln p\\left(\\mathcal{C}_{1}\\right)-\\ln p\\left(\\mathcal{C}_{2}\\right) \\] For \\(K&gt;2\\) classes problem, change the posterior into the softmax function we can take \\[ a_k = \\ln \\big(p\\left(\\mathbf{x} | \\mathcal{C}_{k}\\right) p\\left(\\mathcal{C}_{k}\\right)\\big)=\\boldsymbol{\\lambda}_{k}^{\\mathrm{T}} \\mathbf{x}+\\ln g\\left(\\boldsymbol{\\lambda}_{k}\\right)+\\ln p\\left(\\mathcal{C}_{k}\\right) \\] which are all linear in \\(\\mathbf{x}\\).","link":"/old/PRML4-2.html"},{"title":"Markov Decision Process","text":"Markov decision processes formally describe an environment for reinforcement learning, where the environment is fully observable. Markov Process Definition A Markov Process (or Markov Chain) is a memoryless random process (which satisfies Markov Property), denoted by a tuple \\(⟨\\mathcal{S},\\mathcal{P}⟩\\), where \\(\\mathcal{S}\\) is a (finite) set of states \\(\\mathcal{P}\\) is a state transition probability matrix, \\(\\mathcal{P}_{ss&#39;} = \\mathbb{P}[S_{t+1}=s&#39;|S_t=s]\\) State Transition Matrix State transition matrix \\(\\mathcal{P}\\) defines transition matrix probability from all states \\(s\\) to all successor states \\(s′\\), \\[ \\mathcal{P} = \\begin{bmatrix} \\mathcal{P}_{11} &amp; \\cdots &amp; \\mathcal{P}_{1n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathcal{P}_{n1} &amp; \\cdots &amp; \\mathcal{P}_{nn} \\end{bmatrix} \\] where \\[ \\mathcal{P}_{ss&#39;} = \\mathbb{P}[S_{t+1}=s&#39;|S_t=s] \\] Note that each row of \\(\\mathcal{P}\\) sums to 1. A student Markov Chain example with its transition matrix. Markov Reward Process A Markov reward process is a Markov chain with values, denoted by a tuple \\(\\left\\langle \\mathcal{S,P,\\boldsymbol{R},\\boldsymbol{\\gamma}}\\right\\rangle\\) where \\(\\mathcal{S}\\) is a finite set of states \\(\\mathcal{P}\\) is a state transition probability matrix, \\(\\mathcal{P}_{ss&#39;} = \\mathbb{P}[S_{t+1}=s&#39;|S_t=s]\\) \\(\\boldsymbol{\\mathcal{R}}\\) is a reward function, \\(\\mathcal{R}_s=\\mathbb{E}[R_{t+1}|S_{t}=s]\\) \\(\\boldsymbol{\\mathcal{\\gamma}}\\) is a discount factor, \\(\\mathcal{\\gamma} \\in [0,1]\\) Student MRP, note that each reward is associated to a state, denoting the reward AFTER reaching that state Return The return \\(G_t\\) is the total discounted reward from time-step \\(t\\) \\[ G_t = R_{t+1}+\\gamma{R}_{t+2}+…=\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1} \\] Reasons of using the discount factor \\(\\gamma\\): Mathematically convenient to compute the discounted rewards Avoids infinite returns in cyclic Markov processes Uncertainty about the future may not be fully presented Focus more on immediate reward above delayed reward Human behavior shows preference for immediate reward Value Function The state value function \\(v(s)\\) of an MRP is a expected return starting from state \\(s\\) \\[ v(s) = \\mathbb{E}[G_t|S_t=s] \\] Bellman Equation State value function for Student MRP, using Bellman Equation, for example, \\(-5.0 = -2 + 0.9\\times (-7.6\\times 0.5+0.9\\times 0.5)\\) It can be easily shown that \\[ \\begin{aligned} v(s) &amp;= \\mathbb{E}[R_{t+1}+\\gamma{v}(S_{t+1})|S_t=s]\\\\ &amp;= \\mathcal{R}_s + \\gamma\\sum_{s&#39; \\in \\mathcal{S}} \\mathcal{P}_{ss&#39;}v(s&#39;) \\end{aligned} \\] To express concisely into matrix form \\[ v = \\mathcal{R} + \\gamma{\\mathcal{P}}v \\] Then it can be solved directly since it’s a linear equation. Since the complexity of matrix inversion is \\(O(n^3)\\), direct solution is only possible for small MRPs. For large MRPs we need to use iterative methods. Markov Decision Process A Markov decision process (MDP) is a Markov reward process with decisions, denoted by a tuple \\(\\left\\langle \\mathcal{S,\\boldsymbol{A},P,R,\\gamma}\\right\\rangle\\) where \\(\\mathcal{S}\\) is a finite set of states \\(\\boldsymbol{\\mathcal{A}}\\) is a finite state of actions \\(\\mathcal{P}\\) is a state transition probability matrix, \\(\\mathcal{P}_{ss&#39;}^\\boldsymbol{a} = \\mathbb{P}[S_{t+1}=s&#39;|S_t=s,A_t=\\boldsymbol{a}]\\) \\(\\mathcal{R}\\) is a reward function, \\(\\mathcal{R}_s^\\boldsymbol{a}=\\mathbb{E}[R_{t+1}|S_{t}=s,A_t=\\boldsymbol{a}]\\) \\(\\mathcal{\\gamma}\\) is a discount factor, \\(\\mathcal{\\gamma} \\in [0,1]\\) Policies A policy \\(\\pi\\) is a distribution over actions given states, fully defines the behavior of an agent \\[ \\pi{(a|s)} = \\mathbb{P}[A_t=a|S_t=s] \\] It can be easily shown that by definition, \\[ \\mathcal{P}_{ss&#39;}^{\\pi} = \\sum_{a\\in\\mathcal{A}}\\pi{(a|s)}\\mathcal{P}_{ss&#39;}^a \\\\ \\mathcal{R}_{s}^{\\pi} = \\sum_{a\\in\\mathcal{A}}\\pi{(a|s)}\\mathcal{R}_{s}^a \\] Value Function Different from the value function defined in MRP, we can define two kinds of value functions in MDP. The state value function \\(v_\\pi(s)\\) of an MDP is the expected return starting from state \\(s\\), and then following policy \\(\\pi\\) \\[ v_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_t|S_t=t] \\] The action value function \\(q_\\pi(s)\\) of an MDP is the expected return starting from state \\(s\\), taking an action \\(a\\), and then following policy \\(\\pi\\) \\[ q_{\\pi}(s,a) = \\mathbb{E}_{\\pi}[G_t|S_t=t, A_t=a] \\] Bellman Expectation Equation Suppose an agent follows a specific policy \\(\\pi\\) (may not be deterministic). By decomposing state value function for one action onward, \\[ v_{\\pi}(s) = \\sum_{a\\in{\\mathcal{A}}} \\pi{(a|s)} q_{\\pi}(s,a) \\] By decomposing action value function for one state onward, \\[ q_{\\pi}(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_\\pi(s&#39;) \\] Two steps onward for state value function, \\[ v_{\\pi}(s) = \\sum_{a\\in{\\mathcal{A}}} \\pi{(a|s)} \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_\\pi(s&#39;) \\right) \\] Two steps onward for action value function, \\[ q_{\\pi}(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a \\sum_{a&#39;\\in{\\mathcal{A}}} \\pi{(a&#39;|s&#39;)} q_{\\pi}(s&#39;,a&#39;) \\] Using the induced MRP (just follows the transition matrix and reward vector mentioned in the Policy section), the Bellman Equation can be expressed concisely, \\[ v_\\pi = \\mathcal{R^\\pi}+ \\gamma \\mathcal{P^{\\pi}}v_\\pi \\] which can also be solved directly with inversed matrix \\[ v_\\pi = (1- \\gamma \\mathcal{P^{\\pi}})^{-1}\\mathcal{R^\\pi} \\] Example Again, in the student MDP example, which is slightly modified from the student MRP, with \\(\\pi(a|s)=0.5\\) everywhere and \\(\\gamma=1\\). For the state value 7.4, it is computed by \\(0.5*(1-0.2*1.3+0.4*2.7+0.4*7.4)+0.5*10\\), which is a two steps onward computation. Optimal Value Functions The optimal state value function and optimal action value function over all policies, specifies the best possible performance in the MDP \\[ v_*(s) = \\max_{\\pi} v_{\\pi}(s)\\\\ q_*(s,a) = \\max_{\\pi} q_{\\pi}(s,a) \\] we say that an MDP is “solved” when the optimal value is known. Optimal Policy First define a partial ordering over all policies \\[ \\pi \\geq \\pi&#39; \\ \\ \\text{if}\\ \\ v_\\pi(s)\\geq v_{\\pi&#39;}(s), \\forall s \\] (Why is partial ordering? — There may be two policies that for some state \\(s\\), one value is greater than the other, but for some other state \\(s′\\) isn't.) then the following three theorems comes for any MDP, There exists an optimal policy \\(\\pi^*\\) that \\(\\pi^*\\geq \\pi\\), \\(\\forall\\pi\\) All optimal policies achieve the optimal state-value function, \\(v_{\\pi^*}(s) = v_*(s)​\\) All optimal policies achieve the optimal action-value function, \\(q_{\\pi^*}(s,a) = q_*(s,a)\\) There is always a deterministic optimal policy for any MDP, and if we know \\(q_*(s,a)\\), we immediately have the optimal policy \\[ \\pi_*(a|s) = \\left\\{ \\begin{align} 1 &amp; &amp; &amp;\\text{if} \\ \\ a = \\mathop{\\arg\\max}_{a\\in \\mathcal{A}} q_*(s,a) \\\\ 0 &amp; &amp;&amp; \\text{o.w.} \\end{align}\\right. \\] Bellman Optimality Equation Again, if some agent follows the optimal policy \\(\\pi_*\\), then from the theorem above we know it achieves both the optimal state value function and optimal action value function. By decomposing optimal action value function for one state onward, \\[ v_*(s) = \\max_{a\\in{\\mathcal{A}}} q_*(s,a) \\] By decomposing optimal action value function for one state onward, \\[ q_*(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_*(s&#39;) \\] Two steps onward for optimal state value function, \\[ v_*(s) = \\max_{a\\in{\\mathcal{A}}} \\left( \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a v_*(s&#39;) \\right) \\] Two steps onward for optimal action value function, \\[ q_*(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s&#39;\\in{\\mathcal{S}}} \\mathcal{P}_{ss&#39;}^a \\max_{a&#39;\\in{\\mathcal{A}}} q_*(s&#39;,a&#39;) \\] Example Again, for the student MDP example, the optimal state value function at red state is shown below: Solving the Bellman Optimality Equation Since the Bellman optimality equation is non-linear (maxmax op in it), there’s no closed form solution (in general). However, many iterative solution methods have been designed, and will be introduced in detail in the following notes: Value iteration Policy iteration Q-learning Sarsa Reference [1] David Silver's RL course","link":"/old/RL_MDP.html"},{"title":"Model-Free Prediction","text":"In this and next notes, different from the DP, we do all the things within an unknown MDP, just what the model-free means. Monte-Carlo Learning The basic idea is very simple, it uses the mean return of episodes as the true value. Learn from complete episodes, which refers to the procedure from current state to the final state. (The opposite learning method is bootstrapping, which means updating the current state value from incomplete episodes.) Restrict to the learning method, MC can only be applied to episodic MDPs, whose all episodes must terminates. MC is model-free, it has no knowledge of MDP transitions or rewards. In detail, MC's goal is to learn \\(v_\\pi\\) from episodes of experience under some policy \\(\\pi\\), like \\(S_1, A_1,R_2, ...,S_k \\sim \\pi\\), and using empirical mean-return instead of expected-return: \\[ v_\\pi(s) = \\mathbb{E}_\\pi [G_t|S=s] \\\\ \\text{estimated}~v_\\pi(s) = \\frac{1}{N(s)}\\sum_{i=1}^{N(s)} (G_{t})_i \\] where \\(G_t = R_{t+1}+\\gamma{R}_{t+2}+…\\gamma^{T-1}R_T\\) is the return of the whole episode. Basic Algorithm There are two slightly different strategies to update \\(v_\\pi(s)\\). Since within a single episode, one state may be visited more than one time, the strategies differ in whether to regard only the first visiting time as a beginning of an episode. In detail, one strategy, the first-visit Monte-Carlo policy evaluation for some state \\(s\\) is For the FIRST time-step \\(t​\\) that state \\(s​\\) is visited in an episode Increment counter \\(N(s) \\leftarrow N(s)+1​\\) and total return \\(S(s) \\leftarrow S(s)+G_t​\\) Value is evaluated by averaging over the total return \\(V(s) \\leftarrow S(s)/N(s) ​\\) Repeat steps 1-3, and by law of large numbers, \\(\\mathop{\\lim}_{N(s)\\rightarrow \\infty}V(s)=v_\\pi(s)\\) The other strategy, every-visit Monte-Carlo policy evaluation sum up the return of EVERY time-step \\(t​\\) that state \\(s​\\) is visited into total return \\(V(s)​\\). Example: Blackjack \\(\\mathcal{S}​\\) : \\(10\\times 10\\times 10=200​\\) in total current sum (12-21), if \\(sum &lt; 12​\\) just automatically twist Dealer's showing card (ace-10) whether have a \"usable\" ace (yes-no) \\(\\mathcal{A}\\) : 2 kinds of actions stick -- stop receiving cards twist -- take another card \\(\\mathcal{R}​\\) and \\(\\mathcal{P}​\\): if stick, game terminates, \\(R=+1,0,1\\) for sum of cards \\(&gt;,=,&lt;\\) dealer's cards, respectively if twist, \\(R=-1\\) when cards \\(&gt; 21\\) and game terminates, \\(R=0\\) otherwise. \\(\\mathcal{\\gamma}\\) : \\(0.9\\) For a very simple policy that stick if sum of cards \\(&gt;20​\\) (o.w. twist), the value function after MC learning is shown below Incremental Algorithm Since the mean \\(\\mu_1, \\mu_2, ...\\) of a sequence \\(x_1, x_2,...\\) can be computed incrementally, called incremental mean \\[ \\begin{align} \\mu_k &amp;= \\frac{1}{k} \\sum_{j=1}^k x_j \\\\ &amp;= \\frac{1}{k}\\left(x_k+ \\sum_{j=1}^{k-1} x_j\\right) \\\\ &amp;= \\frac{1}{k}(x_k+ (k-1)\\mu_{k-1} ) \\\\ &amp;= \\mu_{k-1} + \\frac{1}{k}(x_k - \\mu_{k-1} ) \\end{align} \\] Following this idea, we have a new update method. After one episode \\(S_1, A_1, R_2,...,S_T\\), for EVERY visited state \\(S_t\\) with return \\(G_t\\) \\(N(S_t) \\leftarrow N(S_t)+1\\) \\(V(S_t) \\leftarrow V(S_t) + \\frac{1}{N(S_t)}(G_t-V(S_t))\\) Now here comes our final idea, just like momentum is a modified version of SGD, this method keep tracking a running mean, by introducing a learning rate \\(\\alpha\\) to replace the term \\({1}/{N(S_t)}\\), so it forgets too old episodes, useful in non-stationary problems \\[ V(S_t) \\leftarrow V(S_t) + \\alpha(G_t-V(S_t)) \\] Time-Difference Learning TD only differs from MC only in learning from incomplete episodes, by bootstrapping. Compare Incremental every-visit MC: \\(V(S_t) \\leftarrow V(S_t) + \\alpha(\\mathbf{G_t}-V(S_t))\\) Simplest TD algorithm, TD(0): \\[V(S_t) \\leftarrow V(S_t) + \\alpha(\\mathbf{R_{t+1} + \\gamma V(S_{t+1})}-V(S_t))\\] Here the estimated (not actual as in MC) return \\[R_{t+1} + \\gamma V(S_{t+1})\\] is call the TD target, \\[\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\] is called the TD error. Now Let's go through some examples to intuitively feel about the difference between the TD and MC. Example: Driving Home State Elapsed Time (min) Predicted Time to go Predicted Total Time leaving office 0 30 30 reach car, raining 5 35 40 exit highway 20 15 35 behind truck 30 10 40 home street 40 3 43 arrive home 43 0 43 MC vs. TD We can see from the first example that Advantages and Disadvantages of MC vs. TD -- 1 MC TD Learn before knowing final outcome No Yes Furthermore, Learn without final outcome No Yes Example: Random Walk Start from C, choose each direction with equal possibility, without discounting Training error between MC and TD with different learning rate. (Here are some results without any proof.) Advantages and Disadvantages of MC vs. TD -- 2 MC TD Variance high variance low variance Bias zero bias some bias Initial value not very sensitive to initial value sensitive to initial value Efficiency simple but time consuming more efficient than MC Convergence good approximation properties (even with function approximation) only TD(0) converges to \\(v_\\pi(s)\\) without bias (?) (but not always with function approximation under some special cases) Example: AB From the above we know that \\(V(s)\\rightarrow v_\\pi(s)\\) as experience \\(\\rightarrow \\infty\\), but what about we only obtain finite experience, namely, only some episodes (say \\(K\\) episodes) we have experienced, like \\[ s_1^1,a_1^1,r_2^1,...,s_{T_1}^1 \\\\ \\vdots \\\\ s_1^K,a_1^K,r_2^K,...,s_{T_K}^K \\] One possible method is to repeatedly sample the \\(K\\) episodes, feed them to find out the value function via MC or TD(0). Consider the situation that we experienced only two states A and B within 8 episodes, what value will we finally get for each state without discounting? By MC, we will get \\(V (A) = 0\\), since MC converges to solution with minimum mean-squared error, in other words, MC solution best fit to the observed returns \\[ \\sum_{k=1}^{K}\\sum_{t=1}^{T_k}(G_t^k-V(s_t^k))^2 \\] By TD(0), we will get \\(V (A) = 3/4\\), since TD(0) converges to solution of max likelihood Markov model, in other words, TD(0) finds out an MDP that best fits the data \\[ \\hat{\\mathcal{P}}^a_{ss&#39;}=\\frac{1}{N(s,a)} \\sum_{k=1}^{K}\\sum_{t=1}^{T_k}\\mathbf{1}(s_t^k,a_t^k,s_{t+1}^k=s,a,s&#39;) \\\\ \\hat{\\mathcal{R}}^a_{s}=\\frac{1}{N(s,a)} \\sum_{k=1}^{K}\\sum_{t=1}^{T_k}\\mathbf{1}(s_t^k,a_t^k=s,a)r_t^k \\] Advantages and Disadvantages of MC vs. TD -- 3 MC TD Exploit Markov property No Yes Usually more efficient in Markov environments Usually more effective in non-Markov environments Unified View of DP, MC and TD MC backup: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha (G_t-V(S_t)) \\] TD backup: \\[ V(S_t) \\leftarrow V(S_t)+ \\alpha(R_{t+1} + \\gamma V(S_{t+1}-V(S_t))) \\] DP backup: \\[ V(S_t) \\leftarrow \\mathbb{E}_\\pi[R_{t+1}+\\gamma V(S_{t+1})] \\] Comparison: Bootstrapping (update involves an estimate) Sampling (update samples an expectation) MC No Yes TD Yes Yes DP Yes No Two dimensions stands for sampling depth and width, respectively. TD(\\(\\lambda\\)) Till now, the TD algorithm we introduced is actually TD(0), which only looks one step forward. What about looking 2 or more steps forward? n-Step Prediction First of all, define the n-step returns, \\[ G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} +...+ \\gamma^{n-1} R_{t+n}+\\gamma^nV(S_{t+n}) \\] For example, \\[ \\begin{align} \\text{(TD)}~~~~~ G_t^{(1)} &amp;= R_{t+1} + \\gamma V(S_{t+1}) \\\\ G_t^{(2)} &amp;= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2}) \\\\ &amp;~~\\vdots \\\\ \\text{(MC)} ~~~ G_t^{(\\infty)} &amp;= R_{t+1} + \\gamma R_{t+2} + \\gamma^{T-1} R_T \\end{align} \\] And so we have the n-step temporal-difference learning \\[ V(S_t) \\leftarrow V(S_t)+ \\alpha(G_t^{(n)}-V(S_t)) \\] Example: Large Random Walk A basic problem from n-step TD algorithm is which n should we choose to achieve the best result? Researchers have made predictions under different step \\(n\\), different learning rate \\(\\alpha\\), and different training methods -- on-line(update the value after every step within an episode) and off-line (update values at the end of one episode). Two dimensions stands for sampling depth and width, respectively. We can see from the figure that the best \\(n\\) varies in different settings. In order to make an overall consideration, here we introduce a new parameter \\(\\lambda\\) to combine the information from all different time-steps. \\(\\lambda\\)-return The \\(\\lambda\\)-return \\(G_t^\\lambda\\) combines all \\(n\\)-step returns \\(G_t^{(n)}\\) by geometrically weighting them, which requires quite a little additional computation cost. \\[ G_t^\\lambda = (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} G_t^{(n)} \\\\ V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^\\lambda- V(S_t)) \\] It can easily proved that by setting the last term's coefficient to \\(\\lambda^{T-t-1}\\) we have \\[ \\sum_{k=t+1}^{T-1} (1-\\lambda)\\lambda^{k-t-1} + \\lambda^{T-t-1} = 1 \\] Reference [1] David Silver's RL course","link":"/old/RL_prediction.html"},{"title":"(PRML Notes) 3.5 The Evidence Approximation","text":"A series of notes taken from Pattern Recognition and Machine Learning. General Framework of Fully Bayesian Approach In a fully Bayesian treatment, a prior will be introduced over precision hyperparameters \\(\\alpha\\) and \\(\\beta\\), and predictions are made by marginalizing w.r.t. these hyperparameters as well as \\(\\mathbf{w}\\) \\[ p(t | \\boldsymbol{\\mathsf{t}})=\\iiint p(t | \\mathbf{w}, \\beta) p(\\mathbf{w} | \\boldsymbol{\\mathsf{t}}, \\alpha, \\beta) p(\\alpha, \\beta | \\boldsymbol{\\mathsf{t}}) \\mathrm{d} \\mathbf{w} \\mathrm{d} \\alpha \\mathrm{d} \\beta \\tag{1} \\] where the posterior distribution of \\(\\alpha​\\) and \\(\\beta​\\) is given by \\[ p(\\alpha, \\beta | \\boldsymbol{\\mathsf{t}}) \\propto p(\\boldsymbol{\\mathsf{t}} | \\alpha, \\beta) p(\\alpha, \\beta) \\] Usually the complete marginalization over all the parameters in \\(p(t | \\boldsymbol{\\mathsf{t}})\\) is analytically intractable, but we can integrate over \\(\\mathbf{w}\\) under specific values of \\(\\alpha\\) and \\(\\beta\\), so an approximation will be applied for the fully Bayesian treatment. Under such approximation, the fully Bayesian treatment just takes one more step to find out the optimal \\(\\alpha\\) and \\(\\beta\\), instead of manually set them. The approximation is based on the assumption that the prior of hyperparameters \\(p(\\alpha, \\beta)\\) is relatively flat, and the posterior \\(p(\\alpha, \\beta | \\boldsymbol{\\mathsf{t}})\\) will be sharply peaked around values \\(\\widehat{\\alpha}\\) and \\(\\widehat{\\beta}\\). Then we can set \\(\\alpha\\) and \\(\\beta\\) to the most proper values by maximizing the marginal likelihood function \\(p(\\boldsymbol{\\mathsf{t}} | \\alpha, \\beta)\\), which itself is obtained by integrating over \\(\\mathbf{w}\\) \\[ \\widehat{\\alpha}, \\widehat{\\beta} = \\mathop{\\arg\\max}_{\\alpha, \\beta} p(\\boldsymbol{\\mathsf{t}} | \\alpha, \\beta)=\\mathop{\\arg\\max}_{\\alpha, \\beta} \\int p(\\boldsymbol{\\mathsf{t}} | \\mathbf{w}, \\beta) p(\\mathbf{w} | \\alpha) \\mathrm{d} \\mathbf{w} \\] Then the predictive distribution \\((1)\\) now becomes \\[ p(t | \\boldsymbol{\\mathsf{t}}) \\simeq p(t | \\boldsymbol{\\mathsf{t}}, \\widehat{\\alpha}, \\widehat{\\beta})=\\int p(t | \\mathbf{w}, \\widehat{\\beta}) p(\\mathbf{w} | \\boldsymbol{\\mathsf{t}}, \\widehat{\\alpha}, \\widehat{\\beta}) \\mathrm{d} \\mathbf{w} \\] This framework is known in the machine learning literature as evidence approximation. Honestly speaking, the decision of choosing \\(\\alpha\\) and \\(\\beta\\) is just kind of MLE, which maximizes the model evidence \\(p(\\boldsymbol{\\mathsf{t}} | \\alpha, \\beta)\\) under different choices of \\(\\alpha\\) and \\(\\beta\\), and I think the Bayesian are just not willing to admit that so they introduce such assumption to explain the choice of \\(\\alpha\\) and \\(\\beta\\) in a Bayesian way :) Evaluation of the Evidence Function In this section we aim to find out the exact expression for the evidence function \\(p(\\boldsymbol{\\mathsf{t}} | \\alpha, \\beta)\\), under the case of linear regression as we discussed before. Recall that we have defined \\[ p(\\mathbf{w} | \\alpha)=\\mathcal{N}(\\mathbf{w} | \\mathbf{0}, \\alpha^{-1} \\mathbf{I}) \\\\ p(\\boldsymbol{\\mathsf{t}}|\\mathbf{w}, \\beta) = \\mathcal{N}\\left(\\boldsymbol{\\mathsf{t}}| \\boldsymbol{\\Phi}\\mathbf{w}, \\beta^{-1}\\mathbf{I}\\right) \\] To evaluate the evidence, one way is to simply use the result in section 2.3.3. However, here we need to make use of the standard form for the normalization coefficient of a Gaussian, so we will evaluate the integral from scratch by completing the suqare. \\[ \\begin{align} p(\\boldsymbol{\\mathsf{t}} | \\alpha, \\beta) &amp;= \\int p(\\boldsymbol{\\mathsf{t}} | \\mathbf{w}, \\beta) p(\\mathbf{w} | \\alpha) \\mathrm{d} \\mathbf{w} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\left(\\frac{\\alpha}{2 \\pi}\\right)^{M / 2}\\int \\exp \\left\\{ -\\frac{\\beta}{2}\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi} \\mathbf{w}\\|^{2}-\\frac{\\alpha}{2} \\mathbf{w}^{\\mathrm{T}} \\mathbf{w} \\right\\} \\mathrm{d} \\mathbf{w} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\left(\\frac{\\alpha}{2 \\pi}\\right)^{M / 2}\\int \\exp \\left\\{ -\\frac{\\beta}{2}\\left(\\boldsymbol{\\mathsf{t}}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}-2 \\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}+\\mathbf{w}^{\\mathrm{T}} \\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\mathbf{w}\\right)-\\frac{\\alpha}{2} \\mathbf{w}^{\\mathrm{T}} \\mathbf{w} \\right\\} \\mathrm{d} \\mathbf{w} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\left(\\frac{\\alpha}{2 \\pi}\\right)^{M / 2}\\int \\exp \\left\\{ -\\frac{1}{2}\\mathbf{w}^{\\mathrm{T}} \\left(\\alpha\\mathbf{I}+\\beta\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\right) \\mathbf{w} +\\beta \\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} -\\frac{\\beta}{2} \\boldsymbol{\\mathsf{t}}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\right\\} \\mathrm{d} \\mathbf{w} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\left(\\frac{\\alpha}{2 \\pi}\\right)^{M / 2}\\int \\exp \\left\\{ -\\frac{1}{2}\\mathbf{w}^{\\mathrm{T}} \\mathbf{A} \\mathbf{w} + \\mathbf{w}^{\\mathrm{T}} \\mathbf{A} \\left(\\beta\\mathbf{A}^{-1}\\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}\\right) -\\frac{\\beta}{2} \\boldsymbol{\\mathsf{t}}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\right\\} \\mathrm{d} \\mathbf{w} &amp;&amp; \\scriptstyle{(\\text{define } \\mathbf{A} = \\alpha\\mathbf{I}+\\beta\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi}\\text{, note that } \\mathbf{A}^\\mathrm{T} = \\mathbf{A})} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\left(\\frac{\\alpha}{2 \\pi}\\right)^{M / 2}\\int \\exp \\left\\{ -\\frac{1}{2}\\mathbf{w}^{\\mathrm{T}} \\mathbf{A} \\mathbf{w} + \\mathbf{w}^{\\mathrm{T}} \\mathbf{A} \\mathbf{m}_{N} -\\frac{\\beta}{2} \\boldsymbol{\\mathsf{t}}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\right\\} \\mathrm{d} \\mathbf{w} &amp;&amp; \\scriptstyle{(\\text{define } \\mathbf{m}_{N}=\\beta \\mathbf{A}^{-1} \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}})} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\left(\\frac{\\alpha}{2 \\pi}\\right)^{M / 2}\\int \\exp \\left\\{ -\\frac{1}{2}\\left(\\mathbf{w}-\\mathbf{m}_{N}\\right)^{\\mathrm{T}} \\mathbf{A}\\left(\\mathbf{w}-\\mathbf{m}_{N}\\right) +\\frac{1}{2}\\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{A} \\mathbf{m}_{N} -\\frac{\\beta}{2} \\boldsymbol{\\mathsf{t}}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\right\\} \\mathrm{d} \\mathbf{w} \\tag{2} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\left(\\frac{\\alpha}{2 \\pi}\\right)^{M / 2} (2 \\pi)^{M / 2}|\\mathbf{A}|^{-1 / 2}\\exp \\left\\{ -\\frac{\\beta}{2} \\boldsymbol{\\mathsf{t}}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} +\\frac{1}{2}\\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{A} \\mathbf{m}_{N} \\right\\} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\alpha^{M / 2} |\\mathbf{A}|^{-1 / 2}\\exp \\left\\{ -\\frac{\\beta}{2} \\boldsymbol{\\mathsf{t}}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} +\\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{A} \\mathbf{m}_{N} -\\frac{1}{2}\\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{A} \\mathbf{m}_{N} \\right\\} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\alpha^{M / 2} |\\mathbf{A}|^{-1 / 2}\\exp \\left\\{ -\\frac{\\beta}{2} \\boldsymbol{\\mathsf{t}}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} +\\beta\\boldsymbol{\\mathsf{t}}^{\\mathrm{T}}\\boldsymbol{\\Phi} \\mathbf{m}_{N} -\\frac{1}{2}\\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{A} \\mathbf{m}_{N} \\right\\} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\alpha^{M / 2} |\\mathbf{A}|^{-1 / 2}\\exp \\left\\{ -\\frac{\\beta}{2} \\|\\boldsymbol{\\mathsf{t}} - \\boldsymbol{\\Phi}\\mathbf{m}_N\\|^2 +\\frac{\\beta}{2} \\mathbf{m}_{N}^{\\mathrm{T}} \\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi} \\mathbf{m}_{N} -\\frac{1}{2}\\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{A} \\mathbf{m}_{N} \\right\\} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\alpha^{M / 2} |\\mathbf{A}|^{-1 / 2}\\exp \\left\\{ -\\frac{\\beta}{2} \\|\\boldsymbol{\\mathsf{t}} - \\boldsymbol{\\Phi}\\mathbf{m}_N\\|^2 - \\mathbf{m}_{N}^{\\mathrm{T}}\\left( -\\frac{\\beta}{2} \\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi} + \\frac{\\alpha}{2}\\mathbf{I}+\\frac{\\beta}{2}\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\right)\\mathbf{m}_{N} \\right\\} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\alpha^{M / 2} |\\mathbf{A}|^{-1 / 2}\\exp \\left\\{ -\\frac{\\beta}{2} \\|\\boldsymbol{\\mathsf{t}} - \\boldsymbol{\\Phi}\\mathbf{m}_N\\|^2 -\\frac{\\alpha}{2} \\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N} \\right\\} \\\\ &amp;= \\left(\\frac{\\beta}{2 \\pi}\\right)^{N / 2} \\alpha^{M / 2} |\\mathbf{A}|^{-1 / 2}\\exp \\{ -E\\left(\\mathbf{m}_{N}\\right) \\} &amp;&amp;\\scriptstyle{(\\text{define } E\\left(\\mathbf{m}_{N}\\right)=\\frac{\\beta}{2}\\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2}+\\frac{\\alpha}{2} \\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N} )} \\end{align} \\] So we have the log of evidence function to be \\[ \\ln p(\\boldsymbol{\\mathsf{t}} | \\alpha, \\beta)=\\frac{M}{2} \\ln \\alpha+\\frac{N}{2} \\ln \\beta-E\\left(\\mathbf{m}_{N}\\right)-\\frac{1}{2} \\ln |\\mathbf{A}|-\\frac{N}{2} \\ln (2 \\pi) \\] Polynomial Curve FItting Example Return to the problem of polynomial curve fitting problem in section 1.1, if we plot the model evidence against the order of the polynomial \\(M​\\), under some fixed \\(\\alpha​\\) and \\(\\beta​\\), we see that the evidence favors the model with \\(M=3​\\). Plot of the model evidence versus the order \\(M\\) Recall that the data is generated from a sinusoidal function, whose Taylor expansion consists of only odd polynomial functions, so even model with \\(M=2​\\) is more complex than \\(M=1​\\), the data fit is improved little, resulting in low model evidence. And for the larger \\(M&gt;3\\), only small improvements is obtained in fitting data, while suffering more from increasing model complexity. These can be seen from the previous plots in section 1.1. Also note that the generalization error is roughly constant between \\(M=3\\) and \\(M=8\\), however, the evidence values show a clear preference for \\(M=3\\) since it is the simplest. Maximizing the Evidence Function In this part we consider the maximization of model evidence \\(p(\\boldsymbol{\\mathsf{t}} | \\alpha, \\beta)\\) w.r.t. \\(\\alpha\\) and \\(\\beta\\). First define the eigenvector equation \\[ \\left(\\beta \\boldsymbol\\Phi^{\\mathrm{T}} \\boldsymbol\\Phi\\right) \\mathbf{u}_{i}=\\lambda_{i} \\mathbf{u}_{i} \\] then it follows that \\[ \\mathbf{A}\\mathbf{u}_{i} = \\left(\\alpha\\mathbf{I}+ \\beta \\boldsymbol\\Phi^{\\mathrm{T}} \\boldsymbol\\Phi\\right) \\mathbf{u}_{i}=(\\alpha+\\lambda_{i}) \\mathbf{u}_{i} \\] Maximizing \\(\\alpha\\) If we regard \\(\\mathbf{m}_{N}​\\) as a fixed value \\[ \\begin{align} \\frac{d}{d \\alpha}\\ln p(\\boldsymbol{\\mathsf{t}} | \\alpha, \\beta) &amp;= \\frac{d}{d \\alpha} \\left(\\frac{M}{2} \\ln \\alpha+\\frac{N}{2} \\ln \\beta-E\\left(\\mathbf{m}_{N}\\right)-\\frac{1}{2} \\ln |\\mathbf{A}|-\\frac{N}{2} \\ln (2 \\pi)\\right) \\\\ &amp;= \\frac{M}{2\\alpha}+ \\frac{d}{d \\alpha} \\left(-E\\left(\\mathbf{m}_{N}\\right)-\\frac{1}{2} \\ln |\\mathbf{A}|\\right) \\\\ &amp;= \\frac{M}{2\\alpha}+ \\frac{d}{d \\alpha} \\left(-\\frac{\\beta}{2}\\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2}-\\frac{\\alpha}{2} \\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N}-\\frac{1}{2} \\sum_{i} \\ln \\left(\\lambda_{i}+\\alpha\\right)\\right) \\\\ &amp;= \\frac{M}{2\\alpha} -\\frac{1}{2} \\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N}-\\frac{1}{2} \\sum_{i} \\frac{1}{\\lambda_{i}+\\alpha} \\end{align} \\] By setting the derivative to zero, we have \\[ \\begin{align} &amp; \\frac{M}{2\\alpha} -\\frac{1}{2} \\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N}-\\frac{1}{2} \\sum_{i} \\frac{1}{\\lambda_{i}+\\alpha} = 0 \\\\ \\Rightarrow \\quad &amp; M - \\sum_{i} \\frac{\\alpha}{\\lambda_{i}+\\alpha} = \\alpha \\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N} \\\\ \\Rightarrow \\quad &amp; \\sum_{i} \\frac{\\lambda_{i}+\\alpha}{\\lambda_{i}+\\alpha} - \\sum_{i} \\frac{\\alpha}{\\lambda_{i}+\\alpha} = \\alpha \\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N} \\\\ \\Rightarrow \\quad &amp; \\sum_{i} \\frac{\\lambda_i}{\\lambda_{i}+\\alpha} = \\alpha \\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N} \\end{align} \\] If we define \\[ \\gamma = \\sum_{i} \\frac{\\lambda_i}{\\lambda_{i}+\\alpha} \\] Then we get an implicit solution for \\(\\alpha\\) \\[ \\alpha=\\frac{\\gamma}{\\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N}} \\] Note that in this solution both \\(\\gamma\\) and \\(\\mathbf{m}_{N}\\) depend on the choice of \\(\\alpha\\), so we can take an iterative procedure to find out the optimal \\(\\alpha\\), by first pick an initial value \\(\\alpha_0\\), then evaluate \\(\\gamma\\) and \\(\\mathbf{m}_{N}\\), compute the new \\(\\alpha\\) and repeat until convergence. Maximizing \\(\\beta\\) Again we regard \\(\\mathbf{m}_{N}\\) as a fixed value \\[ \\begin{align} \\frac{d}{d \\beta}\\ln p(\\boldsymbol{\\mathsf{t}} | \\alpha, \\beta) &amp;= \\frac{d}{d \\beta} \\left(\\frac{M}{2} \\ln \\alpha+\\frac{N}{2} \\ln \\beta-E\\left(\\mathbf{m}_{N}\\right)-\\frac{1}{2} \\ln |\\mathbf{A}|-\\frac{N}{2} \\ln (2 \\pi)\\right) \\\\ &amp;= \\frac{N}{2\\beta}+ \\frac{d}{d \\beta} \\left(-E\\left(\\mathbf{m}_{N}\\right)-\\frac{1}{2} \\ln |\\mathbf{A}|\\right) \\\\ &amp;= \\frac{N}{2\\beta}+ \\frac{d}{d \\beta} \\left(-\\frac{\\beta}{2}\\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2}-\\frac{\\alpha}{2} \\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N}-\\frac{1}{2} \\sum_{i} \\ln \\left(\\lambda_{i}+\\alpha\\right)\\right) \\\\ &amp;= \\frac{N}{2\\beta} -\\frac{1}{2}\\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2}-\\frac{1}{2\\beta} \\sum_{i} \\frac{\\lambda_i}{\\lambda_{i}+\\alpha} \\\\ &amp;= \\frac{N}{2\\beta} -\\frac{1}{2}\\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2}-\\frac{\\gamma}{2\\beta} \\end{align} \\] In the last second line we use the fact that \\(\\lambda_i\\) is proportional to \\(\\beta\\) since we defined that \\(\\left(\\beta \\boldsymbol\\Phi^{\\mathrm{T}} \\boldsymbol\\Phi\\right) \\mathbf{u}_{i}=\\lambda_{i} \\mathbf{u}_{i}\\), where we have \\(d \\lambda_{i} / d \\beta=\\lambda_{i} / \\beta\\). Then by setting the derivative to zero, we have \\[ \\begin{align} &amp; \\frac{N}{2\\beta} -\\frac{1}{2}\\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2}-\\frac{\\gamma}{2\\beta} = 0 \\\\ \\Rightarrow \\quad &amp; \\frac{N-\\gamma}{\\beta} -\\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2} = 0 \\\\ \\Rightarrow \\quad &amp; \\frac{1}{\\beta} = \\frac{1}{N-\\gamma} \\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2} \\end{align} \\] Again we can take an iterative procedure to find out the optimal \\(\\beta\\). Effective Number of Parameters Insight from \\(\\alpha\\) In this part we will see that the variable \\(\\gamma​\\) has an elegant interpretation as the effective number of model parameters. Recall that in equation \\((2)\\) we see the term \\(\\mathbf{m}_{N}\\) is actually just the maximum posterior estimation \\(\\mathbf{w}_\\mathrm{MAP}\\), that takes the form \\[ \\mathbf{w}_\\mathrm{MAP} = \\mathbf{m}_{N}=\\beta \\mathbf{A}^{-1} \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} = \\beta \\left(\\alpha \\mathbf{I}+\\beta \\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\] and without any prior, the MLE of the parameter is \\[ \\mathbf{w}_\\mathrm{ML} = \\beta \\left(\\beta \\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\] If we rotate the set of axes in parameter space to align with the eigenvectors \\(\\mathbf{u}_i​\\) defined in \\(\\left(\\beta \\boldsymbol\\Phi^{\\mathrm{T}} \\boldsymbol\\Phi\\right) \\mathbf{u}_{i}=\\lambda_{i} \\mathbf{u}_{i}​\\), and then project the vector \\(\\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}​\\) onto this new axes to find out its new coordinates \\((c_1,c_2,...,c_M)​\\) according to the unique linear combination \\[ \\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} = \\sum_i c_i \\mathbf{u}_i \\] we can get the new coordinates for \\(\\mathbf{w}_\\mathrm{MAP}​\\) and \\(\\mathbf{w}_\\mathrm{ML}​\\) \\[ \\mathbf{w}_\\mathrm{MAP} = \\beta\\sum_i c_i \\left(\\alpha \\mathbf{I}+\\beta \\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi}\\right)^{-1} \\mathbf{u}_i= \\beta\\sum_i \\frac{c_i}{\\alpha+\\lambda_i} \\mathbf{u}_i \\\\ \\mathbf{w}_\\mathrm{ML} = \\beta\\sum_i c_i \\left(\\beta \\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi}\\right)^{-1} \\mathbf{u}_i= \\beta\\sum_i \\frac{c_i}{\\lambda_i} \\mathbf{u}_i \\] So the coordinate value of \\(\\mathbf{w}_\\mathrm{MAP}\\) on each axis is proportional to \\(\\mathbf{w}_\\mathrm{ML}\\) with the magnitude \\(\\lambda_i / (\\lambda_i+\\alpha)\\). This ratio will lie between \\(0\\) and \\(1\\), and consequently we have \\(0 \\leqslant \\gamma \\leqslant M\\). Along some axis \\(i\\), if \\(\\lambda_i \\gg \\alpha\\), i.e. there is little constraint for parameter that defined by the prior, then \\(\\lambda_i / (\\lambda_i+\\alpha) \\rightarrow 1\\). Such parameters are called well determined, since their values fit fully into the data. While conversely, if \\(\\lambda_{i} \\ll \\alpha​\\), then \\(\\lambda_i / (\\lambda_i+\\alpha) \\rightarrow 0​\\), the corresponding \\(i​\\)th parameter of \\(\\mathbf{w}_\\mathrm{MAP}​\\) will go to zero. In such directions the likelihood function is relatively insensitive to the parameter value and so the parameter has been set to a small value by the prior. Till now we see that the quantity \\(\\gamma\\) therefore measures the effective total number of well determined parameters. Contours of the likelihood function (red) and the prior (green) in which the axes in parameter space have been rotated to align with the \\(\\mathbf{u}_i\\). Intuitively, as shown in the figure above, smaller \\(\\lambda_i​\\) corresponds to a greater elongation of the contour of likelihood (recall that in section 2.3.0, \\(\\lambda_i​\\) means the precision of Gaussian along each axis, under a transformed coordinate), so the ratio \\(\\lambda_i / (\\lambda_i+\\alpha) \\rightarrow 0​\\), and we see that the value of \\(\\mathbf{w}_\\mathrm{MAP}​\\) in the first axis \\(w_1​\\) is smaller than that in the second axis \\(w_2​\\). An example showing the changes of different \\(w_i\\) and \\(\\gamma\\) as the result of tuning the parameter \\(\\alpha​\\) in the range \\([0,\\infty​]\\) Insight from \\(\\beta\\) Recall that we have found the MLE for \\(\\beta\\) in section 3.1 which takes the form \\[ \\frac{1}{\\beta_{\\mathrm{ML}}} = \\frac{1}{N}\\| \\boldsymbol{\\mathsf{t}} - \\mathbf{\\Phi w} \\|^2 \\] and we know that it is a biased estimation. Actually for model with \\(M\\) parameters, the unbiased estimation for \\(\\beta​\\) is \\[ \\frac{1}{\\beta_{\\mathrm{ML}}} = \\frac{1}{N-M}\\| \\boldsymbol{\\mathsf{t}} - \\mathbf{\\Phi w} \\|^2 \\] where we subtract \\(M\\) degrees of freedom from \\(N\\) in the denominator. With the same interpretation, we see that the evidence maximization result for \\(\\beta\\) subtract \\(\\gamma\\) degrees of freedom from \\(N\\), which is copied here for easier comparison \\[ \\frac{1}{\\beta} = \\frac{1}{N-\\gamma} \\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2} \\] Approximation for Maximizing \\(\\alpha\\) and \\(\\beta\\) Recall that the MAP solution get similar to MLE solution if we have large number of data, i.e. \\(N \\gg M​\\). And since \\(\\lambda_i​\\) means the precision of \\(\\mathbf{w}_\\mathrm{ML}​\\) along each axis, so the ratio \\(\\lambda_i / (\\lambda_i+\\alpha) \\rightarrow 1​\\) after observing enough data (i.e. high precision). In this case, \\(\\gamma = M​\\) and we have \\[ \\alpha=\\frac{\\gamma}{\\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N}} \\simeq \\frac{M}{\\mathbf{m}_{N}^{\\mathrm{T}} \\mathbf{m}_{N}} \\\\ \\beta = \\frac{N-\\gamma}{\\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2}} \\simeq \\frac{N}{\\left\\|\\boldsymbol{\\mathsf{t}}-\\mathbf{\\Phi m}_{N}\\right\\|^{2}} \\] these results can be used as an easy-to-compute approximation of the optimal \\(\\alpha\\) and \\(\\beta\\), since they do not require to compute \\(\\lambda_i\\).","link":"/old/PRML3-5.html"},{"title":"(PRML Notes) 4.3 Probabilistic Discriminative Models","text":"A series of notes taken from Pattern Recognition and Machine Learning. Previously the discussed generative approach of classification is indirect that fits conditional densities and priors separately then applies Bayes' theorem. Now a direct discriminative approach will be discussed with following advantages: Typically fewer adaptive parameters to be determined. May led to improved predictive performance particularly when conditional density is poorly approximated. Fixed Basis Functions Later we will consider the fixed nonlinear transformation of the inputs using a vector of basis functions \\(\\boldsymbol\\phi(\\mathbf{x})​\\) as before, to make linear inseparable data separable in higher transformed space. However, we should note that \\(\\boldsymbol\\phi(\\mathbf{x})\\) will increase the overlap level of conditional density \\(p\\left(\\mathbf{x} | \\mathcal{C}_{k}\\right)\\) of different classes, since \\(\\boldsymbol\\phi(\\mathbf{x})\\) is a multi-to-one mapping, so \\(\\boldsymbol\\phi(\\mathbf{x})\\) should be chosen appropriately. Logistic Regression Recall that for two-class problem the posterior of \\(\\mathcal{C_1}\\) can be written in a sigmoid form \\[ p\\left(\\mathcal{C}_{1} | \\boldsymbol{\\phi}\\right)=y(\\boldsymbol{\\phi})=\\sigma\\left(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\right) \\] and in the previous generative approach, the parameter \\(\\mathbf{w}\\) is expressed by \\[ \\begin{aligned} \\mathbf{w} &amp;=\\mathbf{\\Sigma}^{-1}\\left(\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{2}\\right) \\\\ w_{0} &amp;=-\\frac{1}{2} \\boldsymbol{\\mu}_{1}^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{1}+\\frac{1}{2} \\boldsymbol{\\mu}_{2}^{\\mathrm{T}} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_{2}+\\ln \\frac{p\\left(\\mathcal{C}_{2}\\right)}{p\\left(\\mathcal{C}_{2}\\right)} \\end{aligned} \\] For \\(M\\)-dimensional feature space \\(\\boldsymbol\\phi\\), this results in \\(O(M^2)\\) adjustable parameters. So we can alternatively use \\(M+1\\) adjustable parameters to represent \\(\\mathbf{w}\\) directly, and this model is known as linear regression, though it is indeed a model for classification. Maximum Likelihood Solution For logistic regression given a dataset \\(\\left\\{\\boldsymbol\\phi_{n}, t_{n}\\right\\}​\\), the likelihood function of the dataset takes the form \\[ \\begin{align} p(\\boldsymbol{\\mathsf{t}},\\mathbf{\\Phi}) &amp;= p(\\mathbf{\\Phi})p(\\boldsymbol{\\mathsf{t}}|\\mathbf{\\Phi}) \\\\&amp;\\propto p(\\boldsymbol{\\mathsf{t}}|\\mathbf{\\Phi}) &amp;&amp; \\scriptstyle{(\\text{Since now adjustable parameters only exist in }p(\\boldsymbol{\\mathsf{t}}|\\mathbf{\\Phi}).)} \\\\ &amp;=\\prod_{n=1}^{N}p(t_n|\\boldsymbol{\\phi}_n) \\\\&amp;=\\prod_{n=1}^{N} y_{n}^{t_{n}}\\left\\{1-y_{n}\\right\\}^{1-t_{n}} \\end{align} \\] where \\(y_{n}=p\\left(\\mathcal{C}_{1} | \\boldsymbol{\\phi_{n}}\\right)=\\sigma\\left(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}_n\\right)​\\). As usual, error function could be defined to take the negative logarithm of \\(p(\\boldsymbol{\\mathsf{t}})​\\), which gives \\[ E(\\mathbf{w})=-\\ln p(\\boldsymbol{\\mathsf{t}},\\mathbf{\\Phi})\\propto-\\sum_{n=1}^{N}\\ln p(t_n|\\boldsymbol{\\phi}_n)=-\\sum_{n=1}^{N}\\left\\{t_{n} \\ln y_{n}+\\left(1-t_{n}\\right) \\ln \\left(1-y_{n}\\right)\\right\\} \\] which takes the form of the cross entropy error function, since by sampling we have \\[ KL(p_\\mathrm{real}\\|p) = \\sum_{n=1}^N \\ln \\frac{p_\\mathrm{real}(t_n,_n)}{p(t_n,_n)} = -\\sum_{n=1}^N \\ln p(t_n,_n) + \\mathrm{const}= -\\sum_{n=1}^N \\ln p(t_n|_n) + \\mathrm{const} \\] Taking the gradient of \\(E(\\mathbf{w})​\\), and use the fact that \\({d \\sigma(a)}/{d a}=\\sigma(a)(1-\\sigma(a))​\\), we have \\[ \\begin{align} \\nabla E(\\mathbf{w})&amp;= -\\sum_{n=1}^{N} \\left\\{ t_n\\frac{y_n(1-y_n) }{y_n}\\boldsymbol{\\phi}_n + (1-t_n)\\frac{-y_n(1-y_n) }{1-y_n} \\boldsymbol{\\phi}_n \\right\\} \\\\ &amp;= -\\sum_{n=1}^{N} \\left\\{ t_n(1-y_n)\\boldsymbol{\\phi}_n - (1-t_n)y_n \\boldsymbol{\\phi}_n \\right\\} \\\\ &amp;= \\sum_{n=1}^{N}\\left(y_{n}-t_{n}\\right) \\boldsymbol{\\phi}_{n} \\tag{1} \\\\ &amp;= \\boldsymbol{\\Phi}^\\mathrm{T}\\left(\\boldsymbol{\\mathsf{y}}-\\boldsymbol{\\mathsf{t}}\\right) \\end{align} \\] (Refresh: \\(\\boldsymbol{\\Phi}^\\mathrm{T} = \\left\\{\\boldsymbol\\phi_1^{\\mathrm{T}} ,...,\\boldsymbol\\phi_n^{\\mathrm{T}} \\right\\}​\\)) Note that this give rise to a sequential update form where patterns are presented one at a time. It is also worth noting that MLE can exhibit severe over-fitting for datasets that are linearly separable, which means the sigmoid function becomes infinitely steep. This problem will arise even the dataset is large, so long as the dataset is linearly separable. We can solve this by using regularization or introducing prior of \\(\\mathbf{w}​\\). Iterative Reweighted Least Squares Note that logistic regression has no closed-form solution of \\(\\nabla E(\\mathbf{w})=0​\\) due to the nonlinearity in the part \\(\\boldsymbol{\\mathsf{y}}=\\sigma\\left( \\boldsymbol{\\Phi}\\mathbf{w} \\right)​\\), but as we will see that \\(E(\\mathbf{w})​\\) is indeed still convex. So SGD can be used to approximate to the unique global minimum. This section introduces a more efficient approach called Newton-Raphson, an iterative optimization technique which iteratively make \\(\\nabla E(\\mathbf{w})​\\) closer to \\(0​\\) with a local quadratic approximation according to \\[ \\mathbf{w}^{(\\text {new})}=\\mathbf{w}^{(\\text {old})}-\\mathbf{H}^{-1} \\nabla E(\\mathbf{w}^{(\\text {old})}) \\] where \\(\\mathbf{H}=\\nabla^2 E(\\mathbf{w})​\\). It is more clear to understand the idea by changing to form to \\(\\nabla E(\\mathbf{w}^{(\\text {old})})=\\mathbf{H}\\left(\\mathbf{w}^{(\\text {old})}-\\mathbf{w}^{(\\text {new})}\\right)​\\), or refer to the following one dimension illustration. To gain some intuition, we first apply the this method to the linear regression model with the sum-of-squares error function to see what happens. Recall that in this case \\(\\nabla E(\\mathbf{w})=\\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi} \\mathbf{w}-\\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}\\), so \\[ \\mathbf{H}=\\nabla^2 E(\\mathbf{w})=\\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi} \\] and thus the update rule becomes \\[ \\mathbf{w}^{(\\text {new})}=\\mathbf{w}^{(\\text {old})}-\\left(\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi}\\right)^{-1}\\left\\{\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{\\Phi} \\mathbf{w}^{(\\mathrm{old})}-\\mathbf{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}}\\right\\} =\\left(\\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{\\mathrm{T}} \\boldsymbol{\\mathsf{t}} \\] which is updated to the global minimum after just one step, since \\(\\nabla E(\\mathbf{w})\\) is quadratic. Now apply to the logistic regression model with the cross-entropy error function, recall \\(\\nabla E(\\mathbf{w})= \\boldsymbol{\\Phi}^\\mathrm{T}\\left(\\boldsymbol{\\mathsf{y}}-\\boldsymbol{\\mathsf{t}}\\right) ​\\). To figure out the \\(\\mathbf{H}​\\), first by differential \\[ \\begin{align} \\mathrm{d}\\nabla E(\\mathbf{w}) &amp;= \\boldsymbol{\\Phi}^\\mathrm{T}\\mathrm{d}\\boldsymbol{\\mathsf{y}} \\\\ &amp;= \\boldsymbol{\\Phi}^\\mathrm{T} \\left(\\sigma&#39;\\left( \\boldsymbol{\\Phi}\\mathbf{w} \\right) \\odot \\mathrm{d} (\\boldsymbol{\\Phi}\\mathbf{w})\\right) \\\\ &amp;= \\boldsymbol{\\Phi}^\\mathrm{T} \\mathbf{R}\\boldsymbol{\\Phi} \\mathrm{d}\\mathbf{w} \\end{align} \\] where \\(\\mathbf{R} = \\text{diag}\\left(y_1(1-y_1),...,y_n(1-y_n)\\right)​\\), (refresher: \\(y_n=\\sigma\\left(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}_n\\right)​\\)). so we have \\[ \\mathbf{H} = \\nabla^2 E(\\mathbf{w}) = \\left(\\boldsymbol{\\Phi}^\\mathrm{T} \\mathbf{R}\\boldsymbol{\\Phi}\\right) ^\\mathrm{T} = \\boldsymbol{\\Phi}^\\mathrm{T} \\mathbf{R}\\boldsymbol{\\Phi} \\] We can now show \\(E(\\mathbf{w})\\) is convex, due to \\(\\mathbf{H}\\) is definite, i.e. \\(\\mathbf{u}^{\\mathrm{T}} \\mathbf{H} \\mathbf{u}&gt;0, \\ \\forall \\mathbf{u} \\neq \\mathbf{0}\\), since any element in \\(\\mathbf{R}\\) is greater than 0, thus \\[ \\mathbf{u}^{\\mathrm{T}} \\mathbf{H} \\mathbf{u} = \\mathbf{u}^{\\mathrm{T}} \\boldsymbol{\\Phi}^\\mathrm{T} \\mathbf{R}\\boldsymbol{\\Phi} \\mathbf{u} = \\|\\mathbf{R}^{1/2}\\boldsymbol{\\Phi} \\mathbf{u} \\|^2 &gt;0 \\] So the update rule takes the form \\[ \\begin{align} \\mathbf{w}^{(\\text {new})}&amp;=\\mathbf{w}^{(\\text {old})}-\\left(\\mathbf{\\Phi}^{\\mathrm{T}} \\mathbf{R} \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^{\\mathrm{T}}(\\boldsymbol{\\mathsf{y}}-\\boldsymbol{\\mathsf{t}}) \\\\ &amp;=\\left(\\boldsymbol{\\Phi}^{\\mathrm{T}} \\mathbf{R} \\boldsymbol{\\Phi}\\right)^{-1}\\left\\{\\boldsymbol{\\Phi}^{\\mathrm{T}} \\mathbf{R} \\mathbf{\\Phi} \\mathbf{w}^{(\\mathrm{old})}-\\mathbf{\\Phi}^{\\mathrm{T}}(\\boldsymbol{\\mathsf{y}}-\\boldsymbol{\\mathsf{t}})\\right\\} \\\\ &amp;= \\left(\\boldsymbol{\\Phi}^{\\mathrm{T}} \\mathbf{R} \\boldsymbol{\\Phi}\\right)^{-1} \\boldsymbol{\\Phi}^{\\mathrm{T}} \\mathbf{R} \\mathbf{z} \\end{align} \\] where \\(\\mathbf{z}=\\mathbf{\\Phi} \\mathbf{w}^{(\\mathrm{old})}-\\mathbf{R}^{-1}(\\boldsymbol{\\mathsf{y}}-\\boldsymbol{\\mathsf{t}})\\). We see that this takes the form of normal equations for a weighted least-squares problem with \\(\\mathbf{z}\\) as the target vector. Note that the weight matrix \\(\\mathbf{R}\\) is not constant since it depends on \\(\\mathbf{w}\\) through \\(y_n=\\sigma\\left(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}_n\\right)\\), it should be reweighted after each update. So the algorithm is known as iterative reweighted least squares, or IRLS. There is some more content in text about the interpretation of \\(\\mathbf{R}\\) and \\(\\mathbf{z}\\). Multiclass Logistic Regression For \\(K&gt;2\\) classes logistic regression, the posterior are again determined directly through \\(\\mathbf{w}\\), given by a softmax function as \\[ y_{nk}=p\\left(\\mathcal{C}_{k} | \\boldsymbol{\\phi}_n\\right) = \\frac{\\exp \\left(\\mathbf{w}_{k}^{\\mathrm{T}} \\boldsymbol{\\phi}_n\\right)}{\\sum_{j} \\exp \\left(\\mathbf{w}_{j}^{\\mathrm{T}} \\boldsymbol{\\phi}_n\\right)} \\] We can express it in a more compact way as \\[ \\mathbf{y}_{n}= \\frac{\\exp \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right)}{ \\mathbf{1}^\\mathrm{T} \\exp \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right)} \\equiv \\mathrm{softmax} \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right) \\] where \\(\\mathbf{W} = \\left\\{ \\mathbf{w}_1,...,\\mathbf{w}_K \\right\\}^\\mathrm{T}​\\). Then the likelihood function of a given dataset is given by \\[ \\begin{align} p(\\mathbf{T},\\mathbf{\\Phi}) &amp;= p(\\mathbf{\\Phi})p(\\mathbf{T}|\\mathbf{\\Phi}) \\\\&amp;\\propto p(\\mathbf{T}|\\mathbf{\\Phi}) \\\\ &amp;=\\prod_{n=1}^{N}p(\\mathbf{t}_n|\\boldsymbol{\\phi}_n) \\\\&amp;=\\prod_{n=1}^{N} \\mathbf{t}_n^\\mathrm{T}\\mathbf{y}_n \\end{align} \\] where \\(\\mathbf{T} = \\left\\{ \\mathbf{t}_1,...,\\mathbf{t}_N \\right\\}^\\mathrm{T}\\) is an \\(N\\times K\\) matrix of target variables. The cross-entropy error function is again given by taking the negative logarithm of \\(p(\\mathbf{T},\\mathbf{\\Phi})\\) \\[ \\begin{align} E(\\mathbf{W}) &amp;= - \\ln p(\\mathbf{T},\\mathbf{\\Phi}) \\\\ &amp;\\propto -\\sum_{n=1}^{N} \\mathbf{t}_n^\\mathrm{T}\\ln \\mathbf{y}_n \\\\ &amp;= -\\sum_{n=1}^{N} \\mathbf{t}_n^\\mathrm{T}\\ln \\frac{\\exp \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right)}{ \\mathbf{1}^\\mathrm{T} \\exp \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right)} \\\\ &amp;= -\\sum_{n=1}^{N} \\left(\\mathbf{t}_n^\\mathrm{T}\\mathbf{W} \\boldsymbol{\\phi}_n - \\mathbf{t}_n^\\mathrm{T} \\mathbf{1} \\ln\\left( \\mathbf{1}^\\mathrm{T} \\exp \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right) \\right) \\right) &amp;&amp;\\scriptstyle{(\\text{Note that } \\mathbf{t}_n^\\mathrm{T} \\mathbf{1} = 1)} \\\\ &amp;= \\sum_{n=1}^{N} \\left(-\\mathbf{t}_n^\\mathrm{T}\\mathbf{W} \\boldsymbol{\\phi}_n + \\ln\\left( \\mathbf{1}^\\mathrm{T} \\exp \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right) \\right) \\right) \\end{align} \\] Again to figure out \\(\\mathrm{d} E(\\mathbf{W}) / \\mathrm{d}\\mathbf{W}​\\), first by differential we have \\[ \\begin{align} \\mathrm{d}E(\\mathbf{W}) &amp;\\propto \\sum_{n=1}^{N} \\left(-\\mathbf{t}_n^\\mathrm{T}\\mathrm{d}\\mathbf{W} \\boldsymbol{\\phi}_n + \\left( \\mathbf{1}^\\mathrm{T} \\exp \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right) \\right)^{-1} \\mathbf{1}^\\mathrm{T} \\left(\\exp \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right) \\odot (\\mathrm{d}\\mathbf{W}\\boldsymbol{\\phi}_n) \\right) \\right) \\\\ &amp;= \\sum_{n=1}^{N} \\left(-\\mathbf{t}_n^\\mathrm{T}\\mathrm{d}\\mathbf{W} \\boldsymbol{\\phi}_n + \\left( \\mathbf{1}^\\mathrm{T} \\exp \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right) \\right)^{-1} \\exp \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right) ^\\mathrm{T} \\mathrm{d}\\mathbf{W}\\boldsymbol{\\phi}_n \\right) \\\\ &amp;= \\sum_{n=1}^{N} \\left(-\\mathbf{t}_n^\\mathrm{T}\\mathrm{d}\\mathbf{W} \\boldsymbol{\\phi}_n + \\mathrm{softmax} \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right) ^\\mathrm{T} \\mathrm{d}\\mathbf{W}\\boldsymbol{\\phi}_n \\right) \\\\ &amp;= \\sum_{n=1}^{N} \\left(-\\boldsymbol{\\phi}_n^\\mathrm{T} \\mathbf{t}_n^\\mathrm{T}\\mathrm{d}\\mathbf{W} + \\boldsymbol{\\phi}_n^\\mathrm{T} \\mathrm{softmax} \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right) ^\\mathrm{T} \\mathrm{d}\\mathbf{W} \\right) \\end{align} \\] So we obtain \\[ \\nabla_{\\mathbf{W}} E(\\mathbf{W}) = \\sum_{n=1}^{N} \\left( \\mathrm{softmax} \\left(\\mathbf{W} \\boldsymbol{\\phi}_n\\right) - \\mathbf{t}_n \\right)\\boldsymbol{\\phi}_n = \\sum_{n=1}^{N} \\left( \\mathbf{y}_n - \\mathbf{t}_n \\right)\\boldsymbol{\\phi}_n \\] This result can be decomposed directly to get the equation (4.109) in textbook, since \\(\\mathbf{w}_j\\) is the \\(j\\)th row of \\(\\mathbf{W}\\) \\[ \\nabla_{\\mathbf{w}_j} E(\\mathbf{W}) = \\sum_{n=1}^{N} \\left( {y}_{nj} - {t}_{nj} \\right)\\boldsymbol{\\phi}_n \\tag{2} \\] To apply IRLS algorithm to multiclass logistic regression, we should use the following result that \\[ \\nabla_{\\mathbf{w}_{k}} \\nabla_{\\mathbf{w}_{j}} E\\left(\\mathbf{W}\\right)=-\\sum_{n=1}^{N} y_{n k}\\left(I_{k j}-y_{n j}\\right) \\boldsymbol{\\phi}_{n} \\boldsymbol{\\phi}_{n}^{\\mathrm{T}} \\] where \\(I_{kj}\\) are the elements of the identity matrix. (proof omitted.) Probit Regression The discussion within this part will be restricted to the simple case of \\(K=2\\) classes. In practice, not all choices of class-conditional density give rise to such a simple form for the posterior like sigmoid, so it is worth exploring more general types of posterior. Recall that we call the general linear models which take the form \\[ p(t=1 | a)=f(a),\\text{ where } a = \\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi} \\] and the target value can be set according to \\[ \\left\\{\\begin{array}{ll}{t_{n}=1} &amp; {\\text { if } a_{n} \\geqslant \\theta} \\\\ {t_{n}=0} &amp; {\\text { otherwise }}\\end{array}\\right. \\] We can model \\(\\theta\\) as drawn from a probability density \\(p(\\theta)\\), then the corresponding activation function \\(f\\) will be given by the cumulative distribution function \\[ f(a)=\\int_{-\\infty}^{a} p(\\theta) \\mathrm{d} \\theta \\] as illustrated in the following red line. The cumulative distribution of Gaussian mixture. As a specific example, where \\(p(\\theta)\\) is set as \\(\\mathcal{N}(\\theta|0,1)\\), and the corresponding \\(f\\) is given by \\[ \\Phi(a)\\equiv f(a)=\\int_{-\\infty}^{a} \\mathcal{N}(\\theta | 0,1) \\mathrm{d} \\theta \\] which is known as the inverse probit function, and the generalized linear model based on such \\(\\Phi(a)\\) as posterior is known as probit regression. Plot of the logistic sigmoid function \\(\\sigma(a)\\) and the scaled inverse probit function \\(\\Phi(\\lambda a)\\), where \\(\\lambda^2=\\pi/8\\) The inverse probit function also has a sigmoidal shape, and we can find a scaling value \\(\\lambda​\\) to make it similar to the sigmoid function by having the same slope at the origin \\[ \\begin{align} \\Phi&#39;(\\lambda a)|_{a=0} &amp;= \\sigma&#39;(a)|_{a=0} \\\\ \\lambda\\mathcal{N}(\\lambda a | 0,1)|_{a=0} &amp;= \\sigma(a)(1-\\sigma(a))|_{a=0} \\\\ \\frac{\\lambda}{\\sqrt{2\\pi}} &amp;= \\frac{1}{4} \\\\ \\lambda^2 &amp;= \\frac{\\pi}{8} \\end{align} \\] Robustness The result found by probit regression is similar to those of logistic regression, but it is significantly more sensitive to outliers, since the tails of logistic sigmoid decay asymptotically like \\(\\exp(-x)\\) for \\(x\\rightarrow \\infty\\), while the inverse probit decays like \\(\\exp(-x^2)\\). One way to reduce this impact is to introduce a probability \\(\\epsilon\\) that the target value \\(t\\) is mislabeled, which leads to a target value distribution for data point \\(\\mathbf{x}\\) of the form \\[ \\begin{aligned} p(t=1 | \\mathbf{x}) &amp;=(1-\\epsilon) p(\\mathcal{C}_1|\\mathbf{x})+\\epsilon p(\\mathcal{C}_2|\\mathbf{x}) \\\\ &amp;=(1-\\epsilon) \\sigma(\\mathbf{x})+\\epsilon(1-\\sigma(\\mathbf{x})) \\\\ &amp;=\\epsilon+(1-2 \\epsilon) \\sigma(\\mathbf{x}) \\\\ &amp;\\in (\\epsilon,1-\\epsilon) \\end{aligned} \\] This approach can also be viewed as setting the target variable to \\[ \\left\\{\\begin{array}{ll}{t_{n}=\\epsilon} &amp; {\\text { if } a_{n} \\geqslant \\theta} \\\\ {t_{n}=1-\\epsilon} &amp; {\\text { otherwise }}\\end{array}\\right. \\] Canonical Link Functions We have seen that the same form arising for the gradient like \\(\\sum_n (y_n-t_n)\\boldsymbol{\\phi}_n\\) as was found for the sum-of-squares error function with the linear model in section 3.1.1 the cross-entropy error for the two-class logistic regression model as in equation \\((1)\\) the cross-entropy error for the multi-class logistic regression model as in equation \\((2)\\) This section shows that this is a general result of assuming a conditional distribution for the target variable from the exponential family, along with a corresponding choice for the activation function known as the canonical link function. Details omitted.","link":"/old/PRML4-3.html"},{"title":"(PRML Notes) 4.1 Discriminant Functions","text":"A series of notes taken from Pattern Recognition and Machine Learning. This chapter focuses on solving classification problems, where the input space divided by decision boundaries that can be expressed by linear functions of input vector. About the labels: For probabilistic models, in the case of two-class problem, we usually use single target variable \\(t\\in\\{0,1\\}\\) to present the probability of taking one class label, thus \\(1-t\\) for the other, and for \\(K&gt;2\\) classes, we use a \\(1\\)-of-\\(K\\) coding \\(\\mathbf{t}=(0,1,0,0,0)^{\\mathrm{T}}\\) to present the probability of taking every class label. For non-probabilistic models, alternative choice of target variable representation sometimes will also be convenient. How to model: Recall that by decision theory, we can use a discriminant function \\(f(\\mathbf{x})​\\) that directly assigns input \\(\\mathbf x​\\) to a specific class or use a generative function to model the conditional distribution \\(p\\left(\\mathcal{C}_{k} | \\mathbf{x}\\right)​\\) or use a generative model to model both \\(p\\left(\\mathcal{C}_{k}\\right)​\\) and \\(p\\left( \\mathbf{x}|\\mathcal{C}_{k} \\right)​\\) then get \\(p\\left(\\mathcal{C}_{k} | \\mathbf{x}\\right)=p\\left(\\mathbf{x} | \\mathcal{C}_{k}\\right) p\\left(\\mathcal{C}_{k}\\right)/p(\\mathbf{x})​\\) How can linearity works in classification: Note that for classification problems, we need to predict class labels that interpreted as \\(p\\left(\\mathcal{C}_{k} | \\mathbf{x}\\right)\\in[0,1]​\\), no longer to predict the \\(y​\\) using \\(y(\\mathbf{x})=\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}+w_{0}\\in\\mathbb{R}​\\) as for regression. So to use the simple analytical and computational properties of linear models, we will consider a generalization that transforms the linear function of \\(\\mathbf{w}​\\) using a nonlinear activation function \\(f​\\) so that \\(y(\\mathbf{x})=f\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}+w_{0}\\right)​\\). Even if \\(f​\\) is nonlinear, the decision boundary \\(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}+w_{0} = \\text{constant}​\\) will be linear anyway. In this section, attention is restricted to linear discriminant functions. Unambiguous Discriminant Two Classes The simplest form of linear discriminant function is \\[ y(\\mathbf{x})=\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}+w_{0} \\] where if \\(y(\\mathbf{x})\\geq 0\\) we assign \\(\\mathbf{x}\\) to \\(\\mathcal{C}_1\\) and to \\(\\mathcal{C}_2\\) otherwise. So the decision boundary is given by \\(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}+w_{0}=0\\). We can see that \\(\\mathbf{w}​\\) determines the orientation of the decision boundary since for any different two points \\(\\mathbf{x}_{\\mathrm{A}}​\\) and \\(\\mathbf{x}_{\\mathrm{B}}​\\) on the boundary, we have \\(\\mathbf{w}^{\\mathrm{T}}\\left(\\mathbf{x}_{\\mathrm{A}}-\\mathbf{x}_{\\mathrm{B}}\\right)=0​\\). Using this fact, we can derive the normal distance from the origin to the decision boundary \\[ \\mathrm{d} = \\frac{\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}}{\\|\\mathbf{w}\\|}=-\\frac{w_{0}}{\\|\\mathbf{w}\\|} \\] Also note that \\(y(\\mathbf{x})\\) gives a signed measure of the perpendicular distance \\(r\\) of the point \\(\\mathbf{x}\\) to the decision boundary, since \\[ \\begin{align} &amp;&amp;\\mathbf{x}&amp;=\\mathbf{x}_{\\perp}+r \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|} \\\\ \\Rightarrow &amp;&amp; \\mathbf{w}^\\mathrm{T} \\mathbf{x}&amp;=\\mathbf{w}^\\mathrm{T}\\mathbf{x}_{\\perp}+r \\frac{\\mathbf{w}^\\mathrm{T}\\mathbf{w}}{\\|\\mathbf{w}\\|} \\\\ \\Rightarrow&amp;&amp; \\mathbf{w}^\\mathrm{T} \\mathbf{x} + w_0&amp;=\\mathbf{w}^\\mathrm{T}\\mathbf{x}_{\\perp}+w_0+r \\frac{\\|\\mathbf{w}\\|^2}{\\|\\mathbf{w}\\|} \\\\ \\Rightarrow&amp;&amp; y( \\mathbf{x}) &amp;= r \\|\\mathbf{w}\\| \\\\ \\Rightarrow&amp;&amp; r &amp;= \\frac{y( \\mathbf{x})}{\\|\\mathbf{w}\\|} \\end{align} \\] where \\(\\mathbf{x}_\\perp\\) is the orthogonal projection of \\(\\mathbf{x}\\) onto the decision surface. The signed orthogonal distance of a general point \\(\\mathbf{x}\\) from the decision surface is given by \\(r = y( \\mathbf{x}) / \\|\\mathbf{w}\\|\\) Multiple Classes Building a \\(K\\)-class discriminant by combining a number of two-class discriminant leads to serious difficulties. Concretely, if we build a one-versus-the-rest classifier by introducing \\(K-1\\) functions, or build a one-versus-one classifier by introducing \\(K(K-1)/2\\) functions, will result in ambiguous regions. Left shows one-versus-the-rest and right one-versus-one These difficulties can be avoided by introducing a single \\(K\\)-class discriminant (one-versus-all) comprising \\(K\\) linear functions of the form \\[ y_{k}(\\mathbf{x})=\\mathbf{w}_{k}^{\\mathrm{T}} \\mathbf{x}+w_{k 0} \\] and then assign a point \\(\\mathbf{x}\\) to class \\(\\mathcal{C}_k\\) if \\(y_{k}(\\mathbf{x})&gt;y_{j}(\\mathbf{x}),\\ \\forall j \\neq k​\\). Note that the decision regions of such a discriminant are always singly connected and convex. Proof: For any two points \\(\\mathbf{x}_{\\mathrm{A}}\\) and \\(\\mathbf{x}_{\\mathrm{B}}\\) inside the region \\(\\mathcal{R}_k\\), and any point \\(\\widehat{\\mathbf{x}}=\\lambda \\mathbf{x}_{\\mathrm{A}}+(1-\\lambda) \\mathbf{x}_{\\mathrm{B}},\\ \\forall\\lambda\\in[0,1]\\) that lies between the line connecting them, from the linearity we have \\[ y_{k}(\\widehat{\\mathbf{x}})=\\lambda y_{k}\\left(\\mathbf{x}_{\\mathrm{A}}\\right)+(1-\\lambda) y_{k}\\left(\\mathbf{x}_{\\mathrm{B}}\\right) \\] since \\(y_{k}\\left(\\mathbf{x}_{\\mathrm{A}}\\right)&gt;y_{j}\\left(\\mathbf{x}_{\\mathrm{A}}\\right), \\forall j \\neq k\\), and the same for \\(\\mathbf{x}_{\\mathrm{B}}\\), we have \\(y_{k}(\\widehat{\\mathbf{x}}) &gt; y_{j}(\\widehat{\\mathbf{x}}),\\ \\forall j\\neq k\\), so \\(\\widehat{\\mathbf{x}}\\) also lies in \\(\\mathcal{R}_k\\). Next, three approaches to learning the parameters of linear discriminant functions will be introduced, and first is a probabilistic model. Probabilistic Model: Least Squares for Classification Recall that the sum-of-squares error function approximates the conditional expectation \\(\\mathbb{E}[\\mathbf{t} | \\mathbf{x}]​\\) and leads to a simple closed-form solution which is tempting to apply it to classification problems, however, for the binary coding scheme, \\(\\mathbb{E}[\\mathbf{t} | \\mathbf{x}] = [p\\left(\\mathcal{C}_{1} | \\mathbf{x}\\right),...,p\\left(\\mathcal{C}_{K} | \\mathbf{x}\\right)]^\\mathrm{T}​\\) is typically approximated poorly by linear model. Problem Formulation and Solution As we discussed before, we model each class with \\[ y_{k}(\\mathbf{x})=\\mathbf{w}_{k}^{\\mathrm{T}} \\mathbf{x}+w_{k 0} \\] We can group the \\(K\\) classes results and get \\[ \\mathbf{y}(\\mathbf{x})=\\widetilde{\\mathbf{W}}^{\\mathrm{T}} \\widetilde{\\mathbf{x}} \\] where \\(\\widetilde{\\mathbf{W}}​\\) is a \\((D+1)\\times K​\\) matrix, whose column is given by \\(\\widetilde{\\mathbf{w}}_{k}=\\left(w_{k 0}, \\mathbf{w}_{k}^{\\mathrm{T}}\\right)^{\\mathrm{T}}​\\), and \\(\\widetilde{\\mathbf{x}}=\\left(1, \\mathbf{x}^{\\mathrm{T}}\\right)^{\\mathrm{T}}​\\) is the augmented input vector. If we have a training set \\(\\widetilde{\\mathbf{X}} =[\\widetilde{\\mathbf{x}}_1,...,\\widetilde{\\mathbf{x}}_N]^\\mathrm{T}\\), whose target values are \\(\\mathbf{T} = [\\mathbf{t}_{1},...,\\mathbf{t}_{N}]^{\\mathrm{T}}\\), then the sum-of-squares error function can then be written as \\[ E_{D}(\\widetilde{\\mathbf{W}})=\\frac{1}{2} \\operatorname{Tr}\\left\\{(\\widetilde{\\mathbf{X}} \\widetilde{\\mathbf{W}}-\\mathbf{T})^{\\mathrm{T}}(\\widetilde{\\mathbf{X}} \\widetilde{\\mathbf{W}}-\\mathbf{T})\\right\\} \\] Again by setting the derivative w.r.t. \\(\\widetilde{\\mathbf{W}}​\\) to zero we have \\[ \\widetilde{\\mathbf{W}}=\\left(\\widetilde{\\mathbf{X}}^{\\mathrm{T}} \\widetilde{\\mathbf{X}}\\right)^{-1} \\widetilde{\\mathbf{X}}^{\\mathrm{T}} \\mathbf{T}=\\widetilde{\\mathbf{X}}^{\\dagger} \\mathbf{T} \\] So the predicted target of new observation will be given by \\[ \\mathbf{y}(\\mathbf{x})=\\widetilde{\\mathbf{W}}^{\\mathrm{T}} \\widetilde{\\mathbf{x}}=\\mathbf{T}^{\\mathrm{T}}\\left(\\widetilde{\\mathbf{X}}^{\\dagger}\\right)^{\\mathrm{T}} \\widetilde{\\mathbf{x}} \\] Problems of Least Squares It can be proved that if every target vector satisfies some linear constraint \\(\\mathbf{a}^{\\mathrm{T}} \\mathbf{t}_{n}+b=0​\\), then the prediction will also satisfies \\(\\mathbf{a}^{\\mathrm{T}} \\mathbf{y}(\\mathbf{x})+b=0​\\). So under a \\(1​\\)-of-\\(K​\\) coding scheme, where the constraint is given by \\(\\mathbf{a}= [1,...,1]^\\mathrm{T}​\\) and \\({b} = -1​\\), the elements in the predicted target \\(\\mathbf{y}(\\mathbf{x})​\\) will be ensured to sum to \\(1​\\). Highly sensitive to outliers (purple), unlike logistic regression (green) Although we have this nice property, there is a severe problem that \\(\\mathbf{y}(\\mathbf{x})​\\) is still not constrained to lie within the interval \\((0,1)​\\). Indeed, the least-squares solution lack of robustness to outliers, since the error function will penalizes predictions that are \"too correct\" in that they lie a long way on the correct side of the decision boundary. And there may be more severe problem if we need to predict three classes where one locates between the other two, where the middle class will be predicted with a small region. Recall that least squares correspond to MLE under conditional Gaussian assumption, whereas binary vectors clearly more like a conditional Bernoulli distribution (modeled in logistic regression), so it is not surprised that least squares fail. Conclusion: although both least squares (MLE of conditional Gaussian) and the logistic regression (MLE of conditional Bernoulli) approximate the conditional mean, they differ in the assumption of underlying distribution, which result in really different performance. Next comes two non-probabilistic models. Nonprobabilistic Model: Fisher's Linear Discriminant Two Classes We can view the linear discriminant as a projection function, that projects the input vector \\(\\mathbf{x}​\\) down to one dimension \\[ y=\\mathbf{w}^{\\mathrm{T}} \\mathbf{x} \\] Then to do classification for two-class problem with \\(N_1\\) points of class \\(\\mathcal{C}_1\\) and \\(N_2\\) points of class \\(\\mathcal{C}_2\\), we need to select a projection that maximizes the class separation, and the simplest measure is the separation of the projected class means \\[ \\text{class separation}\\equiv m_{2}-m_{1}=\\mathbf{w}^{\\mathrm{T}}\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right) \\] where we defined \\[ \\mathbf{m}_{k}=\\frac{1}{N_{k}} \\sum_{n \\in \\mathcal{C}_{k}} \\mathbf{x}_{k}, \\quad m_{k}=\\mathbf{w}^{\\mathrm{T}} \\mathbf{m}_{k}, \\quad k=1,2 \\] and constrain \\(\\mathbf{w}\\) to unit length since we we only care about the direction of projection, we get the following optimization problem \\[ \\max_\\mathbf{w}\\ \\mathbf{w}^{\\mathrm{T}}\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right) \\\\ \\text{s.t. }\\mathbf{w}^{\\mathrm{T}}\\mathbf{w}=1 \\] by solving this we get \\[ \\color{red}{\\mathbf{w} \\propto\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)} \\] However, this simplest measure has some problem for class distribution with strongly nondiagonal covariance, we can see that there will be considerable class overlap in the projected space. Simplest measure with large overlap (left), compared with Fisher LDA (right) The idea proposed by Fisher is to maximize a function that will give a large separation between the projected class means while also giving a small variance within each class, thereby minimizing the class overlap. Hence what we want to maximize now is \\[ J(\\mathbf{w})=\\frac{\\left(m_{2}-m_{1}\\right)^{2}}{s_{1}^{2}+s_{2}^{2}} \\equiv \\frac{\\text{between-class variance}}{\\text{within-class variance}} \\] where \\[ s_{k}^{2}=\\sum_{n \\in \\mathcal{C}_{k}}\\left(y_{n}-m_{k}\\right)^{2},\\quad k=1,2 \\] To make the dependence on \\(\\mathbf{w}\\) explicit we can rewrite \\[ \\begin{align} J(\\mathbf{w})&amp;=\\frac{\\left(m_{2}-m_{1}\\right)^{2}}{s_{1}^{2}+s_{2}^{2}} \\\\ &amp;= \\frac{\\mathbf{w}^{\\mathrm{T}}\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)^{\\mathrm{T}}\\mathbf{w}}{\\sum_{n \\in \\mathcal{C}_{1}}\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_n-\\mathbf{w}^{\\mathrm{T}} \\mathbf{m}_{1}\\right)^2 + \\sum_{n \\in \\mathcal{C}_{2}}\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_n-\\mathbf{w}^{\\mathrm{T}} \\mathbf{m}_{2}\\right)^2} \\\\ &amp;= \\frac{\\mathbf{w}^{\\mathrm{T}}\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)^{\\mathrm{T}}\\mathbf{w}}{\\mathbf{w}^\\mathrm{T}\\left[\\sum_{n \\in \\mathcal{C}_{1}}\\left(\\mathbf{x}_{n}-\\mathbf{m}_{1}\\right)\\left(\\mathbf{x}_{n}-\\mathbf{m}_{1}\\right)^{\\mathrm{T}}+\\sum_{n \\in \\mathcal{C}_{2}}\\left(\\mathbf{x}_{n}-\\mathbf{m}_{2}\\right)\\left(\\mathbf{x}_{n}-\\mathbf{m}_{2}\\right)^{\\mathrm{T}}\\right]\\mathbf{w} } \\\\ &amp;\\equiv \\frac{\\mathbf{w}^{\\mathrm{T}} \\mathbf{S}_{\\mathrm{B}} \\mathbf{w}}{\\mathbf{w}^{\\mathrm{T}} \\mathbf{S}_{\\mathrm{W}} \\mathbf{w}} \\end{align} \\] By differentiating \\(J(\\mathbf{w})\\) w.r.t. \\(\\mathbf{w}\\) we have \\(\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{S}_{\\mathrm{B}} \\mathbf{w}\\right) \\mathbf{S}_{\\mathrm{W}} \\mathbf{w}=\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{S}_{\\mathrm{W}} \\mathbf{w}\\right) \\mathbf{S}_\\mathrm{B} \\mathbf{w}\\), Thus \\[ \\begin{align} \\mathbf{w} &amp;\\propto \\mathbf{S}_{\\mathrm{W}}^{-1}\\mathbf{S}_\\mathrm{B} \\mathbf{w} \\\\&amp;=\\mathbf{S}_{\\mathrm{W}}^{-1} \\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)^{\\mathrm{T}}\\mathbf{w} \\\\&amp;\\propto \\color{red}{\\mathbf{S}_{\\mathrm{W}}^{-1} (\\mathbf{m}_{2}-\\mathbf{m}_{1})} \\end{align} \\] Note that Fisher LDA degrades to simplest form as we discussed before when \\(\\mathbf{S}_{\\mathrm{W}}\\propto \\mathbf{I}\\). Now that we have developed a way to determine the direction of \\(\\mathbf{w}\\), what remained is to choose a threshold \\(y_0\\) to classify a new point as belonging to \\(\\mathcal{C}_1\\) if \\(y(\\mathbf{x}) \\geqslant y_{0}\\) and vice versa. We can do this, for example, by modeling the class-conditional densities \\(p\\left(y | \\mathcal{C}_{k}\\right)\\) using Gaussian distributions and then use decision theory to find out the optimal threshold. Relation to Least Squares In this section we show that for the two-class problem, the Fisher criterion can be obtained as a special case of least squares. First modify the targets for class \\(\\mathcal{C}_1\\) to be \\(N/N_1\\) and for \\(\\mathcal{C}_2\\) to be \\(-N/N_2\\), where \\(N_k\\) is the number of patterns in class \\(\\mathcal{C}_k​\\), so that we have \\[ \\sum_{n=1}^{N} t_{n}=N_{1} \\frac{N}{N_{1}}-N_{2} \\frac{N}{N_{2}}=0 \\] Rewrite the sum-of-error function to make \\(w_0\\) explicit \\[ E=\\frac{1}{2} \\sum_{n=1}^{N}\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_{n}+w_{0}-t_{n}\\right)^{2} \\] then set derivative of \\(E\\) w.r.t. \\(w_0\\) and \\(\\mathbf{w}\\) to zero \\[ \\begin{align} \\sum_{n=1}^{N}\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_{n}+w_{0}-t_{n}\\right) &amp;=0 \\tag{1} \\\\ \\sum_{n=1}^{N}\\mathbf{x}_{n}\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_{n}+w_{0}-t_{n}\\right) &amp;=0 \\tag{2}\\end{align} \\] From equation \\((1)​\\) we get \\[ w_{0}= -\\frac{1}{N}\\sum_{n=1}^{N}\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_{n}\\right) = -\\mathbf{w}^{\\mathrm{T}} \\mathbf{m} \\] where as before we defined \\[ \\mathbf{m}=\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_{n}=\\frac{1}{N}\\left(N_{1} \\mathbf{m}_{1}+N_{2} \\mathbf{m}_{2}\\right),\\text{ where } \\mathbf{m}_{k}=\\frac{1}{N_{k}} \\sum_{n \\in \\mathcal{C}_{k}} \\mathbf{x}_{k}, \\ k=1,2 \\] Substitute it into equation \\((2)​\\) we get \\[ \\begin{align} \\\\ \\sum_{n=1}^{N}\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_{n}+w_{0}-t_{n}\\right) \\mathbf{x}_{n} &amp;=0 \\\\ \\sum_{n=1}^{N}\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_{n}-\\mathbf{w}^{\\mathrm{T}} \\mathbf{m}-t_{n}\\right) \\mathbf{x}_{n} &amp;=0 \\\\ \\sum_{n=1}^{N} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}}\\mathbf{w}-\\sum_{n=1}^{N} \\mathbf{x}_{n} \\mathbf{m}^{\\mathrm{T}}\\mathbf{w} &amp;=\\sum_{n=1}^{N} t_n \\mathbf{x}_{n} \\\\ \\sum_{n\\in\\mathcal{C}_1} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}}\\mathbf{w} + \\sum_{n\\in\\mathcal{C}_2} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}}\\mathbf{w}-\\sum_{n\\in\\mathcal{C}_1} \\mathbf{x}_{n} \\mathbf{m}^{\\mathrm{T}}\\mathbf{w}-\\sum_{n\\in\\mathcal{C}_2} \\mathbf{x}_{n} \\mathbf{m}^{\\mathrm{T}}\\mathbf{w} &amp;=\\frac{N}{N_1}\\sum_{n\\in\\mathcal{C}_1} \\mathbf{x}_{n} - \\frac{N}{N_2}\\sum_{n\\in\\mathcal{C}_2}\\mathbf{x}_{n} \\\\ \\sum_{n\\in\\mathcal{C}_1} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}}\\mathbf{w} + \\sum_{n\\in\\mathcal{C}_2} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}}\\mathbf{w}-N_1\\mathbf{m}_1 \\mathbf{m}^{\\mathrm{T}}\\mathbf{w}-N_2\\mathbf{m}_2 \\mathbf{m}^{\\mathrm{T}}\\mathbf{w} &amp;= N(\\mathbf{m}_{1} -\\mathbf{m}_{2}) \\\\ \\sum_{n\\in\\mathcal{C}_1} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}}\\mathbf{w} + \\sum_{n\\in\\mathcal{C}_2} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}}\\mathbf{w}-\\frac{N_1}{N}\\mathbf{m}_1\\left(N_{1} \\mathbf{m}_{1}+N_{2} \\mathbf{m}_{2}\\right)^{\\mathrm{T}}\\mathbf{w}- \\frac{N_2}{N}\\mathbf{m}_2\\left(N_{1} \\mathbf{m}_{1}+N_{2} \\mathbf{m}_{2}\\right)^{\\mathrm{T}}\\mathbf{w} &amp;= N(\\mathbf{m}_{1} -\\mathbf{m}_{2}) \\\\ \\left(\\sum_{n\\in\\mathcal{C}_1} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}} + \\sum_{n\\in\\mathcal{C}_2} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}} -\\frac{N_1^2}{N}\\mathbf{m}_1\\mathbf{m}_{1}^{\\mathrm{T}} -\\frac{N_1 N_2}{N}\\mathbf{m}_1\\mathbf{m}_{2}^{\\mathrm{T}} -\\frac{N_1 N_2}{N}\\mathbf{m}_2\\mathbf{m}_{1}^{\\mathrm{T}} -\\frac{N_2^2}{N}\\mathbf{m}_2\\mathbf{m}_{2}^{\\mathrm{T}}\\right)\\mathbf{w} &amp;= N(\\mathbf{m}_{1} -\\mathbf{m}_{2}) \\\\ \\left(\\sum_{n\\in\\mathcal{C}_1} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}} + \\sum_{n\\in\\mathcal{C}_2} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}} -\\frac{(N-N_2)N_1}{N}\\mathbf{m}_1\\mathbf{m}_{1}^{\\mathrm{T}} -\\frac{N_1 N_2}{N}\\mathbf{m}_1\\mathbf{m}_{2}^{\\mathrm{T}} -\\frac{N_1 N_2}{N}\\mathbf{m}_2\\mathbf{m}_{1}^{\\mathrm{T}} -\\frac{(N-N_1)N_2}{N}\\mathbf{m}_2\\mathbf{m}_{2}^{\\mathrm{T}}\\right)\\mathbf{w} &amp;= N(\\mathbf{m}_{1} -\\mathbf{m}_{2}) \\\\ \\left(\\sum_{n\\in\\mathcal{C}_1} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}} + \\sum_{n\\in\\mathcal{C}_2} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}} -N_1\\mathbf{m}_1\\mathbf{m}_{1}^{\\mathrm{T}} +\\frac{N_1N_2}{N}\\mathbf{m}_1\\mathbf{m}_{1}^{\\mathrm{T}} -\\frac{N_1 N_2}{N}\\mathbf{m}_1\\mathbf{m}_{2}^{\\mathrm{T}} -\\frac{N_1 N_2}{N}\\mathbf{m}_2\\mathbf{m}_{1}^{\\mathrm{T}} -N_2\\mathbf{m}_2\\mathbf{m}_{2}^{\\mathrm{T}} +\\frac{N_1N_2}{N}\\mathbf{m}_2\\mathbf{m}_{2}^{\\mathrm{T}}\\right)\\mathbf{w} &amp;= N(\\mathbf{m}_{1} -\\mathbf{m}_{2}) \\\\ \\left(\\sum_{n\\in\\mathcal{C}_1} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}} + \\sum_{n\\in\\mathcal{C}_2} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}} -N_1\\mathbf{m}_1\\mathbf{m}_{1}^{\\mathrm{T}} -N_2\\mathbf{m}_2\\mathbf{m}_{2}^{\\mathrm{T}} +\\frac{N_1N_2}{N}\\left(\\mathbf{m}_1\\mathbf{m}_{1}^{\\mathrm{T}} -\\mathbf{m}_1\\mathbf{m}_{2}^{\\mathrm{T}} -\\mathbf{m}_2\\mathbf{m}_{1}^{\\mathrm{T}} +\\mathbf{m}_2\\mathbf{m}_{2}^{\\mathrm{T}}\\right) \\right)\\mathbf{w} &amp;= N(\\mathbf{m}_{1} -\\mathbf{m}_{2}) \\\\ \\left(\\sum_{n\\in\\mathcal{C}_1} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}} + \\sum_{n\\in\\mathcal{C}_2} \\mathbf{x}_{n} \\mathbf{x}_{n}^{\\mathrm{T}} -\\sum_{n\\in\\mathcal{C}_1}\\mathbf{m}_1\\mathbf{m}_{1}^{\\mathrm{T}} -\\sum_{n\\in\\mathcal{C}_2}\\mathbf{m}_2\\mathbf{m}_{2}^{\\mathrm{T}} +\\frac{N_1N_2}{N}\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)^{\\mathrm{T}} \\right)\\mathbf{w} &amp;= N(\\mathbf{m}_{1} -\\mathbf{m}_{2}) \\\\ \\left(\\sum_{n \\in \\mathcal{C}_{1}}\\left(\\mathbf{x}_{n}-\\mathbf{m}_{1}\\right)\\left(\\mathbf{x}_{n}-\\mathbf{m}_{1}\\right)^{\\mathrm{T}}+\\sum_{n \\in \\mathcal{C}_{2}}\\left(\\mathbf{x}_{n}-\\mathbf{m}_{2}\\right)\\left(\\mathbf{x}_{n}-\\mathbf{m}_{2}\\right)^{\\mathrm{T}} +\\frac{N_1N_2}{N}\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)^{\\mathrm{T}} \\right)\\mathbf{w} &amp;= N(\\mathbf{m}_{1} -\\mathbf{m}_{2}) \\\\ \\left(\\mathbf{S}_{\\mathrm{W}}+\\frac{N_{1} N_{2}}{N} \\mathbf{S}_{\\mathrm{B}}\\right) \\mathbf{w}&amp;=N\\left(\\mathbf{m}_{1}-\\mathbf{m}_{2}\\right) \\end{align} \\] And recall that \\(\\mathbf{S}_{\\mathrm{B}}\\mathbf{w}\\) is always in the direction of \\(\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)\\), thus we again have \\[ \\mathbf{w} \\propto \\mathbf{S}_{\\mathrm{W}}^{-1}\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right) \\] Note that from this approach we additionally found an expression for the bias value \\(w_{0}=-\\mathbf{w}^{\\mathrm{T}} \\mathbf{m}\\). Multiple Classes To generalize Fisher LDA to \\(K&gt;2\\) classes, we use weight matrix to introduce multiple 'features' \\[ \\mathbf{y}=\\mathbf{W}^{\\mathrm{T}} \\mathbf{x} \\] The generalization of the within-class covariance matrix is easy to achieve \\[ \\mathbf{S}_{\\mathrm{W}}=\\sum_{k=1}^{K} \\mathbf{S}_{k} = \\sum_{k=1}^{K}\\sum_{n \\in \\mathcal{C}_{k}}\\left(\\mathbf{x}_{n}-\\mathbf{m}_{k}\\right)\\left(\\mathbf{x}_{n}-\\mathbf{m}_{k}\\right)^{\\mathrm{T}}, \\text{ where } \\mathbf{m}_{k}=\\frac{1}{N_{k}} \\sum_{n \\in \\mathcal{C}_{k}} \\mathbf{x}_{n} \\] However, it's hard to generalize the between-class covariance matrix from \\(\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)\\left(\\mathbf{m}_{2}-\\mathbf{m}_{1}\\right)^{\\mathrm{T}}​\\). Instead we can consider first the total matrix \\[ \\mathbf{S}_{\\mathrm{T}}=\\sum_{n=1}^{N}\\left(\\mathbf{x}_{n}-\\mathbf{m}\\right)\\left(\\mathbf{x}_{n}-\\mathbf{m}\\right)^{\\mathrm{T}},\\text{ where } \\mathbf{m}=\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_{n}=\\frac{1}{N} \\sum_{k=1}^{K} N_{k} \\mathbf{m}_{k} \\] and then take \\(\\mathbf{S}_\\mathrm{B}\\) as the form \\[ \\begin{align} \\mathbf{S}_{\\mathrm{B}} &amp;=\\mathbf{S}_{\\mathrm{T}} - \\mathbf{S}_{\\mathrm{W}} \\\\ &amp;= \\sum_{k=1}^{K} \\sum_{n \\in \\mathcal{C}_k}\\left(\\mathbf{x}_{n}-\\mathbf{m}\\right)\\left(\\mathbf{x}_{n}-\\mathbf{m}\\right)^{\\mathrm{T}} - \\sum_{k=1}^{K}\\sum_{n \\in \\mathcal{C}_{k}}\\left(\\mathbf{x}_{n}-\\mathbf{m}_{k}\\right)\\left(\\mathbf{x}_{n}-\\mathbf{m}_{k}\\right)^{\\mathrm{T}} \\\\ &amp;= \\sum_{k=1}^{K} \\sum_{n \\in \\mathcal{C}_k} \\left( \\mathbf{x}_{n}\\mathbf{x}_{n}^\\mathrm{T} - \\mathbf{m}\\mathbf{x}_{n}^\\mathrm{T} - \\mathbf{x}_{n}\\mathbf{m}^\\mathrm{T} + \\mathbf{m}\\mathbf{m}^\\mathrm{T} - \\mathbf{x}_{n}\\mathbf{x}_{n}^\\mathrm{T} + \\mathbf{m}_k\\mathbf{x}_{n}^\\mathrm{T} + \\mathbf{x}_{n}\\mathbf{m}_k^\\mathrm{T} - \\mathbf{m}_k\\mathbf{m}_k^\\mathrm{T} \\right) \\\\ &amp;= \\sum_{k=1}^{K} \\sum_{n \\in \\mathcal{C}_k} \\left( - \\mathbf{m}\\mathbf{x}_{n}^\\mathrm{T} - \\mathbf{x}_{n}\\mathbf{m}^\\mathrm{T} + \\mathbf{m}\\mathbf{m}^\\mathrm{T} + \\mathbf{m}_k\\mathbf{x}_{n}^\\mathrm{T} + \\mathbf{x}_{n}\\mathbf{m}_k^\\mathrm{T} - \\mathbf{m}_k\\mathbf{m}_k^\\mathrm{T} \\right) \\\\ &amp;= \\sum_{k=1}^{K} N_k \\left( - \\mathbf{m}\\mathbf{m}_k^\\mathrm{T} - \\mathbf{m}_k\\mathbf{m}^\\mathrm{T} + \\mathbf{m}\\mathbf{m}^\\mathrm{T} + \\mathbf{m}_k\\mathbf{m}_k^\\mathrm{T} \\right) \\\\ &amp;= \\sum_{k=1}^{K} N_{k}\\left(\\mathbf{m}_{k}-\\mathbf{m}\\right)\\left(\\mathbf{m}_{k}-\\mathbf{m}\\right)^{\\mathrm{T}} \\end{align} \\] We can similarly define them in the projected \\(\\mathbf{y}\\)-space as \\[ \\mathbf{s}_{W}= \\mathbf{W}^{T} \\mathbf{S}_{W} \\mathbf{W} \\\\ \\mathbf{s}_{B}= \\mathbf{W}^{T} \\mathbf{S}_{B} \\mathbf{W} \\] Again we wish to construct a scalar that is large when the between-class covariance is large and when the within-class covariance is small, one possible choice is \\[ J(\\mathbf{W})=\\operatorname{Tr}\\left\\{\\mathbf{s}_{\\mathrm{W}}^{-1} \\mathbf{s}_{\\mathrm{B}}\\right\\} \\] It can be shown that the weight values are determined by those eigenvectors of \\(\\mathbf{S}_{\\mathrm{W}}^{-1} \\mathbf{S}_{\\mathrm{B}}\\), but we are unable to find more than \\((K − 1)\\) linear ‘features’ by this means since from the definition of \\(\\mathbf{S}_\\mathrm{B}\\) we see that the rank of \\(\\mathbf{S}_{\\mathrm{B}}\\) is no longer than \\((K-1)​\\). Nonprobabilistic Model: The Perceptron Algorithm The perceptron algorithm occupies an important place in the history of pattern recognition, which takes the form \\[ y(\\mathbf{x})=f\\left(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}(\\mathbf{x})\\right) \\] where \\(f\\) is a nonlinear activation function given by a step function of the form \\[ f(a)=\\left\\{\\begin{array}{ll}{+1,} &amp; {a \\geq 0} \\\\ {-1,} &amp; {a&lt;0}\\end{array}\\right. \\] So to match the choice of activation function, we use target values \\(t=+1\\) for \\(\\mathcal{C}_1\\) and \\(t=-1\\) for \\(\\mathcal{C}_2\\), which will also help in the later expression of error function. We can determine \\(\\mathbf{w}\\) by simply minimizing the total misclassification patterns. However, this approach corresponds to a piecewise constant error function w.r.t. \\(\\mathbf{w}​\\), gradient based optimization methods cannot be applied. An alternative error function is known as perceptron criterion, which takes the form \\[ E_{\\mathrm{P}}(\\mathbf{w})=-\\sum_{n \\in \\mathcal{M}} \\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}_{n}\\left(\\mathbf{x}\\right) t_{n} \\] where \\(\\mathcal{M}\\) denotes the set of all misclassified patterns. It is motivated by the fact that all correctly classified patterns satisfy \\(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right) t_{n}&gt;0\\), so the larger the value of \\(\\mathbf{w}^{\\mathrm{T}} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\) is, the more confident the classification is. Then an SGD method can be applied after observing one single point \\[ \\mathbf{w}^{(\\tau+1)}=\\mathbf{w}^{(\\tau)}-\\eta \\nabla E_{\\mathrm{P}}(\\mathbf{w})=\\mathbf{w}^{(\\tau)}+\\eta \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right) t_{n} \\] where \\(\\eta​\\) is the learning rate. There is a simple interpretation of perceptron algorithm that when \\(\\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\) is misclassified, the current estimate of \\(\\mathbf{w}\\) will move in the direction of \\(\\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)​\\). An illustration of the interpretation, where \\(\\eta=1\\), each row corresponds to one update step. The arrow points towards the decision region of the red class. After two steps the algorithm converges. Though it can be proved that if the training data set is linearly separable, then the perceptron learning algorithm is guaranteed to find an exact solution in a finite number of steps, this simple algorithm has many limitations as follows: If linear separable, there are many possible solutions depending on different initialization, If not, this algorithm never converge Does not provide probabilistic outputs Cannot generalize to \\(K&gt;2\\) classes","link":"/old/PRML4-1.html"}],"posts":[{"title":"Hexo配置备忘","text":"Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 Minos is a simple and retro styled Hexo theme, concentrating more on your ideas. 基本流程 hexo建站基本配置直接参照官网步骤，配置主题为Minos。 公式支持 网上广为流传的替换hexo默认渲染为hexo-renderer-kramed的方法，仍然会存在行内公式含两个引号时中间部分被渲染为斜体的问题。 最后参照Hexo 书写 LaTeX 公式时的一些问题及解决方法，替换默认渲染为hexo-renderer-pandoc。 -- 注意1：事先去pandoc官方下载最新版本的pandoc，否则直接使用apt install pandoc可能在run服务时因为版本过低出现pandoc: Unknown extension: smart的问题 -- 注意2：替换完成后主题内的依赖关系不会自动更改，需要将themes/minos/scripts/01_check.js中对默认渲染的依赖项'hexo-renderer-marked'手动替换为'hexo-renderer-pandoc' 另外默认mathjax的js脚本无法显示出粗体如\\mathbf，应将layout/plugins/mathjax.ejs文件中的src路径改为 1https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML 图像居中 为了美观（强迫症），修改themes/source/css/style.scss中display属性为block，添加属性text-align: center。","link":"/2018/01/12/hexo-install/"},{"title":"Python 装饰器","text":"装饰器（decorator）是Python的一种高级语法，是用于修改其它函数功能的函数。装饰器代码本身比较复杂，但是使用起来方便简洁，大大提高可读性。 最简单的装饰器例子：记录运行时间 假设我们需要log某个函数的运行时间，最直接的做法便是在函数开头和结束处分别插入记录时间的代码片段，并在最后打印时长： 12345def task(task_num): # begin_time = time.time() print('doing task %d...' % task_num) # end_time = time.time() # print('this task ends in %f seconds.\\n' % (end_time - begin_time)) 这样的做法使函数定义中混入了用于logging的代码，降低了可读性，同时也无法在其他也需要log运行时间的函数中复用这段代码。 而装饰器则可以在不改变原函数本身功能的基础上增加新的功能，同时也能做到快速复用。装饰器的定义实际上就是一个closure，接收原函数作为参数，返回wrap后的函数。我们可以将以上记录运行时长的功能写进装饰器runtime_logging中： 1234567891011import timedef runtime_logging(func): def wrapped(*args, **kwargs): begin_time = time.time() func(*args, **kwargs) end_time = time.time() print('this task ends in %f seconds.\\n' % (end_time - begin_time)) return wrapped 注意wrapper函数使用了万能的*args, **kwargs作为参数以适应各种含不同参数的函数。使用装饰器时只需要简单地在原来函数定义的基础上增加一行@语句： 123@runtime_loggingdef task(task_num): print('doing task %d...' % task_num) 这一行@语句的作用相当于： 1task = runtime_logging(task) 这时候再执行task函数时的效果为： 1234567891011121314for i in range(3): task(i)\"\"\" Output:doing task 0...this task ends in 0.000019 seconds.doing task 1...this task ends in 0.000003 seconds.doing task 2...this task ends in 0.000002 seconds.\"\"\" 使函数被装饰前后的名字保持一致 在Python中，函数也是一个对象，在经过装饰器装饰后，函数功能虽然未发生改变，但函数的一些内置成员变量却会悄悄发生改变。例如在装饰前后，函数名称会发生变化： 123456789101112def task(task_num): print('doing task %d...' % task_num)print(task.__name__)# Output: task@runtime_loggingdef task(task_num): print('doing task %d...' % task_num)print(task.__name__)# Output: wrapped 我们如果需要直接把名字改回去，需要编写wrapped.__name__ = func.__name__这样的代码。而Python发开者早就想到了这一点，可以通过使用内置的functools.wraps达到同样的效果。 123456789import functoolsdef runtime_logging(func): @functools.wraps(func) def wrapped(*args, **kwargs): # the same code as before ... return wrapped 含参的装饰器 如果我们想进一步地扩展装饰器的功能，例如在log的时候根据需要决定是否将日志输出到文件中。这时我们可以在原有装饰器的基础上再包裹上一层含参的函数，这层函数会根据参数来提供不同功能的装饰器： 1234567891011121314151617181920import timeimport functoolsdef log(logfile=None): def runtime_logging(func): @functools.wraps(func) def wrapped(*args, **kwargs): func(*args, **kwargs) message = 'function %s done.' % func.__name__ if logfile: with open(logfile, 'w') as f: f.write(message) else: print(message) return wrapped return runtime_logging 运行效果： 123456789101112131415161718192021@log(None)def task(task_num): print('doing task %d...' % task_num)task(1)\"\"\"Output:doing task 1...function task done.\"\"\"@log('log.txt')def task(task_num): print('doing task %d...' % task_num)task(2)\"\"\"Output:In shell) doing task 2...In 'log.txt') function task done.\"\"\" 这里的@log('log.txt')装饰task函数实际上就相当于： 1task = log('log.txt')(task) 将装饰器用于装饰类 在上面的例子中，装饰器都是起到加工函数的效果。而实际上装饰器还可以被拓展用来加工类，其关键就在于用一个变量记录下被装饰的类的信息。例如我们要为一个Task类引入一个新的计数功能，记录打印信息的次数： 12345678910111213141516171819202122def decorator(cls): class Wrapped(object): def __init__(self, info): self.executed_times = 0 self.wrapped = cls(info) def display_info(self): self.executed_times += 1 self.wrapped.display_info() print('total executed times:', self.executed_times) return Wrapped@decoratorclass Task(object): def __init__(self, info): self.info = info def display_info(self): print('Info:', self.info) 执行效果如下： 123456789101112t = Task('just for test!')for _ in range(3): t.display_info()\"\"\"Output:Info: just for test!total executed times: 1Info: just for test!total executed times: 2Info: just for test!total executed times: 3\"\"\" 将装饰器写成类的形式 在Python中函数实际上就是一个带__call__方法的类，将装饰器写成类的形式可以大大减少装饰器中函数的嵌套层数，同时可以使用继承进一步方便地扩展装饰器的功能。 1234567891011121314151617181920class log(object): def __init__(self, func): self.func = func def __call__(self, *args, **kwargs): result = self.func(*args, **kwargs) print('function %s done.' % self.func.__name__) return result @logdef task(task_num): print('doing task %d...' % task_num)task(2)\"\"\"Output:doing task 2...function task done.\"\"\" 参考资料 [1] Intermediate Python: 7. Decorators [2] Python深入05 装饰器 [3] 廖雪峰Python教程：装饰器","link":"/2018/02/02/py-decorator/"},{"title":"Calculus of Variations","text":"Calculus of variations is a field of mathematical analysis that uses variations, which are small changes in functions and functionals, to find maximum and minimum of functionals. Introduction Functional is the function's function, in other words, functional takes a function as input and outputs a real number, or more mathematically, it refers to a mapping from a space \\(X\\) (commonly \\(X\\) is a space of functions) into the real numbers. In the calculus of variations, we seek a functional \\(f(x)\\) that maximizes (or minimizes) a functional \\(F[f]\\). For example, as we all know, the shortest curve between two fixed points is the straight line, and this can be proved using calculus of variations. Example 1: Shortest Curve To find out the shortest curve \\(y(x)\\) between two fixed points \\((a_1, b_1)\\) and \\((a_2, b_2)\\), we first express the length of the curve in the form of integration \\[ F[y] = \\int_{a_1}^{a_2} \\sqrt{1+[y&#39;(x)]^2} dx \\] where we have the boundary conditions of \\(y(x)\\) \\[ y(a_1) = b_1,\\ y(a_2) = b_2 \\] Suppose \\(y(x)\\) is twice continuously differentiable. Now if the functional \\(F\\) attains a local minimum at \\(y=f\\), consider \\(\\eta(x)\\) to be an arbitrary function that has at least one derivative and vanishes at end points \\(a_1\\) and \\(a_2\\), i.e. \\(\\eta(a_1) = 0\\) and \\(\\eta(a_2)=0\\), then we have \\[ F[f] \\leq F[f + \\epsilon\\eta] \\] where \\(\\epsilon\\) can be any number close to \\(0\\), here the term \\(\\epsilon \\eta\\) is called the variation of the function \\(f\\) and is denoted by \\(\\delta f\\). We can view \\(F[f+\\epsilon \\eta]\\) as a function of \\(\\epsilon\\) and denote it as \\(J(\\epsilon)\\), then \\(J&#39;(0) = 0\\) must be true for any arbitrary choice of \\(\\eta\\) since the functional \\(F[y]\\) achieves minimum when \\(y=f\\). By calculating \\[ \\begin{aligned} \\left. J&#39;(\\epsilon) \\right|_{\\epsilon=0} &amp;= \\left. \\frac{d}{d\\epsilon}F[f+\\epsilon\\eta] \\right|_{\\epsilon=0} \\\\ &amp;= \\left. \\frac{d}{d\\epsilon} \\int_{a_1}^{a_2} \\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2}dx \\right|_{\\epsilon=0} \\\\ &amp;= \\int_{a_1}^{a_2} \\left. \\frac{d}{d\\epsilon} \\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2} \\right|_{\\epsilon=0} dx \\\\ &amp;= \\int_{a_1}^{a_2} \\left. \\frac{[f&#39;(x) + \\epsilon\\eta&#39;(x)]\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x) + \\epsilon\\eta&#39;(x)]^2}} \\right|_{\\epsilon=0} dx \\\\ &amp;= \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx \\end{aligned} \\] We get \\[ \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx = 0 \\] which means that in differentiable function space, \\(F[y]\\)'s derivatives in all directions (all \\(\\eta\\)) are zero. Since \\(y(x)\\) is twice continuously differentiable, we can further integrate the left hand side of above equation by parts \\[ \\begin{aligned} \\int_{a_1}^{a_2} \\frac{f&#39;(x)\\eta&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}dx &amp;= \\left. \\eta(x) \\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right|_{a_1}^{a_2} -\\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx \\\\ &amp;= -\\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx\\ \\ \\ \\text{(since $\\eta(a_1) = 0$ and $\\eta(a_2) = 0$)} \\end{aligned} \\] then now we have \\[ \\int_{a_1}^{a_2} \\eta(x) \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}}\\right]dx = 0 \\] Since \\(\\eta(x)\\) is arbitrary, imagine choosing a perturbation \\(\\eta(x)\\) that is zero everywhere except in the neighborhood of a point \\(\\hat{x}\\), in which case the functional derivative must be zero at \\(x=\\hat{x}\\), otherwise the integral will not be equal to zero. However, because this must be true for every choice of \\(\\hat{x}\\), the functional derivative must vanish for all values of \\(x\\), that is \\[ \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right] = 0 \\] This is called the fundamental lemma of calculus of variations. Follow this result we have \\[ \\frac{d}{dx}\\left[\\frac{f&#39;(x)}{\\sqrt{1 + [f&#39;(x)]^2}} \\right] = \\frac{f&#39;&#39;(x)}{(1 + [f&#39;(x)]^2)^{\\frac{3}{2}}} = 0 \\] So \\[ f&#39;&#39;(x)=0 \\] Euler–Lagrange Equation Now consider the more general case, for the functional \\[ F[y] = \\int_{x_1}^{x_2} L(x,y(x), y&#39;(x)) dx \\] Again suppose the functional attains a local minimum at \\(y=f\\), and define \\(J(\\epsilon) = F[f+\\epsilon\\eta]\\). Then we have \\[ \\begin{aligned} \\left. J&#39;(\\epsilon) \\right|_{\\epsilon=0} &amp;= \\int_{x_1}^{x_2} \\left. \\frac{dL}{d\\epsilon} \\right|_{\\epsilon=0}dx \\\\ &amp;= \\int_{x_1}^{x_2} \\left(\\frac{\\partial L}{\\partial f}\\eta +\\frac{\\partial L}{\\partial f&#39;}\\eta&#39; \\right) dx \\\\ &amp;= \\int_{x_1}^{x_2} \\frac{\\partial L}{\\partial f}\\eta\\ dx +\\left.\\frac{\\partial L}{\\partial f&#39;}\\eta \\ \\right|_{x_1}^{x_2} - \\int_{x_1}^{x_2} \\eta \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] dx \\\\ &amp;= \\int_{x_1}^{x_2} \\frac{\\partial L}{\\partial f}\\eta\\ dx - \\int_{x_1}^{x_2} \\eta \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] dx \\\\ &amp;= \\int_{x_1}^{x_2} \\eta \\left(\\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] \\right) dx \\end{aligned} \\] According to the fundamental lemma of calculus of variations, we get \\[ \\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] = 0 \\] which is called the Euler–Lagrange equation. The left hand side of this equation is called the functional derivative of \\(F[f]\\), denoted as \\[ \\frac{\\delta J}{\\delta f} = \\frac{\\partial L}{\\partial f}- \\frac{d}{dx}\\left[ \\frac{\\partial L}{\\partial f&#39;}\\right] \\] References [1] Wikipedia: Calculus of variations [2] Bishop, C. M. (2006). Pattern recognition and machine learning, appendix D. New York: Springer.","link":"/2018/04/18/variations/"},{"title":"Lagrange Multiplier and KKT Conditions","text":"In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maximum and minimum of a function subject to equality constraints, while KKT conditions solve the problems with inequality constraints. Naive Approach Consider the problem of finding the maximum of a function \\(f(x_1, x_2)\\) subject to a constraint relating \\(x_1\\) and \\(x_2\\), which we write in the form \\[ g(x_1, x_2) = 0 \\] One approach would be to solve the above equation to express \\(x_2\\) as a function of \\(x_1\\) in the form of \\(x_2 = h(x_1)\\), then apply differentiation to \\(f(x_1, h(x_1))\\) w.r.t. \\(x_1\\) in the usual way. However, there are at least two drawbacks of this simple approach: Sometimes difficult or even impossible to find out an expression like \\(x_2 = h(x_1)\\) Spoil the natural symmetric between these variables A more elegant way is introducing a parameter \\(\\lambda\\) called a Lagrange multiplier. Lagrange Multiplier An Intuitive Understanding Now consider in a \\(D\\)-dimensional space where we need to \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) = 0 \\] Geometrically, the constraint \\(g(\\mathbf{x}) = 0\\) represents a \\((D-1)\\)-dimensional surface, and what's worth noting is that at any point on the constraint surface, the gradient \\(\\nabla g(\\mathbf{x})\\) will be orthogonal to the surface. To see this, consider the total differential \\(dg = \\nabla g(\\mathbf{x})^T d \\mathbf{x}\\), then if the point \\(\\mathbf{x}+d\\mathbf{x}\\) also lies on the constraint surface, then \\(dg=0\\), so we have \\(\\nabla g(\\mathbf{x})^T d\\mathbf{x} = 0\\). Next, the point \\(\\mathbf{x}^*\\) on the constraint surface that maximize \\(f(\\mathbf{x})\\) must have the property that \\(\\nabla g(\\mathbf{x}^*) \\parallel \\nabla f(\\mathbf{x^*})\\), in other words, \\(\\nabla g(\\mathbf{x}^*)\\) is orthogonal to the constraint surface, because otherwise we could increase the value of \\(f(\\mathbf{x})\\) by moving a short distance along the constraint surface. Personally, I think it's quite similar to the force resolution, where we can view the point \\(\\mathbf{x}\\) as a ball on its orbit -- the constraint surface. A force, \\(\\nabla f(\\mathbf{x})\\), is applied on it and the ball will finally moves up to a place where no force can be resolved on the tangent plane of the place. Since \\(\\nabla g(\\mathbf{x}^*) \\parallel \\nabla f(\\mathbf{x^*})\\), there must exist a parameter \\(\\lambda\\) such that \\[ \\nabla f(\\mathbf{x}^*) +\\lambda \\nabla g(\\mathbf{x^*}) = 0 \\] where \\(\\lambda\\neq 0\\) is known as Lagrange multiplier. For representation convenience, we can introduce the Lagrangian function defined by \\[ L(\\mathbf{x}, \\lambda) \\equiv f(\\mathbf{x}) + \\lambda g(\\mathbf{x}) \\] By setting \\(\\nabla_{\\mathbf{x}} L=\\mathbf{0}\\) together with the constraint \\(g(\\mathbf{x}) = 0\\), we can get all the necessary conditions (but not sufficient) when \\(\\mathbf{x}\\) is an extreme point on the constraint surface. Example 1 \\[ \\max \\ 1-x_1^2 - x_2^2 \\\\ \\text{s.t.} \\ \\ x_1 + x_2 -1 =0 \\] The corresponding Lagrange function is given by \\(L(\\mathbf{x}, \\lambda) = 1-x_1^2 - x_2^2 + \\lambda (x_1 + x_2 -1)\\), then set \\[ \\begin{aligned} \\frac{\\partial L}{\\partial x_1} &amp;= -2x_1 + \\lambda = 0 \\\\ \\frac{\\partial L}{\\partial x_2} &amp;= -2x_2 + \\lambda = 0 \\\\ \\frac{\\partial L}{\\partial \\lambda} &amp;= x_1 + x_2 -1 = 0 \\end{aligned} \\] By solving the system of equations we get the stationary point on the constraint surface is \\((\\frac{1}{2},\\frac{1}{2})\\), and it can be verified that it is a maximum point by checking nearby points or using second-order sufficient conditions (see further reading 2). KKT Conditions So far we have considered the optimization problem with equality constraint only. Now consider such problem with inequality constraint \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) \\geq 0 \\] The solution of this problem can be classified into two kinds: When the constraint is active, that is when the stationary point lies on the boundary \\(g(\\mathbf{x})=0\\), the problem is just analogous to the one discussed previously and corresponds to a stationary point with \\(\\lambda \\neq 0\\). Note that now the sign of \\(\\lambda\\) is crucial, since \\(f(\\mathbf{x})\\) will only be at a maximum if \\(\\nabla f(\\mathbf{x})\\) is away from the region \\(g(\\mathbf{x})&gt;0\\). So further we have the corresponding \\(\\lambda &gt; 0\\). When the constraint is inactive, the stationary point lies in the region \\(g(\\mathbf{x})&gt;0\\), with corresponding \\(\\lambda = 0\\). Again for the convenience of representation, note that for both cases, the product \\(\\lambda g(\\mathbf{x})=0\\), thus the solution can be obtained by maximizing the Lagrange function w.r.t. \\(\\mathbf{x}\\) s.t. the conditions \\[ \\begin{aligned} g(\\mathbf{x}) &amp;\\geq 0 &amp; &amp; \\text{(constriant function)} \\\\ \\lambda &amp;\\geq 0&amp;&amp; \\text{(gradient direction)} \\\\ \\lambda g(\\mathbf{x})&amp;=0&amp;&amp; \\text{(complementary slackness condition)} \\end{aligned} \\] These are known as Karush-Kuhn-Tucker(KKT) conditions. If we need to apply KKT conditions to minimization problems, just change the sign of \\(\\lambda\\) in Lagrange function to keep the gradient direction reversed, i.e. \\(L(\\mathbf{x}, \\lambda) \\equiv f(\\mathbf{x}) - \\lambda g(\\mathbf{x})\\) ### Multiple Equality and Inequality Constraints Finally, it's now easy to extend the method to the case of multiple equality and inequality constraints \\[ \\max f(\\mathbf{x}) \\\\ \\text{s.t.} \\ \\ g_i(\\mathbf{x}) \\geq 0, \\ i=1,...,m \\\\ \\ \\ \\ \\ \\ h_j(\\mathbf{x}) =0, \\ j=1,...,l \\] Define the Lagrange function to be \\[ L(\\mathbf{x},\\mathbf{v},\\mathbf{w}) = f(\\mathbf{x}) + \\sum_{i=1}^m w_i g_i(\\mathbf{x}) + \\sum_{j=1}^l v_j h_j(\\mathbf{x}) \\] Now the necessary conditions when a point \\(\\mathbf{x}\\) becomes the local optimum is \\[ \\left\\{\\begin{aligned} &amp; \\nabla_x L(\\mathbf{x},\\mathbf{w},\\mathbf{v}) = 0 \\\\&amp;g_i(\\mathbf{x}) \\geq 0, \\ i=1,...,m \\\\&amp;h_j(\\mathbf{x}) = 0, \\ j=1,...,l \\\\&amp; w_i \\geq 0,\\ i=1,...,m \\\\&amp; w_i g(\\mathbf{x})=0,\\ i=1,...,m \\end{aligned}\\right. \\] Example 2 \\[ \\min \\ x_1^2 - x_2 - 3x_3 \\\\ \\text{s.t.} \\ \\ g(\\mathbf{x}) = -x_1 - x_2 - x_3 \\geq 0 \\\\ \\ \\ \\ \\ \\ \\ h(\\mathbf{x}) = x_1^2 + 2x_2 - x_3 = 0 \\] Write down Lagrange function for minimization problem \\[ L(\\mathbf{x}, w, v) = (x_1^2 - x_2 - 3x_3) - w(-x_1 - x_2 - x_3) - v(x_1^2 + 2x_2 - x_3 ) \\] As described above, the first-order necessary condition of optimum is \\[ \\left\\{\\begin{aligned} &amp;L_{x_1}&#39; = 2x_1 + w - 2vx_1 =0 \\\\ &amp;L_{x_2}&#39; = -1 + w - 2v=0 \\\\ &amp;L_{x_3}&#39; = -3 + w +v=0 \\\\ &amp;-x_1 - x_2 - x_3 \\geq 0 \\\\ &amp; x_1^2 + 2x_2 - x_3 = 0 \\\\&amp;w\\geq 0 \\\\&amp; w(-x_1 - x_2 - x_3) = 0 \\end{aligned}\\right. \\] The solution is \\(\\mathbf{x}^* = (-\\frac{7}{2}, -\\frac{35}{12}, \\frac{77}{12}), \\ w = \\frac{7}{3}, \\ v=\\frac{2}{3}\\). Further Reading Lagrange Multipliers Can Fail To Determine Extrema In single variable case, when the gradient of the constraint function \\(\\nabla g(\\mathbf{x})=0\\), the simple Lagrangian multiplier discussed above doesn't work. e.g. \\(\\min \\ x \\text{, s.t.} \\ \\ g(x,y) = y^2 + x^4 - x^3=0\\). http://web.cs.iastate.edu/~cs577/handouts/lagrange-multiplier.pdf Explain Lagrange multiplier in multi-constraint case in detail, and give the second-order sufficient conditions, which use the definitive property of Lagrangian function's Hessian on the tangent space to verify if a stationary point is also an extreme point in a more mathematical way. Besides, it avoids discussing the cases like that mentioned in the above article by introducing the concept of regular point. Tangent space at x^* on a surface defined by (a) one constraint (shown as a tangent plane) and (b) two constraints (shown as a tangent line). According to the second-order sufficient conditions mentioned in further reading 2, we can verifying the solution as optimum in example 2 by first calculating the Hessian at this point \\[ \\nabla_{\\mathbf{x}}^2L(\\mathbf{x}^*, w, v) = \\begin{bmatrix} \\frac{2}{3} &amp;0&amp; 0 \\\\0&amp;0&amp;0\\\\0&amp;0&amp;0 \\end{bmatrix} \\] Since both the two constraints are active at \\(\\mathbf{x^*}\\), we can solving the systems of equations below to get the expression of the tangent space at this point \\[ \\left\\{ \\begin{aligned} \\nabla g(\\mathbf{x}^*)^T \\mathbf{d} = 0 \\\\ \\nabla h(\\mathbf{x}^*)^T \\mathbf{d} = 0 \\end{aligned} \\right. \\] where \\(\\mathbf{d} = (d_1, d_2, d_3)^T\\) stands for any feasible direction on the tangent space, and \\(g(\\mathbf{x}^*) = (-1,-1,-1)^T\\), \\(h(\\mathbf{x}^*) = (-7,2,-1)\\). By solving that we have \\(\\mathbf{d} = (d_1,2d_1,-3 d_1)^T\\). Because \\(\\mathbf{d}^T\\nabla^2_{\\mathbf{x}}L(\\mathbf{x}^*, w, v) \\mathbf{d}=\\frac{2}{3}d_1^2&gt;0\\), any little change away from \\(\\mathbf{x}^*\\) always makes \\(L\\) increase, the objective function really reaches local minimum at the point \\(\\mathbf{x}^*\\), which is also the global minimum point as well since there is no other local minimum. References [1] Wikipedia: Lagrange multiplier [2] Wikipedia: Karush–Kuhn–Tucker conditions [3] Bishop, C. M. (2006). Pattern recognition and machine learning, appendix E. New York: Springer.","link":"/2018/04/21/KKT/"},{"title":"1st Place Solution of NeurIPS 2022 The Neural MMO Challenge","text":"The Neural MMO environment was firstly released by OpenAI in 2019[1] and is now developed and maintained by Joseph Suarez in MIT[2]. It is a platform for large-scale multi-agent research. The design of Neural MMO is inspired by large-scale multiplayer online role-playing games (MMORPGs) and simulates a large ecosystem in which a variable number of players can compete in a vast environment. Unlike Dota[3] and Starcraft[4], where AI has already achieved super-human performance, AI design in Neural MMO should not only consider coordination among a large number of agents within a team, but also consider competition with dozens of enemy teams or even more. Based on this environment, the Neural MMO challenge has been held for three times so far. By introducing new equipment system, trade system and poison fog, the difficulty of the competition has been largely increased for the NeurIPS challenge. This post will briefly introduce our solution of realikun, which got the 1st place in this challenge with the highest kill number, longest alive time, highest received damage and most gold farmed. Click here to see the solution code. The method is a combination of multi-agent reinforcement learning and rule-based approach. We train the agents with PPO[5] in a CTCE (Centralized-Training-Centralized-Execution) style, where the whole observation is team-based and also augmented from the teammates for each agent. The reinforcement learning model is heavily involved in the decision of choosing moving directions, targets of attacking, and the using and selling of only consumable items, some corner cases like forcing the agent not to walk onto lava or always not attacking the neutral NPCs in our PvP version, are controlled via action masks. The rule-based policy controls the selection of attacking styles, the pricing of selling, selling of non-consumable items and all kinds of buying. We do not use the communication action from the raw action space as our method is a CTCE approach. Model Architecture The most parts of the post will be focused on the introduction to the model architecture we use. The transformer and LSTM locates in the core of the network, which compose most of the model’s total parameter count. Each of the eight players on the team is controlled by a replica of this network but with nearly totally different input of their own. The design is mainly borrowed from OpenAI Five[3] and Hide and Seek[6]. Now let’s dive into the details of each part. Model Architecture The first to introduce is the Item Encoder. For each item, an embedding is generated according to its type, and concatenated with some other statistical information of the item like the level or damage, and then projected through a fully connected layer. The 12 item hiddens are then pooled into a single vector to get a summary of the player’s inventory. The Action Encoder is rather simple, it takes the 4 kinds of chosen actions in the last turn as the input / and outputs the concatenation of their embeddings. We use a 3-layer resnet as Tile Encoder to encode the 2-dimensional features, the input for each player has 7 channels with a \\(25\\times25\\) plane. Each resnet layer consists of 2 residual blocks. The internal representation for each player is a combination of the hidden representations from the previous 3 encoders, plus 3 kinds of raw features including the action masks we defined, player info like position, health, and some info about the whole environment like the game progress. All these informations are concatenated to form a summary vector about each controlled player. Now we get 8 vectors in total, representing the 8 players in the controlled team respectively. To do feature interactions among different entities, we use a 3-layer transformer encoder as the backbone. For each agent, the input for the transformer consists of hidden representations of 26 entities, 1 from the controlled player itself, 7 from the teammates, and the nearest 9 NPCs and 9 enemies. Note that we extend the info of nearest NPCs and enemies of the controlled player from its teammates’ view, so for example, if some other teammate finds an enemy, the feature of this enemy may appear in the input of the controlled agent, though the agent can’t see the enemy by itself. To reduce the computation cost of transformer, the hidden representations of controlled players are first projected into a space with lower dimension. To motivate stronger interaction between the controlled player and other entities, the hidden representation of the controlled player itself is concatenated with the representation of others and goes through an additional fully connected layer before they are fed into the transformer. The last part of the model is the Action Decoder, where a 1-layer LSTM is applied to deal with the partially observable problems in this environment. We first max pool the output of the transformer over the dimension of sequence, but before it is further to be pro’cessed, it is concatenated of some additional information provided by projection from the original hidden representation of the controlled player, because we think some important information may get lost from the low-dimension input of transformer we used before. After some interactions with historial infos, we use four separate policy heads to output the agent’s decisions about moving, targets of attacking, using and selling, action masks are applied here to reduce the some exploration cost. We only use a single scalar output to predict the value of the whole team, whose input is an averaged pool over the output of LSTM from all the 8 agents. 7 Channels of Tile Feature As we mentioned before that the 2-dimensional feature of tile encoder has the shape of \\(7\\times25\\times25\\), these channels are specially designed as follows. First we extend the original \\(15\\times15\\) view of agent to \\(25\\times25\\), the information in the extended areas come from both the historical view and something currently shared by its teammates. The first channel is the tile type, the type info is compressed into a single channel by dividing by 16, and we add an additional type representing the unexplored area. The second channel is the entity type, the different values are assigned for teammates, enemies and 3 kinds of NPCs. The third channel is the fog of war, which is a common concept used in RTS games, where a higher value indicates the tile has been recently explored. The fourth channel is the footprint, which explicitly encodes the historical moves of the agent. The last three channels are the poison intensity and coordinates of each tile. Reward Design Only six kinds of reward is straightly defined as follows: Type Value dead \\(-1\\) defeat an enemy \\(+0.3\\) last hit on a passive NPC \\(+0.02\\) per HP change \\(\\pm0.005\\) hostile NPC in view \\(-0.1\\) collect a poultice by walking onto a herb tile when #\\(item &lt; 10\\) \\(+0.025\\) Behavior Analysis Blocking passerby_82 finds two ikuns and starts to move backward, realikun_25 tries to chase it. passerby_82 doesn’t know the place of realikun_31, it chooses move along the edge of death fog while keeping away from realikun_25, getting closer to realikun_31. realikun_25 and realikun_31 join together and drive passerby_82 to a corner, where it either receives a lot of damage from death fog if it continues to escape, or being attacked by two ikuns if it wants to receive less damage from death fog. Note that realikun_25 and realikun_31 are also farming by killing NPCs around in the meanwhile. No NPC is left, realikun_31 thinks passerby_82 won’t move away and believes its teammate can handle the case (my guess) and moves away. now the damage of death fog is really high and passerby_82 is going to die, realikun_25 gets closer to it and takes the last hit, then quickly move towards the map center. Crossfire zhangzhang_65 is found by realikun_31. After some battle, none of them can effectively hurt each other because both of them equips high level armors/tool and they have the same profession. realikun_25 and realikun_28 come to help. As the agent ids of realikun team are smaller than zhangzhang’s, they have higher priority to move (IIRC), and finally with the help of stone terrain, the three ikuns block all the directions to move for zhangzhang_65, and finally kill it. Unfortunately, zhangzhang_65 has stored many poultices before this combat, which can’t be known by other agents. It takes too many rounds for realikun_31 to kill it, but this agent doesn’t have enough poultices or rations and soon dies of starving. Group Fighting In the very beginning, killing even passive NPC can take a lot of time for a single agent. The realikun team qiuckly form into two groups, one has 3 agents, and the other has 5 agents with more NPCs wandering around them. All the agents’ highest combat levels are at least 3, and they do not witness any enemy in their sight, so now they decide to farm separately. During this period, some of them sometimes get closer as they find some enemy they can kill, but not very strong enemy. realikun_109 loses its life for running out of resource. The team fighting starts. realikun_112 alone meets three agents from the team passerby and starts to escape, then all other alive teammates, except reaikun_106/111 who are busy fighting against Qinwen_15, start moving towards it to help. realikun_105 and realikun_108 meet mori_89 on their way and change their idea to kill the opponent at their present, realikun_106/107/110 reach the battleground and turn the situation from 1v3 into 4v3, and then quickly eliminate two of the passerby agents, the left one escaped. All the four realikuns who just fight against passerby team join the group of reaikun_106/111 to chase the not killed opponent mori_89. realikun_110 leaves them, now there are 5 realikuns chasing mori_89. realikun_105 in the mori-chasing group finds another 2 passerby agents, and at the same time the realikun_110, who is now grouping with realikun_111 and chasing the escaping passerby agent, is also coming. Seven realikuns fight together, try to lock up the opponents from different sides with the help of surrounding stones. Finally, mori_89 and another passerby agent get killed. Big Picture This segment shows the tactics of the realikun team, born on the left side of the map, in the first 600 rounds of the game from a more macro perspective. In the first 50 rounds, due to the weak individual combat power, realikun spontaneously formed two small teams to kill NPCs. Between the round 50th and 250th, as each agent had accumulated a certain amount of equipment, they chose the more efficient strategy of dispersed money making, but still kept a certain distance from each other and could form small teams to protect each other when encountering threats. After the 250th round, when the equipment set was basically formed, each agent had strong combat ability, and the team goal changed to actively searching for other players to kill, and the team's action radius was further expanded. References [1] Neural MMO: A massively multiagent game environment for training and evaluating intelligent agents [2] The Neural MMO Platform for Massively Multiagent Research [3] Dota 2 with Large Scale Deep Reinforcement Learning [4] AlphaStar: Mastering the real-time strategy game StarCraft II [5] Proximal Policy Optimization Algorithms [6] Emergent Tool Use From Multi-Agent Autocurricula","link":"/2022/12/18/nmmo/"},{"title":"ShadowsocksR资源整理","text":"由于一些众所周知的原因，我们并不能自由访问完整的互联网。但是互联网审查粒度过粗，导致许多本不需要审查的内容被拒之墙外，如Google Scholar，Wikipedia等。 ShadowsocksR是一种基于Socks5代理的加密传输协议，是shadowsocks的加强版本，可以帮助我们突破中国互联网审查(GFW)从而浏览被封锁的内容。ShadowsocksR分为服务器端和客户端，在使用之前，需要先将服务器端部署到服务器上面，然后通过客户端连接并创建本地代理。 下面整理的是近期配置SSR时收集的一些资源。 服务端基本搭建流程 为了锐速的安装，Vultr购买系统为CentOS 6的服务器。尽量避免开到45.76, 208.开头的IP重灾区号段。 检验拿到的IP是否被封通过端口扫描，常见封22端口禁止ssh——TCP阻断。 服务端安装脚本基本参照【新手向】【梯子/代理】Vultr的购买+SSR+锐速+多端口的配置。但其中部分设置根据调研存在不合理之处。 更换内核以便安装锐速： 12345yum updaterpm -ivh http://www.aloneray.com/wp-content/uploads/2017/03/kernel-firmware-2.6.32-504.3.3.el6.noarch.rpmrpm -ivh http://www.aloneray.com/wp-content/uploads/2017/03/kernel-2.6.32-504.3.3.el6.x86_64.rpm --forcerpm -qa | grep kernel # 确认内核是否更换成功,当看到有kernel-2.6.32-504.3.3.el6.x86_64就说明更换成功了reboot 安装SSR客户端，以及个人推荐使用的设置： 12345678910111213141516wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocksR.shchmod +x shadowsocksR.sh./shadowsocksR.sh 2&gt;&amp;1 | tee shadowsocksR.log# port: 80# protocol: auth_chain_a# encrpt: none# obfs: plain/simple_http# ================= 常用指令 =================# 启动：/etc/init.d/shadowsocks start# 停止：/etc/init.d/shadowsocks stop# 重启：/etc/init.d/shadowsocks restart# 状态：/etc/init.d/shadowsocks status# 卸载：./shadowsocksR.sh uninstall# 配置文件路径： /etc/shadowsocks.json 首先18年3月有推文指出SSR的tls凭据复用已经成为安全问题，即发布于17年8月后长期未更新的ShadowsocksR 协议插件文档中强烈推荐使用的混淆方法tls1.2_ticket_auth已不再能使用。亲测使用后48小时内被封。 端口理论上应当使用80/443分别对应需要伪装成的http/https流量。若使用simple_http混淆则应使用80端口。 混淆参数的设置参见SSR混淆及混淆协议参数的设置，http_simple可以自定义几乎完整的http header。 auth_chain_a基本是目前 SSR 最佳的稳定版协议，根据ShadowsocksR 协议插件文档应对应使用无加密none。 安装锐速加速，实测东京节点锐速效果要优于BBR，这也是不使用Vultr上自带BBR的Debian 9等系统的原因： 123456789wget -N --no-check-certificate https://raw.githubusercontent.com/91yun/serverspeeder/master/serverspeeder-all.sh &amp;&amp; bash serverspeeder-all.shservice iptables stop # 关闭防火墙chkconfig iptables off# ================= 常用指令 =================# 重启：/serverspeeder/bin/serverSpeeder.sh restart# 启动：/serverspeeder/bin/serverSpeeder.sh start# 停止：/serverspeeder/bin/serverSpeeder.sh stop# 状态：/serverspeeder/bin/serverSpeeder.sh status 客户端安装 Platform URL Windows https://github.com/shadowsocksr-backup/shadowsocksr-csharp/releases MacOS https://github.com/shadowsocksr-backup/ShadowsocksX-NG/releases Android https://github.com/shadowsocksr-backup/shadowsocksr-android/releases 针对普通用户的使用教程详见[ShadowsocksR] 大概是萌新也看得懂的SSR功能详细介绍&amp;使用教程，十分详尽。 Linux下参考在Linux的环境安装shadowsocksR客户端： 12345678# 安装wget https://onlyless.github.io/ssrsudo mv ssr /usr/local/binsudo chmod 766 /usr/local/bin/ssrssr install# 配置与启动ssr configssr start 为了方便使用还需要设置开机自动启动ssr。不过ubuntu 18.04不能像ubuntu 14一样通过编辑rc.local来设置开机启动脚本，通过下列简单设置后，可以使rc.local重新发挥作用。参考ubuntu-18.04 设置开机启动脚本进行设置： 建立rc-local.service文件，sudo vim /etc/systemd/system/rc-local.service，复制进以下内容： 1234567891011121314[Unit]Description=/etc/rc.local CompatibilityConditionPathExists=/etc/rc.local [Service]Type=forkingExecStart=/etc/rc.local startTimeoutSec=0StandardOutput=ttyRemainAfterExit=yesSysVStartPriority=99 [Install]WantedBy=multi-user.target 创建文件rc.local，sudo vim /etc/rc.local，将下列内容复制进去： 1234567891011121314#!/bin/sh -e## rc.local## This script is executed at the end of each multiuser runlevel.# Make sure that the script will &quot;exit 0&quot; on success or any other# value on error.## In order to enable or disable this script just change the execution# bits.## By default this script does nothing.sudo ssr startexit 0 给rc.local加上权限，启用服务，检查状态 1234sudo chmod +x /etc/rc.localsudo systemctl enable rc-localsudo systemctl start rc-local.servicesudo systemctl status rc-local.service GenPac的使用 使用系统的自动代理功能，实现墙内外流量分流，科学上网。GenPac是基于gfwlist的多种代理软件配置文件生成工具。安装后根据gfwlist生成代理规则.pac文件，最后设置系统根据该.pac自动代理即可。Linux下的一般安装步骤： 12pip install -U genpacgenpac --proxy=\"SOCKS5 127.0.0.1:1080\" --gfwlist-proxy=\"SOCKS5 127.0.0.1:1080\" -o autoproxy.pac --gfwlist-url=\"https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt\" # 生成代理规则文件 其他平台可直接根据gfwlist.txt使用在客户端中使用pac模式。 其他资源链接汇总 参照SS-and-SSR-Collection，这里备份用再写一遍。 Shadowsocks Platform URL Windows https://github.com/shadowsocks/shadowsocks-windows/releases MacOS https://github.com/shadowsocks/ShadowsocksX-NG/releases Android https://github.com/shadowsocks/shadowsocks-android/releases obfs https://github.com/shadowsocks/simple-obfs-android/releases SSTap 用于非http流量的代理，主要用于游戏。https://www.sockscap64.com/sstap-enjoy-gaming-enjoy-sstap/ Rules Rule URL SSTap https://github.com/FQrabbit/SSTap-Rule GFWList https://github.com/gfwlist/gfwlist ChinaList https://github.com/felixonmars/dnsmasq-china-list PAC https://github.com/breakwa11/gfw_whitelist chnrouter IP URL IPIP https://raw.githubusercontent.com/17mon/china_ip_list/master/china_ip_list.txt APNIC curl 'http://ftp.apnic.net/apnic/stats/apnic/delegated-apnic-latest' | grep ipv4 | grep CN | awk -F\\| '{ printf(\"%s/%d\\n\", $4, 32-log($5)/log(2)) }' &gt; chnroute.txt DNS Reference: 域名服务器缓存污染。 DNS URL ChinaDNS https://github.com/shadowsocks/ChinaDNS Pcap DNSProxy https://github.com/chengr28/Pcap_DNSProxy overture https://github.com/shawn1m/overture","link":"/2018/03/21/ssr/"}],"tags":[{"name":"memo","slug":"memo","link":"/tags/memo/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"math","slug":"math","link":"/tags/math/"},{"name":"reinforcement learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"ssr","slug":"ssr","link":"/tags/ssr/"}],"categories":[]}