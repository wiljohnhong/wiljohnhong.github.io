<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
<title>(PRML Notes) 1.5 Decision Theory - Wiljohn&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">








    <meta name="description" content="A series of notes taken from Pattern Recognition and Machine Learning.  Decision theory allows us to make optimal decisions in situations involving uncertainty. Informally, for classification problem">
<meta name="keywords" content="PRML">
<meta property="og:type" content="website">
<meta property="og:title" content="(PRML Notes) 1.5 Decision Theory">
<meta property="og:url" content="http://wiljohn.top/cache/PRML1-5.html">
<meta property="og:site_name" content="Wiljohn&#39;s Blog">
<meta property="og:description" content="A series of notes taken from Pattern Recognition and Machine Learning.  Decision theory allows us to make optimal decisions in situations involving uncertainty. Informally, for classification problem">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/11235035.jpg">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/80196176.jpg">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/76982210.jpg">
<meta property="og:updated_time" content="2020-10-24T15:22:50.873Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="(PRML Notes) 1.5 Decision Theory">
<meta name="twitter:description" content="A series of notes taken from Pattern Recognition and Machine Learning.  Decision theory allows us to make optimal decisions in situations involving uncertainty. Informally, for classification problem">
<meta name="twitter:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/11235035.jpg">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">

<link rel="stylesheet" href="/css/style.css">

<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


    
    

    
    
    
    
    


</head>
<body>
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item " href="/archives">Archives</a>
            
            <a class="navbar-item " href="/tags">Tags</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/wiljohnhong">
                
                <i class="fab-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            (PRML Notes) 1.5 Decision Theory
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2018-04-30T10:38:59.000Z" itemprop="datePublished">Apr 30 2018</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            16 minutes read (About 2404 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<p>Decision theory allows us to make optimal decisions in situations involving uncertainty.</p>
<p>Informally, for classification problem, we are interested in the posterior probability given data which belongs to a class <span class="math inline">\(\mathcal{C}_k\)</span> <span class="math display">\[
p(\mathcal{C}_k|\mathbf{x}) = \frac{p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\mathbf{x})}
\]</span> Our aim is to minimize the chance of assigning <span class="math inline">\(\mathbf{x}\)</span> to the wrong class, then intuitively we would choose the class having the higher posterior probability. Actually the intuition is correct, which is shown below.</p>
<a id="more"></a>
<h2 id="minimizing-the-miss-classification-rate">Minimizing the Miss Classification Rate</h2>
<p>Suppose the input space is divided into regions <span class="math inline">\(\mathcal{R_k}\)</span>, called <strong>decision regions</strong>, one for each class, so that all points in it are classified to <span class="math inline">\(\mathcal{C_k}\)</span>. The boundaries between decision regions are called <strong>decision boundaries</strong> or <strong>decision surfaces</strong>. First consider the binary classification, the probability of a mistake that <span class="math inline">\(\mathbf{x}\)</span> in <span class="math inline">\(\mathcal{C_1}\)</span> assigned to <span class="math inline">\(\mathcal{C_2}\)</span> and vice versa is <span class="math display">\[
\begin{aligned}
p(\mathrm{mistake}) &amp;= p(\mathbf{x}\in \mathcal{R}_1, \mathcal{C_2}) + p(\mathbf{x}\in \mathcal{R}_2, \mathcal{C_1})
\\&amp;= \int_\mathcal{R_1} p(\mathbf{x}, \mathcal{C}_2)\ \mathrm{d}\mathbf{x} + \int_\mathcal{R_2} p(\mathbf{x}, \mathcal{C}_1)\ \mathrm{d}\mathbf{x}
\end{aligned}
\]</span> For a <em>given</em> value of <span class="math inline">\(\mathbf{x}\)</span>, to minimize <span class="math inline">\(p(\mathrm{mistake})\)</span>, obviously we should assign it to class <span class="math inline">\(\mathcal{C_1}\)</span> if <span class="math inline">\(p(\mathbf{x}, \mathcal{C}_1) &gt; p(\mathbf{x}, \mathcal{C}_2)\)</span>. And since <span class="math inline">\(p(\mathbf{x}, \mathcal{C}_k) = p(\mathcal{C}_k|\mathbf{x})p(\mathbf{x})\)</span>, where <span class="math inline">\(p(\mathbf{x})\)</span> is common for both terms, we can only compare the posterior <span class="math inline">\(p(\mathcal{C}_k|\mathbf{x})\)</span>, and the minimum <span class="math inline">\(p(\mathrm{mistake})\)</span> is achieved by assigning <span class="math inline">\(\mathbf{x}\)</span> to the class with the larger <span class="math inline">\(p(\mathcal{C}_k|\mathbf{x})\)</span>.</p>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/11235035.jpg" alt="The decision boundary is at x_0, since when \hat{x} moves towards x_0, the red error area disappears."><figcaption>The decision boundary is at <span class="math inline">\(x_0\)</span>, since when <span class="math inline">\(\hat{x}\)</span> moves towards <span class="math inline">\(x_0\)</span>, the red error area disappears.</figcaption>
</figure>
<p>For the more general case of <span class="math inline">\(K\)</span> classes, the mistake occurs when <span class="math inline">\(\mathbf{x}\)</span> in class <span class="math inline">\(\mathcal{C_k}\)</span> is assigned to <span class="math inline">\(\mathcal{C_j}\)</span> <span class="math display">\[
p(\mathrm{mistake}) = \sum_j \sum_{k\neq j}\int_\mathcal{R_j} p(\mathbf{x}, \mathcal{C}_k)\ \mathrm{d}\mathbf{x}
\]</span> It is equivalent and more clear to maximize the probability of being correct <span class="math display">\[
p(\mathrm{correct}) = 1-p(\mathrm{mistake}) =\sum_k \int_\mathcal{R_k} p(\mathbf{x}, \mathcal{C}_k)\ \mathrm{d}\mathbf{x}
\]</span> Again if aim to minimizing the misclassification rate only, each single <span class="math inline">\(\mathbf{x}\)</span> should be assigned to the class having the maximum posterior <span class="math inline">\(p(\mathcal{C}_k|\mathbf{x})\)</span>.</p>
<h2 id="minimizing-the-expected-loss">Minimizing the Expected Loss</h2>
<p>Usually some misclassification like diagnosing a patient with cancer as healthy is much more severe than the opposite case and should be punished more. Here we can introduce a <strong>loss matrix</strong> like <span class="math display">\[
\begin{array}{@{}r@{}c@{}c@{}c@{}c@{}l@{}}
    &amp; \mathrm{cancer} &amp; \mathrm{normal}  \\
    \left.\begin{array}
    {c} \mathrm{cancer} \\ \mathrm{normal}  \end{array}\right(
                    &amp; \begin{array}{c} 0 \\ 1  \end{array}
                    &amp; \begin{array}{c} 1000 \\ 0 \end{array}
                          &amp; \left)\begin{array}{c} \\  \\ \end{array}\right.
  \end{array}
\]</span> Here 1000 denotes a man with cancer diagnosed as normal will be punished 1000 times than a normal man misdiagnosed.</p>
<p>Again the mistake occurs when <span class="math inline">\(\mathbf{x}\)</span> in class <span class="math inline">\(\mathcal{C_k}\)</span> is assigned to <span class="math inline">\(\mathcal{C_j}\)</span>, and the expected loss is given by <span class="math display">\[
\mathbb{E}[L]= \sum_j \sum_{k}\int_\mathcal{R_j} L_{kj}p(\mathbf{x}, \mathcal{C}_k)\ \mathrm{d}\mathbf{x}
\]</span> Consider for a single data <span class="math inline">\(\mathbf{x}\)</span> and choose the region <span class="math inline">\(\mathcal{R_j}\)</span> it belongs to, we should minimize <span class="math inline">\(\sum_k L_{kj}p(\mathbf{x}, \mathcal{C}_k)\)</span>, and as before it is equivalent to minimize <span class="math inline">\(\sum_k L_{kj}p( \mathcal{C}_k|\mathbf{x})\)</span> w.r.t. <span class="math inline">\(j\)</span>. For example, if a patient with diagnosis <span class="math inline">\(\mathbf{x}\)</span> has <span class="math inline">\(0.1\)</span> probability having cancer, i.e. <span class="math inline">\(p( \mathcal{C}_\mathrm{cancer}|\mathbf{x})=0.1\)</span> and <span class="math inline">\(p( \mathcal{C}_\mathrm{normal}|\mathbf{x})=0.9\)</span>, then the expected error at this specific point <span class="math inline">\(\mathbf{x}\)</span> is <span class="math inline">\(0*0.1+1*0.9 = 0.9\)</span> if we classify it as having cancer and <span class="math inline">\(1000*0.1+0*0.9=100\)</span> if we classify it as normal.</p>
<h2 id="the-rejection-opinion">The Rejection Opinion</h2>
<p>Usually it will be appropriate to <em>avoid making decision</em> if we are uncertain about class membership, by introducing a threshold <span class="math inline">\(\theta\)</span> to reject those inputs <span class="math inline">\(\mathbf{x}\)</span> where <span class="math inline">\(\max_k p( \mathcal{C}_k|\mathbf{x}) &lt; \theta\)</span>.</p>
<p><img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/80196176.jpg"></p>
<h2 id="inference-and-decision">Inference and Decision</h2>
<p>Now we have broken the classification problem down into two separate stages</p>
<ul>
<li><strong>Inference</strong> stage: learning a model for <span class="math inline">\(p(\mathcal{}C_k|\mathbf{x})\)</span> from a set of training data</li>
<li><strong>Decision</strong> stage: making optimal class assignments for new data</li>
</ul>
<p>Alternatively we can combine the two stages and simply learn a discriminant function that maps inputs <span class="math inline">\(\mathbf{x}\)</span> directly into decisions.</p>
<p>In fact, there are three approaches to solve decision problem, in decreasing order of complexity they are</p>
<ol type="1">
<li><p><strong>Generative model</strong>: explicitly or implicitly model the distribution of inputs as well as outputs, by sampling from it is possible to generate synthetic data points in the input space. It first solve the inference problem by determining <span class="math inline">\(p(\mathbf{x}|\mathcal{C_k})\)</span> and <span class="math inline">\(p(\mathcal{C_k})\)</span>, then we can infer the posterior and make decision from <span class="math display">\[
p(\mathcal{C_k}|\mathbf{x}) = \frac{p(\mathbf{x}|\mathcal{C_k})p(\mathcal{C_k})}{p(\mathbf{x})} = \frac{p(\mathbf{x}|\mathcal{C_k})p(\mathcal{C_k})}{\sum_kp(\mathbf{x}|\mathcal{C_k})p(\mathcal{C_k})}
\]</span></p></li>
<li><strong>Discriminative model</strong>: solve the inference problem directly from learning the posterior <span class="math inline">\(p(\mathcal{C_k}|\mathbf{x})\)</span> and then make decision.</li>
<li><p><strong>Discriminative function</strong>: combine both the inference and decision stage, learn a function that maps each input directly onto a class label.</p></li>
</ol>
<p><em>G model VS D model:</em> Generative model is the most demanding and often need large data to ensure accuracy, but is useful for detecting new data with low probability, known as <strong>outlier detection</strong> or <strong>novelty detection</strong>. If we only need to make decisions for classification, we only need the <span class="math inline">\(p(\mathcal{C_k}|\mathbf{x})\)</span> in discriminative model as shown below.</p>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/76982210.jpg" alt="The class-conditional density (or likelihood function) has no effect on posterior probabilities."><figcaption>The class-conditional density (or likelihood function) has no effect on posterior probabilities.</figcaption>
</figure>
<p><em>Drawbacks of only D function:</em> There are many reasons for wanting to compute the posterior, which can not be accessed in a single discriminative function</p>
<ul>
<li><em>Minimizing risk</em>: no need to retrain the model if we only modify the loss matrix in decision stage.</li>
<li><em>Reject option</em>: depends on posterior.</li>
<li><em>Compensating for class prior</em>: for better generalization, compute posterior on modified data set and then compensate the effects of modification. (Details shown below)</li>
<li><em>Combining models</em>: combine posterior for small problems which is tackle by a separate model. (Details shown below)</li>
</ul>
<blockquote>
<p>Details of "compensating for class prior": For example, cancer is rare and there may be only 1 in every 1000 training examples corresponds to cancer, our model can achieve 99.9% accuracy by simply assigning every point to the normal class, which is lack of generalization. One approach is to artificially select equal number of examples from both classes, but we have to compensate for this modification. The compensated posterior takes the form <span class="math display">\[
\begin{aligned}
p(\mathcal{C_\mathrm{cancer}}|\mathbf{x}) &amp;= \frac{p(\mathbf{x}|\mathcal{C_\mathrm{cancer}})p(\mathcal{C_\mathrm{cancer}})}{p(\mathbf{x})} 
\\&amp;= \frac{p(\mathbf{x}|\mathcal{C_\mathrm{cancer}})\hat{p}(\mathcal{C_\mathrm{cancer}})}{p(\mathbf{x})} \frac{p(\mathcal{C_\mathrm{cancer}})}{\hat{p}(\mathcal{C_\mathrm{cancer}})} 
\\&amp;= \hat{p}(\mathcal{C_\mathrm{cancer}}|\mathbf{x}) \frac{p(\mathcal{C_\mathrm{cancer}})}{\hat{p}(\mathcal{C_\mathrm{cancer}})}
\end{aligned}
\]</span> where <span class="math inline">\(\hat{p}(\mathcal{C_\mathrm{cancer}}|\mathbf{x})\)</span> is the posterior generated in balanced data set, and both the priors <span class="math inline">\(p(\mathcal{C_\mathrm{cancer}})\)</span> and <span class="math inline">\(\hat{p}(\mathcal{C_\mathrm{cancer}})\)</span> can be interpreted as the fractions of points in each class.</p>
<p>Details of "combining models": If the input spaces, for example, not only include the X-ray images but also blood tests, but now there are models using only one kind of data. Using <strong>conditional independent property</strong> we can combine these two models <span class="math display">\[
\begin{aligned}
p(\mathcal{C_k}|\mathbf{x_I}, \mathbf{x_B}) &amp;\propto p(\mathbf{x_I}, \mathbf{x_B}|\mathcal{C_k}) p(\mathcal{C_k})
\\ &amp;= p(\mathbf{x_I}|\mathcal{C_k}) p(\mathbf{x_B}|\mathcal{C_k}) p(\mathcal{C_k})
\\ &amp;= \frac{p(\mathbf{x_I}|\mathcal{C_k}) p(\mathcal{C_k})p(\mathbf{x_B}|\mathcal{C_k}) p(\mathcal{C_k})}{p(\mathcal{C_k})}
\\ &amp;= \frac{ p(\mathcal{C_k}|\mathbf{x_I})p(\mathbf{x_I}) p(\mathcal{C_k}|\mathbf{x_B})p(\mathbf{x_B}) }{p(\mathcal{C_k})}
\\ &amp;\propto  \frac{ p(\mathcal{C_k}|\mathbf{x_I}) p(\mathcal{C_k}|\mathbf{x_B}) }{p(\mathcal{C_k})}
\end{aligned}
\]</span> And finally we should normalize the new posteriors to ensure they sum to 1.</p>
</blockquote>
<h2 id="loss-functions-for-regression">Loss Functions for Regression</h2>
<p>For regression problem, the decision can be made similarly by minimizing the expected loss <span class="math display">\[
\mathbb{E}[L] = \int\int L(t,y(\mathbf{x}))p(\mathbf{x},t) \ \mathrm{d}\mathbf{x}\ \mathrm{d}t
\]</span> A common choice for loss function in regression problem is <span class="math inline">\(L(t,y(\mathbf{x})) = [y(\mathbf{x})-t]^2\)</span>, and then minimize the <span class="math inline">\(\mathbb{E}[L]â€‹\)</span> using the calculus of variations we have <span class="math display">\[
\frac{\delta\mathbb{E}[L]}{\delta y(\mathbf{x})} = 2 \int [y(\mathbf{x})-t]p(\mathbf{x},t) \ \mathrm{d}t = 0
\]</span> Then solving <span class="math inline">\(y(\mathbf{x})\)</span> from it <span class="math display">\[
y(\mathbf{x}) = \frac{\int tp(\mathbf{x},t)\ \mathrm{d}t}{p(\mathbf{x})} = \int tp(t|\mathbf{x})\ \mathrm{d}t = \mathbb{E}[t|\mathbf{x}]
\]</span> which is the conditional average of <span class="math inline">\(t\)</span> conditioned on <span class="math inline">\(\mathbf{x}\)</span> and is known as <strong>regression function</strong>.</p>
<p>An alternative way to derive this result is using the knowledge that the optimum is the conditional expectation. This approach shed light on the nature of the regression problem. First expand the squared loss term into <span class="math display">\[
\begin{aligned}
\ [y(\mathbf{x})-t]^2 &amp;= [y(\mathbf{x})-\mathbb{E}[t|\mathbf{x}]+\mathbb{E}[t|\mathbf{x}]-t]^2
\\ &amp;= [y(\mathbf{x})-\mathbb{E}[t|\mathbf{x}]]^2+2[y(\mathbf{x})-\mathbb{E}[t|\mathbf{x}]][\mathbb{E}[t|\mathbf{x}]-t]+[\mathbb{E}[t|\mathbf{x}]-t]^2
\end{aligned}
\]</span> Substituting the into the loss function respectively we get for the first term <span class="math display">\[
\begin{aligned}
&amp;\int \int [y(\mathbf{x})-\mathbb{E}[t|\mathbf{x}]]^2p(\mathbf{x},t) \ \mathrm{d}\mathbf{x}\ \mathrm{d}t 
\\=&amp; 
\int [y(\mathbf{x})-\mathbb{E}[t|\mathbf{x}]]^2 \int p(\mathbf{x},t)\ \mathrm{d}t  \ \mathrm{d}\mathbf{x}
\\ =&amp; \int [y(\mathbf{x})-\mathbb{E}[t|\mathbf{x}]]^2 p(\mathbf{x}) \ \mathrm{d}\mathbf{x}
\end{aligned}
\]</span></p>
<p>For the second term, which is vanished <span class="math display">\[
\begin{aligned}
&amp;\int \int 2[y(\mathbf{x})-\mathbb{E}[t|\mathbf{x}]][\mathbb{E}[t|\mathbf{x}]-t]p(\mathbf{x},t) \ \mathrm{d}\mathbf{x}\ \mathrm{d}t 
\\= &amp;
\ 2\int  [y(\mathbf{x})-\mathbb{E}[t|\mathbf{x}]]p(\mathbf{x})\int[\mathbb{E}[t|\mathbf{x}]-t]p(t|\mathbf{x})\ \mathrm{d}t \ \mathrm{d}\mathbf{x}
\\ =&amp;\ 2\int  [y(\mathbf{x})-\mathbb{E}[t|\mathbf{x}]]p(\mathbf{x})*0\ \mathrm{d}\mathbf{x}
\\=&amp;\ 0
\end{aligned}
\]</span></p>
<p>And for the last term <span class="math display">\[
\begin{aligned}
&amp;\int \int [\mathbb{E}[t|\mathbf{x}]-t]^2p(\mathbf{x},t) \ \mathrm{d}\mathbf{x}\ \mathrm{d}t 
\\ =&amp;\int p(\mathbf{x})\int [\mathbb{E}[t|\mathbf{x}]-t]^2p(t|\mathbf{x})\ \mathrm{d}t  \ \mathrm{d}\mathbf{x}
%\\ =&amp; \int \int [\mathbb{E}^2[t|\mathbf{x}]-2t\mathbb{E}[t|\mathbf{x}]+t^2]p(t|\mathbf{x})p(\mathbf{x}) \ \mathrm{d}\mathbf{x}\ \mathrm{d}t 
%\\ =&amp; \int  \mathbb{E}^2[t|\mathbf{x}]p(\mathbf{x})\int p(t|\mathbf{x})\ \mathrm{d}t  \ \mathrm{d}\mathbf{x} + \int \mathbb{E}[t|\mathbf{x}]p(\mathbf{x}) \int -2tp(t|\mathbf{x})\ \mathrm{d}t \ \mathrm{d}\mathbf{x} + \int p(\mathbf{x})  \int t^2p(t|\mathbf{x})\ \mathrm{d}t \ \mathrm{d}\mathbf{x}
%\\ =&amp; \int  \mathbb{E}^2[t|\mathbf{x}]p(\mathbf{x})\ \mathrm{d}\mathbf{x} + \int -2\mathbb{E}^2[t|\mathbf{x}]p(\mathbf{x}) \ \mathrm{d}\mathbf{x} + \int \mathbb{E}[t^2|\mathbf{x}]p(\mathbf{x}) \ \mathrm{d}\mathbf{x}
%\\ =&amp;  \int \left[\mathbb{E}[t^2|\mathbf{x}]-\mathbb{E}^2[t|\mathbf{x}]\right]p(\mathbf{x}) \ \mathrm{d}\mathbf{x}
\\ =&amp;  \int \mathrm{var}[t|\mathbf{x}]p(\mathbf{x}) \ \mathrm{d}\mathbf{x}
\end{aligned}
\]</span></p>
<p>So finally we have <span class="math display">\[
\mathbb{E}[L] = \int [y(\mathbf{x})-\mathbb{E}[t|\mathbf{x}]]^2 p(\mathbf{x}) \ \mathrm{d}\mathbf{x} + \int \mathrm{var}[t|\mathbf{x}]p(\mathbf{x}) \ \mathrm{d}\mathbf{x}
\]</span> The function <span class="math inline">\(y(\mathbf{x})\)</span> we seek to determine only enters in the first term called <strong>bias</strong>, which will vanish if we choose <span class="math inline">\(y(\mathbf{x}) = \mathbb{E}[t|\mathbf{x}]\)</span>, and the second term is called <strong>noise</strong>, which represents the noise during data generation and is irreducible.</p>
<p>Similarly, there are three approaches to solve a regression problem, respectively by using:</p>
<ol type="1">
<li><p><strong>Generative model</strong>: learn a joint distribution <span class="math inline">\(p(\mathbf{x}, t)\)</span> then compute posterior <span class="math inline">\(p(t|\mathbf{x}) = p(\mathbf{x}, t)/\int p(\mathbf{x}, t)\ \mathrm{d}t\)</span>, and finally get <span class="math inline">\(y(\mathbf{x}) = \int t p(t|\mathbf{x})\ \mathrm{d}t\)</span>.</p></li>
<li><p><strong>Discriminative model</strong>: only learn the posterior <span class="math inline">\(p(t|\mathbf{x}) = p(\mathbf{x}, t)/\int p(\mathbf{x}, t)\ \mathrm{d}t\)</span>, and then get <span class="math inline">\(y(\mathbf{x}) = \int t p(t|\mathbf{x})\ \mathrm{d}t\)</span>.</p></li>
<li><p><strong>Discriminative function</strong>: Directly find <span class="math inline">\(y(\mathbf{x})\)</span> like what we did in multinomial curve fitting before.</p></li>
</ol>
<p>Sometimes like when <span class="math inline">\(p(t|\mathbf{x})\)</span> is multimodal, the square loss leads to poor results since <span class="math inline">\(t\)</span> may less likely locates at <span class="math inline">\(\mathbb{E}[t|\mathbf{x}]\)</span>. We can consider a generalization of the square loss called <strong>Minkowski loss</strong> <span class="math display">\[
\mathbb{E}[L_q]=\int\int |y(\mathbf{x})-t|^q p(\mathbf{x},t) \ \mathrm{d}\mathbf{x}\ \mathrm{d}t
\]</span> The minimum of <span class="math inline">\(\mathbb{E}[L_2]\)</span> is as before the conditional mean, and the minimum of <span class="math inline">\(\mathbb{E}[L_1]\)</span> is the conditional median, and as <span class="math inline">\(q\rightarrow 0\)</span>, the minimum of <span class="math inline">\(\mathbb{E}[L_q]\)</span> is the conditional mode.</p>

    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/">#</a></span>
    
    </div>
    
    
</article>


<div class="sharebox">
    
<div class="sharethis-inline-share-buttons"></div>
<script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5c1be99859895a00110ffa34&amp;product=inline-share-buttons" async="async"></script>

</div>



<div class="comments">
    <h3 class="title is-4">Comments</h3>
    
<script>
    var disqus_config = function () {
        this.page.url = 'http://wiljohn.top/cache/PRML1-5.html';
        this.page.identifier = 'prml1-5';
        
        this.language = 'en';
        
    };
    (function() {
        var d = document, s = d.createElement('script');  
        s.src = '//' + 'wiljohnhong-github-io' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<div id="disqus_thread">
    
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>

    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 wiljohn&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" href="https://github.com/wiljohnhong">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>


    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    


<script src="/js/script.js"></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
    
</body>
</html>