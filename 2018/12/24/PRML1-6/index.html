<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
<title>(PRML Notes) 1.6 Information Theory - Wiljohn&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">








    <meta name="description" content="A series of notes taken from Pattern Recognition and Machine Learning.  When observing a specific value of \(x\), the amount of information received can be viewed as the &quot;degree of surprise&quot;. Higher">
<meta name="keywords" content="PRML">
<meta property="og:type" content="article">
<meta property="og:title" content="(PRML Notes) 1.6 Information Theory">
<meta property="og:url" content="http://wiljohn.top/2018/12/24/PRML1-6/index.html">
<meta property="og:site_name" content="Wiljohn&#39;s Blog">
<meta property="og:description" content="A series of notes taken from Pattern Recognition and Machine Learning.  When observing a specific value of \(x\), the amount of information received can be viewed as the &quot;degree of surprise&quot;. Higher">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/71688837.jpg">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/97084488.jpg">
<meta property="og:updated_time" content="2018-12-25T04:45:33.648Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="(PRML Notes) 1.6 Information Theory">
<meta name="twitter:description" content="A series of notes taken from Pattern Recognition and Machine Learning.  When observing a specific value of \(x\), the amount of information received can be viewed as the &quot;degree of surprise&quot;. Higher">
<meta name="twitter:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/71688837.jpg">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">

<link rel="stylesheet" href="/css/style.css">

<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    

    
    
    
    
    


</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item " href="/archives">Archives</a>
            
            <a class="navbar-item " href="/tags">Tags</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/wiljohnhong">
                
                <i class="fab-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            (PRML Notes) 1.6 Information Theory
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2018-12-24T11:53:23.000Z" itemprop="datePublished">Dec 24 2018</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            14 minutes read (About 2052 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<p>When observing a specific value of <span class="math inline">\(x\)</span>, the amount of information received can be viewed as the "degree of surprise". Higher information is received when a highly improbable value of <span class="math inline">\(x\)</span> has just observed, so the measure of information will depend monotonically on <span class="math inline">\(p(x)\)</span>, we denote it as <span class="math inline">\(h(x)\)</span>.</p>
<a id="more"></a>
<h2 id="entropy-of-discrete-distribution">Entropy of Discrete Distribution</h2>
<p>For independent variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, for which <span class="math inline">\(p(x,y) = p(x)p(y)\)</span>, the information should be addictive, i.e. <span class="math inline">\(h(x,y) = h(x) + h(y)\)</span>, meaning that the information gained from observing both of them should be the sum of the information gained from each of them separately. Then it can be shown that the relationship between <span class="math inline">\(h(x)\)</span> and <span class="math inline">\(p(x)\)</span> must be logarithm, so we can define <span class="math display">\[
h(x) = -\log_2p(x)
\]</span> The basis is chosen to be 2 by convention, so that the units of <span class="math inline">\(h(x)\)</span> can be interpreted as bits used when transmitting a specific value of <span class="math inline">\(x\)</span>. Following this interpretation, the average amount of bits during one transmission is <span class="math display">\[
\mathrm{H}[x] = - \sum_x p(x)\log_2p(x)
\]</span></p>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/71688837.jpg" alt="Image of y=x\log_2 x, the maximum locates at (\frac{1}{e}, \frac{1}{e\ln 2})."><figcaption>Image of <span class="math inline">\(y=x\log_2 x\)</span>, the maximum locates at <span class="math inline">\((\frac{1}{e}, \frac{1}{e\ln 2})\)</span>.</figcaption>
</figure>
<p>This important quantity is called the <strong>entropy</strong> of the random variable <span class="math inline">\(x\)</span>. As the <strong>noiseless coding theorem</strong> stated, <em>the entropy is a lower bound on the number of bits needed to transmit the state of a random variable</em>. For example,</p>
<blockquote>
<p>Consider a sender wishes to transmit a variable <span class="math inline">\(x\)</span> having 8 possible states <span class="math inline">\(\{a,b,c,d,e,f,g,h\}\)</span>, for which the respective probabilities are given by <span class="math inline">\(\left( \frac{1}{2}, \frac{1}{4},\frac{1}{8},\frac{1}{16},\frac{1}{64},\frac{1}{64},\frac{1}{64},\frac{1}{64}\right)\)</span>, according to Haffman coding the optimal set of coding string is <span class="math inline">\(0\)</span>, <span class="math inline">\(10\)</span>, <span class="math inline">\(110\)</span>, <span class="math inline">\(1110\)</span>, <span class="math inline">\(111100\)</span>, <span class="math inline">\(111101\)</span>, <span class="math inline">\(111110\)</span>, <span class="math inline">\(111111\)</span>, hence the average length of code to be transmitted is <span class="math display">\[
\frac{1}{2} \times 1 +\frac{1}{4} \times 2 +\frac{1}{8} \times 3 +\frac{1}{16} \times 4 +4 \times \frac{1}{64} \times 6  = 2\ \mathrm{bits}
\]</span> which is the same as the entropy of this random variable <span class="math display">\[
\mathrm{H}[x] = -\frac{1}{2} \times \log_2 \frac{1}{2} -\frac{1}{4} \times \log_2 \frac{1}{4} -\frac{1}{8} \times \log_2 \frac{1}{8} -\frac{1}{16} \times \log_2 \frac{1}{16} -4\times\frac{1}{64} \times \log_2 \frac{1}{64} = 2 \ \mathrm{bits}
\]</span></p>
</blockquote>
<p>Note that the nonuniform distribution has a smaller entropy than a uniform one, which can be interpreted in terms of disorder.</p>
<blockquote>
<p>Consider sending another variable having 8 possible states but with equal possibilities, now the entropy is given by <span class="math display">\[
\mathrm{H}[x] = -8 \times \frac{1}{8} \times \log_2 \frac{1}{8} = 3 \ \mathrm{bits}
\]</span></p>
</blockquote>
<p>More generally, we switch to the use of natural logarithm in entropy in order to follow its much earlier origins in physics, and this will also provide a more convenient link with ideas in later chapters.</p>
<blockquote>
<p>To understand this view of entropy, consider a set of <span class="math inline">\(N\)</span> identical objects that are to be divided amongst <span class="math inline">\(M\)</span> bins, so that there are <span class="math inline">\(n_i\)</span> objects in the <span class="math inline">\(i^\mathrm{th}\)</span> bin. The total number of ways allocating, which is also called the <strong>multiplicity</strong>, is <span class="math display">\[
W = C_N^{n_1}C_{N-n_1}^{n_2}C_{N-n_1-n_2}^{n_3}\cdots C_{N-n_1-\cdots-n_{M-1}}^{n_M} = \frac{N!}{\prod_in_i!}
\]</span> In physics, the entropy is defined as the logarithm of the multiplicity scaled by an appropriate constant <span class="math display">\[
\mathrm{H} = \frac{1}{N}\ln W = \frac{1}{N}\ln N! -\frac{1}{N}\sum_i \ln n_i!
\]</span> Now consider the limit <span class="math inline">\(N \rightarrow \infty\)</span> with <span class="math inline">\(n_i/N\)</span> held fixed, applying Stirling approximation, <span class="math inline">\(N! \approx \sqrt{2\pi N} (N/e)^N\)</span>, we have <span class="math display">\[
\ln N! \approx N\ln N - N
\]</span> where the <span class="math inline">\(O(\ln N)\)</span> term is dropped. This gives <span class="math display">\[
\begin{aligned}
\mathrm{H} &amp;= \lim_{N\rightarrow\infty} \ln N -1 -\frac{1}{N} \sum_i (n_i \ln n_i -n_i)
\\ &amp;= \lim_{N\rightarrow\infty}\ln N -\frac{1}{N} \sum_i (n_i \ln n_i)
\\ &amp;= \lim_{N\rightarrow\infty}\frac{1}{N}\sum_i n_i\ln N -\frac{1}{N} \sum_i (n_i \ln n_i)
\\ &amp;= -\lim_{N\rightarrow\infty}\sum_i \frac{n_i}{N} \ln \frac{n_i}{N}
\\ &amp;= -\sum_i p_i \ln p_i
\end{aligned}
\]</span> Here <span class="math inline">\(p_i = \lim_{N\rightarrow\infty}\frac{n_i}{N}\)</span> is the probability of an object being assigned to the <span class="math inline">\(i^\mathrm{th}\)</span> bin. We can interpret the bins as the states <span class="math inline">\(x_i\)</span> of a discrete random variable <span class="math inline">\(X\)</span>, where <span class="math inline">\(p(X=x_i) = p_i\)</span>, now the entropy is given in the similar form as before <span class="math display">\[
\mathrm{H}[p] =-\sum_i p(x_i) \ln p(x_i)
\]</span></p>
</blockquote>
<p>The maximum of the entropy above can be obtained by using Jensen's inequality <span class="math display">\[
\begin{aligned}
\mathrm{H}[p] &amp;=-\sum_i p(x_i) \ln p(x_i) 
\\ &amp;= \sum_i p(x_i) \ln \frac{1}{p(x_i)}
\\ &amp;\leq \ln \sum_i p(x_i) \frac{1}{p(x_i)}
\\ &amp;= \ln M
\end{aligned}
\]</span> Here <span class="math inline">\(M\)</span> is the total number of states, or bins. The maximum is achieved if and only if <span class="math inline">\(p(x_1) = p(x_2)=p(x_M)=1/M\)</span>. Intuitively speaking, the more scatter the distribution is, the larger the entropy is.</p>
<p><img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/97084488.jpg"></p>
<h2 id="entropy-of-continuous-distribution">Entropy of Continuous Distribution</h2>
<p>The definition of entropy can be extended to include distributions over continuous variables <span class="math inline">\(x\)</span> as follows. First divide <span class="math inline">\(x\)</span> into bins of width <span class="math inline">\(\Delta\)</span>, then according to mean value theorem, in the <span class="math inline">\(i^\mathrm{th}\)</span> bin there must exist a value <span class="math inline">\(x_i\)</span> such that <span class="math display">\[
\int_{i\Delta}^{(i+1)\Delta} p(x) \mathrm{d}x = p(x_i)\Delta
\]</span> Here <span class="math inline">\(p(x_i)\Delta\)</span> is exactly the probability of <span class="math inline">\(x\)</span> allocated to the <span class="math inline">\(i^\mathrm{th}\)</span> bin. So now the entropy takes the form <span class="math display">\[
\begin{aligned}
\mathrm{H}_\Delta &amp;= - \sum_i p(x_i)\Delta \ln (p(x_i)\Delta)
\\ &amp;= - \sum_i p(x_i)\Delta \ln p(x_i)- \sum_i p(x_i)\Delta \ln \Delta
\\ &amp;= - \sum_i p(x_i)\Delta \ln p(x_i)- \ln \Delta
\end{aligned}
\]</span> If we omit the second term <span class="math inline">\(-\ln\Delta\)</span> on r.h.s. and consider the limit <span class="math inline">\(\Delta \rightarrow 0\)</span>, we have <span class="math display">\[
\mathrm{H}_\Delta = - \int p(x) \ln p(x) \mathrm{d}x
\]</span> This is called the <strong>differential entropy</strong>. The omitted term <span class="math inline">\(\Delta \rightarrow 0\)</span> reflects the fact that to specify a continuous variable very precisely requires a large number of bits, and it is irrelevant to <span class="math inline">\(p(x)\)</span>, that is why we should omit it.</p>
<p>The maximum of the entropy above can be obtained under the three constraints <span class="math display">\[
\begin{aligned}
\int_{-\infty}^{\infty} p(x) \mathrm{d}x\ &amp;=\ 1 \\
\int_{-\infty}^{\infty} xp(x) \mathrm{d}x\ &amp;=\ \mu \\
\int_{-\infty}^{\infty} (x-\mu)^2p(x) \mathrm{d}x\ &amp;=\ \sigma^2
\end{aligned}
\]</span></p>
<p>Using Lagrange multiplier we get <span class="math display">\[
-\int_{-\infty}^{\infty} p(x)\ln p(x) \mathrm{d}x+
\lambda_1 \left(\int_{-\infty}^{\infty} p(x) \mathrm{d}x-1\right) +
\lambda_2\left(\int_{-\infty}^{\infty} xp(x) \mathrm{d}x - \mu \right) +
\lambda_3 \left(\int_{-\infty}^{\infty} (x-\mu)^2p(x) \mathrm{d}x- \sigma^2\right)
\]</span> and then maximize the functional w.r.t. <span class="math inline">\(p(x)\)</span> by setting the derivative of this functional to zero, giving <span class="math display">\[
-1 - \ln p(x) +\lambda_1 +\lambda_2 x+ \lambda_3 (x-\mu)^2 = 0
\]</span> Then <span class="math display">\[
p(x) = \exp \left\{ -1 +\lambda_1 +\lambda_2 x+ \lambda_3 (x-\mu)^2 \right\}
\]</span> Solving the last equation together with three constraints (this is somewhat difficult, I don't knowhow to solve actually...) we get <span class="math display">\[
\begin{aligned}
\lambda_1\ &amp;=\ 1-\frac{1}{2} \ln (2\pi \sigma^2) \\
\lambda_2 \ &amp;=\ 0 \\
\lambda_3\ &amp;=\ -\frac{1}{2\sigma^2}
\end{aligned}
\]</span> We see that <span class="math inline">\(p(x)\)</span> is actually Gaussian <span class="math display">\[
p(x) = \frac{1}{(2\pi\sigma^2)^{1/2}} \exp\left\{ -\frac{(x-\mu)^2}{2\sigma^2} \right\}
\]</span></p>
<blockquote>
<p>This is why Gaussian is found widely in nature, which is developing with the trend of entropy increase.</p>
</blockquote>
<p>We can see that the differential entropy, unlike the discrete entropy, can be negative. For example, the entropy of Gaussian is <span class="math display">\[
\begin{aligned}
\mathrm{H}[x] &amp;= - \int_{-\infty}^\infty p(x) \ln p(x) \mathrm{d}x \\
&amp;= - \int_{-\infty}^\infty p(x) \left( -\frac{1}{2}\ln (2\pi\sigma^2) -\frac{(x-\mu)^2}{2\sigma^2} \right)\mathrm{d}x \\
&amp;= \frac{1}{2}\ln (2\pi\sigma^2) + \frac{1}{2\sigma^2}\int_{-\infty}^\infty p(x) (x-\mu)^2 \mathrm{d}x \\
&amp;= \frac{1}{2}\ln (2\pi\sigma^2) +\frac{1}{2}
\end{aligned}
\]</span> <span class="math inline">\(\mathrm{H}[x]&lt;0\)</span> for <span class="math inline">\(\sigma^2 &lt; 1/(2\pi e)\)</span>.</p>
<blockquote>
<p>We omitted the term <span class="math inline">\(-\ln \Delta\)</span> in original entropy expression before, which is an infinite positive term.</p>
</blockquote>
<h2 id="conditional-entropy">Conditional Entropy</h2>
<p>Now consider a joint distribution <span class="math inline">\(p(x,y)\)</span>, if a value of <span class="math inline">\(x\)</span> is already known, then the additional information needed to specify the corresponding value of <span class="math inline">\(y\)</span> is given by <span class="math inline">\(-\ln p(y|x)\)</span>, thus the average additional information needed to specify <span class="math inline">\(y\)</span> is <span class="math display">\[
\mathrm{H}[y|x] = -\int\int p(y,x)\ln p(y|x)\ \mathrm{d}y\ \mathrm{d}x
\]</span> which is called the <strong>conditional entropy</strong> of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>. We can see that <span class="math display">\[
\begin{aligned}
\mathrm{H}[y,x] &amp;= -\int\int p(y,x)\ln p(y,x)\ \mathrm{d}y\ \mathrm{d}x \\
&amp;= -\int\int p(y,x)\ln p(y|x)\ \mathrm{d}y\ \mathrm{d}x\  -\int\int p(y,x)\ln p(x)\ \mathrm{d}y\ \mathrm{d}x \\
&amp;= \mathrm{H}[y|x] + \mathrm{H}[x]
\end{aligned}
\]</span> which means the information needed to describe <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is given by the sum of the information needed to describe <span class="math inline">\(x\)</span> alone plus the additional information required to specify <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>.</p>
<h2 id="kl-divergence">KL Divergence</h2>
<p>Now it's time to relate these idea to pattern recognition. Given an unknown distribution <span class="math inline">\(p(x)\)</span>, which has been approximated using distribution <span class="math inline">\(q(x)\)</span>, the <strong>KL divergence</strong>, or the <strong>relative entropy</strong> is the average addition amount of information required to transmit values of <span class="math inline">\(x\)</span> if we use <span class="math inline">\(q(x)\)</span> instead of <span class="math inline">\(p(x)\)</span> to construct a coding scheme. So under this definition we have <span class="math display">\[
\begin{aligned}
\mathrm{KL}(p|| q) &amp;=- \int p(x) \ln q(x) \mathrm{d}x - \left(- \int p(x) \ln p(x) \mathrm{d}x \right)\\\\
&amp;=- \int p(x) \ln \left\{ \frac{q(x)}{p(x)} \right\} \mathrm{d}x \\
\end{aligned}
\]</span> Note that <span class="math inline">\(\mathrm{KL}(p|| q) \neq \mathrm{KL}(q|| p)\)</span>. And since <span class="math inline">\(-\ln x\)</span> is strictly concave, using Jensen inequality we have <span class="math display">\[
\mathrm{KL}(p|| q) = - \int p(x) \ln \left\{ \frac{q(x)}{p(x)} \right\} \mathrm{d}x \geq - \ln\int p(x) \left( \frac{q(x)}{p(x)} \right)\mathrm{d}x = -\ln 1 = 0
\]</span> which shows that <span class="math inline">\(\mathrm{KL}(p|| q) \geq 0\)</span> with equality if and only if <span class="math inline">\(p(x)= q(x)\)</span>. Thus we can interpret the KL divergence as a measure of the dissimilarity of the two distributions, and we can show that minimizing the KL divergence is equivalent to maximizing the likelihood function. Suppose we try to approximate <span class="math inline">\(p(x)\)</span> using a parametric distribution <span class="math inline">\(q(x|\theta)\)</span>. Although we don't know <span class="math inline">\(p(x)\)</span> to compute KL divergence directly, we can sample from <span class="math inline">\(p(x)\)</span>, <span class="math display">\[
\begin{aligned}
\mathrm{KL}(p|| q) &amp;= - \int p(x) \ln \left\{ \frac{q(x)}{p(x)} \right\} \mathrm{d}x \\ 
&amp;\simeq - \frac{1}{N} \sum_n  \ln \left\{ \frac{q(x_n|\theta)}{p(x_n)} \right\} \\
&amp;= - \frac{1}{N} \sum_n  \ln q(x_n|\theta) + \frac{1}{N} \sum_n  \ln p(x_n)
\end{aligned}
\]</span> Then <span class="math display">\[
\mathop{\arg \min}_\theta \mathrm{KL}(p|| q) = \mathop{\arg \max}_\theta \sum_n  \ln q(x_n|\theta)
\]</span></p>
<h2 id="mutual-information">Mutual Information</h2>
<p>If the two variables of distribution <span class="math inline">\(p(x,y)​\)</span> are not independent, we can measure how "close" they are to being independent by using KL divergence <span class="math display">\[
\begin{aligned}
\mathrm{I}[x,y] &amp;= \mathrm{KL}(p(x,y)||p(x)p(y))\\
&amp;= - \int\int p(x,y) \ln \left\{ \frac{p(x)p(y)}{p(x,y)} \right\} \ \mathrm{d}x\ \mathrm{d}y
\end{aligned}
\]</span> Here <span class="math inline">\(\mathrm{I}[x,y]\)</span> is called the mutual information between the variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. We can see that <span class="math inline">\(\mathrm{I}[x,y] \geq 0\)</span> with equality iff <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent.</p>
<p>Note that the fraction in logarithm can be rewritten as <span class="math inline">\(p(x)/p(x|y)\)</span> or <span class="math inline">\(p(y)/p(y|x)\)</span>, the mutual information can be related to the conditional entropy through <span class="math display">\[
\mathrm{I}[x,y] = \mathrm{H}[x] - \mathrm{H}[x|y] = \mathrm{H}[y] - \mathrm{H}[y|x]
\]</span> From this point of view, the mutual information represents the reduction in uncertainty about <span class="math inline">\(x\)</span> as a consequence of the new observation <span class="math inline">\(y\)</span>, or vice versa.</p>

    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/PRML/">#PRML</a></span>
    
    </div>
    
    
    <div class="columns is-mobile is-multiline article-nav">
        <span class="column is-12-mobile is-half-desktop  article-nav-prev">
            
            <a href="/2019/02/27/PRML2-1/">(PRML Notes) 2.1 Binary Variables</a>
            
        </span>
        <span class="column is-12-mobile is-half-desktop  article-nav-next">
            
            <a href="/2018/12/21/ssr/">ShadowsocksR资源整理</a>
            
        </span>
    </div>
    
</article>


<div class="sharebox">
    
<div class="sharethis-inline-share-buttons"></div>
<script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5c1be99859895a00110ffa34&amp;product=inline-share-buttons" async="async"></script>

</div>



<div class="comments">
    <h3 class="title is-4">Comments</h3>
    
<script>
    var disqus_config = function () {
        this.page.url = 'http://wiljohn.top/2018/12/24/PRML1-6/';
        this.page.identifier = 'prml1-6';
        
        this.language = 'en';
        
    };
    (function() {
        var d = document, s = d.createElement('script');  
        s.src = '//' + 'wiljohnhong-github-io' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<div id="disqus_thread">
    
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>

    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2019 wiljohn&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" href="https://github.com/wiljohnhong">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>


    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    


<script src="/js/script.js"></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script>
    
</body>
</html>