<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
<title>Tag: PRML - Wiljohn&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">






    <meta property="og:type" content="website">
<meta property="og:title" content="Wiljohn&#39;s Blog">
<meta property="og:url" content="http://wiljohn.top/tags/PRML/index.html">
<meta property="og:site_name" content="Wiljohn&#39;s Blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Wiljohn&#39;s Blog">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">

<link rel="stylesheet" href="/css/style.css">

<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    

    
    
    
    
    


</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item " href="/archives">Archives</a>
            
            <a class="navbar-item " href="/tags">Tags</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/wiljohnhong">
                
                <i class="fab-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section section-heading">
    <div class="container">
        <div class="content">
            <h5>#PRML</h5>
        </div>
    </div>
</section>
<section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2019/03/15/PRML2-3-7~9/" itemprop="url">(PRML Notes) 2.3.7-9 Miscellaneous Gaussian</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-03-15T08:42:04.000Z" itemprop="datePublished">Mar 15 2019</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            11 minutes read (About 1720 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<h2 id="students-t-distribution">Student's t-distribution</h2>
<p>Recall that for a univariate Gaussian <span class="math inline">\(\mathcal{N}(x|\mu,\tau^{-1})\)</span>, its conjugate prior when <span class="math inline">\(\mu\)</span> is known and <span class="math inline">\(\tau^{-1}\)</span> is unknown is a Gamma distribution <span class="math inline">\(\mathrm{Gam}(\tau|a,b)\)</span>. <strong>Student's t-distribution</strong> is the result where we integrate out the precision of the posterior</p>
        <p class="article-more-link">
            <a href="/2019/03/15/PRML2-3-7~9/#more">Read More</a>
        </p>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2019/03/02/PRML2-3-6/" itemprop="url">(PRML Notes) 2.3.6 Bayesian Inference for Gaussian</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-03-02T08:07:04.000Z" itemprop="datePublished">Mar 2 2019</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            10 minutes read (About 1453 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<p>Now for univariate Gaussian, we use <span class="math inline">\(\mathtt{x} = \{x_1,...,x_N\}​\)</span> to denote the <span class="math inline">\(N​\)</span> observations, and <span class="math inline">\(\mathbf{X}=\{\mathbf{x}_1,...,\mathbf{x}_N\}​\)</span> for the multivariate Gaussian. For convenience, the likelihood functions of the observed data given mean and variance are written below <span class="math display">\[
p(\mathtt{x}|\mu,\sigma^2) = \frac{1}{(2\pi\sigma^2)^{N/2}}\exp\left\{ -\sum_{n=1}^N\frac{(x_n-\mu)^2}{2\sigma^2} \right\}
\]</span></p>
<p><span class="math display">\[
p(\mathbf{X}| \boldsymbol{\mu},\mathbf{\Sigma}) = \frac{1}{(2\pi)^{ND/2}} \frac{1}{|\mathbf{\Sigma}|^{N/2}} \exp\left\{ -\frac{1}{2} \sum_{n=1}^{N}(\mathbf{x}_n- \boldsymbol{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}_n- \boldsymbol{\mu}) \right\}
\]</span></p>
<p>In this section a Bayesian treatment that mainly focuses on univariate case by introducing prior will be developed.</p>
        <p class="article-more-link">
            <a href="/2019/03/02/PRML2-3-6/#more">Read More</a>
        </p>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2019/03/02/PRML2-3-4~5/" itemprop="url">(PRML Notes) 2.3.4-5 Frequentist Estimate for Gaussian</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-03-02T06:20:04.000Z" itemprop="datePublished">Mar 2 2019</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            11 minutes read (About 1576 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<h2 id="maximum-likelihood-for-gaussian">Maximum Likelihood for Gaussian</h2>
<p>Given a dataset <span class="math inline">\(\mathbf{X} = (\mathbf{x}_1,...,\mathbf{N})^T\)</span> where <span class="math inline">\(\mathbf{x_n}\)</span> are drawn independently from a multivariate Gaussian. To derive MLE, the log likelihood of the dataset is given by <span class="math display">\[
\begin{align}
\ln p(\mathbf{X}|\boldsymbol{\mu},\mathbf{\Sigma}) &amp;= \ln \prod_{n=1}^{N}\frac{1}{(2\pi)^{D/2}} \frac{1}{|\mathbf{\Sigma}|^{1/2}} \exp\left\{ -\frac{1}{2} (\mathbf{x}_n- \boldsymbol{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}_n- \boldsymbol{\mu}) \right\}
\\ &amp;= -\frac{ND}{2}\ln (2\pi) - \frac{N}{2}\ln|\mathbf{\Sigma}|  - \frac{1}{2}\sum_{n=1}^N (\mathbf{x}_n- \boldsymbol{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}_n- \boldsymbol{\mu})
\end{align}
\]</span></p>
        <p class="article-more-link">
            <a href="/2019/03/02/PRML2-3-4~5/#more">Read More</a>
        </p>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2019/03/01/PRML2-3-1~3/" itemprop="url">(PRML Notes) 2.3.1-3 Conditional and Marginal Gaussian</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-03-01T07:29:04.000Z" itemprop="datePublished">Mar 1 2019</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            11 minutes read (About 1660 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<p>If two sets of variables are jointly Gaussian, then the conditional distribution of one conditioned on the other is again Gaussian, and the marginal of either set is also Gaussian.</p>
<h2 id="conditional-gaussian">Conditional Gaussian</h2>
<p>If we partition <span class="math inline">\(\mathbf{x}\)</span> into two disjoint subsets <span class="math inline">\(\mathbf{x}_a\)</span> and <span class="math inline">\(\mathbf{x}_b\)</span>, w.l.o.g. we can take <span class="math inline">\(\mathbf{x}_a\)</span> to be the first <span class="math inline">\(M\)</span> components of <span class="math inline">\(\mathbf{x}\)</span>, then correspondingly we have <span class="math display">\[
\mathbf{x} = \left(\matrix{\mathbf{x}_a\\\mathbf{x}_b}\right) \ \ \boldsymbol{\mu} = \left(\matrix{\boldsymbol{\mu}_a\\\boldsymbol{\mu}_b}\right) \ \ \mathbf{\Sigma} = \left(\matrix{\mathbf{\Sigma}_{aa}&amp;\mathbf{\Sigma}_{ab}\\\mathbf{\Sigma}_{ba} &amp; \mathbf{\Sigma}_{bb}}\right) \ \ \mathbf{\Lambda} = \left(\matrix{\mathbf{\Lambda}_{aa}&amp;\mathbf{\Lambda}_{ab}\\\mathbf{\Lambda}_{ba} &amp; \mathbf{\Lambda}_{bb}}\right)
\]</span></p>
        <p class="article-more-link">
            <a href="/2019/03/01/PRML2-3-1~3/#more">Read More</a>
        </p>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2019/02/28/PRML2-3-0/" itemprop="url">(PRML Notes) 2.3.0 Gaussian Distribution</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-02-28T12:36:04.000Z" itemprop="datePublished">Feb 28 2019</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            12 minutes read (About 1813 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>In the case of single variable <span class="math inline">\(x​\)</span>, the Gaussian distribution can be written in the form <span class="math display">\[
\mathcal{N}(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{(x-\mu)^2}{2\sigma^2} \right\}
\]</span> with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. For a <span class="math inline">\(D\)</span>-dimensional vector <span class="math inline">\(\mathbf{x}\)</span>, the multivariate Gaussian distribution takes the form <span class="math display">\[
\mathcal{N}(\mathbf{x}| \boldsymbol{\mu},\mathbf{\Sigma}) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\mathbf{\Sigma}|^{1/2}} \exp\left\{ -\frac{1}{2} (\mathbf{x}- \boldsymbol{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}- \boldsymbol{\mu}) \right\}
\]</span></p>
        <p class="article-more-link">
            <a href="/2019/02/28/PRML2-3-0/#more">Read More</a>
        </p>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2019/02/28/PRML2-2/" itemprop="url">(PRML Notes) 2.2 Multinomial Variables</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-02-28T06:46:04.000Z" itemprop="datePublished">Feb 28 2019</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            4 minutes read (About 573 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<h2 id="multivariate-bernoulli-distribution">Multivariate Bernoulli Distribution</h2>
<p>For a random multivariate variable <span class="math inline">\(\mathbf{x}\)</span> that, for example, describes the <span class="math inline">\(K=6\)</span> possible outcomes of rolling a damaged dice, each outcome is presented in an one-hot vector like <span class="math inline">\(\mathbf{x}=(0,0,1,0,0,0)^T\)</span>. If we denote the probability of <span class="math inline">\(x_k=1\)</span> by <span class="math inline">\(u_k\)</span>, then the distribution of <span class="math inline">\(\mathbf{x}\)</span> is given by <span class="math display">\[
p(\mathbf{x}|\boldsymbol{\mu}) = \prod_{k=1}^K \mu_k^{x_k}
\]</span> where <span class="math inline">\(\boldsymbol{\mu}=(\mu_1, ...,\mu_K)^T​\)</span> satisfying <span class="math inline">\(\sum_k \mu_k =1​\)</span>. It can be easily verified that <span class="math display">\[
\mathbb{E}[\mathbf{x}|\boldsymbol{\mu}] = \sum_\mathbf{x} p(\mathbf{x}|\boldsymbol{\mu})\mathbf{x} =(\mu_1, ...,\mu_K)^T=\boldsymbol{\mu}
\]</span></p>
        <p class="article-more-link">
            <a href="/2019/02/28/PRML2-2/#more">Read More</a>
        </p>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2019/02/27/PRML2-1/" itemprop="url">(PRML Notes) 2.1 Binary Variables</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2019-02-27T13:04:04.000Z" itemprop="datePublished">Feb 27 2019</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            9 minutes read (About 1345 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<h2 id="bernoulli-distribution">Bernoulli Distribution</h2>
<p>For a random variable <span class="math inline">\(x\)</span> that, for example, describes the outcome of flipping a damaged coin, with the probability of landing heads <span class="math inline">\(p(x=1|\mu)=\mu\)</span>, and landing tails <span class="math inline">\(p(x=0|\mu)=1-\mu\)</span>. The <strong>Bernoulli distribution</strong> is the probability over such <span class="math inline">\(x\)</span> that can be written in the form</p>
<p><span class="math display">\[
\mathrm{Bern}(x|\mu) = \mu^x(1-\mu)^{1-x}
\]</span></p>
<p>whose mean and variance are given by <span class="math display">\[
\begin{aligned}
\mathbb{E}[x] &amp;= \mu \\
\mathrm{var}[x] &amp;= \mu(1-\mu)
\end{aligned}
\]</span></p>
        <p class="article-more-link">
            <a href="/2019/02/27/PRML2-1/#more">Read More</a>
        </p>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/12/24/PRML1-6/" itemprop="url">(PRML Notes) 1.6 Information Theory</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2018-12-24T11:53:23.000Z" itemprop="datePublished">Dec 24 2018</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            14 minutes read (About 2052 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<p>When observing a specific value of <span class="math inline">\(x\)</span>, the amount of information received can be viewed as the "degree of surprise". Higher information is received when a highly improbable value of <span class="math inline">\(x\)</span> has just observed, so the measure of information will depend monotonically on <span class="math inline">\(p(x)\)</span>, we denote it as <span class="math inline">\(h(x)\)</span>.</p>
        <p class="article-more-link">
            <a href="/2018/12/24/PRML1-6/#more">Read More</a>
        </p>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/04/30/PRML1-5/" itemprop="url">(PRML Notes) 1.5 Decision Theory</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2018-04-30T10:38:59.000Z" itemprop="datePublished">Apr 30 2018</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            16 minutes read (About 2382 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<p>Decision theory allows us to make optimal decisions in situations involving uncertainty.</p>
<p>Informally, for classification problem, we are interested in the posterior probability given data which belongs to a class <span class="math inline">\(\mathcal{C}_k\)</span> <span class="math display">\[
p(\mathcal{C}_k|\mathbf{x}) = \frac{p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\mathbf{x})}
\]</span> Our aim is to minimize the chance of assigning <span class="math inline">\(\mathbf{x}\)</span> to the wrong class, then intuitively we would choose the class having the higher posterior probability. Actually the intuition is correct, which is shown below.</p>
        <p class="article-more-link">
            <a href="/2018/04/30/PRML1-5/#more">Read More</a>
        </p>
    
    </div>
    
    
</article>




    
        <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/2018/04/29/PRML1-4/" itemprop="url">(PRML Notes) 1.4 The Curse of Dimensionality</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2018-04-29T07:18:34.000Z" itemprop="datePublished">Apr 29 2018</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            2 minutes read (About 331 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>A series of notes taken from <em>Pattern Recognition and Machine Learning</em>.</p>
</blockquote>
<p>Consider a naive "cell" solution for classification, which divides the input space into many cells and each test point is assigned to the class that has a majority number of representatives in the same cell.</p>
<p><img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/18-12-24/49913951.jpg"></p>
<p>One problem is we need to gather exponentially large quantity of data as the number of cells grows exponentially in higher dimensions in order to ensure that the cells are not empty.</p>
        <p class="article-more-link">
            <a href="/2018/04/29/PRML1-4/#more">Read More</a>
        </p>
    
    </div>
    
    
</article>




    
    
        
<nav class="pagination is-centered is-rounded" role="navigation" aria-label="pagination">
    <div class="pagination-previous is-invisible is-hidden-mobile">
        <a href="/tags/PRML/page/0/">Prev</a>
    </div>
    <div class="pagination-next">
        <a href="/tags/PRML/page/2/">Next</a>
    </div>
    <ul class="pagination-list is-hidden-mobile">
        
        <li><a class="pagination-link is-current" href="/tags/PRML/">1</a></li>
        
        <li><a class="pagination-link" href="/tags/PRML/page/2/">2</a></li>
        
    </ul>
</nav>
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2019 wiljohn&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" href="https://github.com/wiljohnhong">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {matchFontHeight: false},
        SVG: {matchFontHeight: false},
        CommonHTML: {matchFontHeight: false}
    });
</script>


    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
      })
    </script>

    


<script src="/js/script.js"></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script>
    
</body>
</html>