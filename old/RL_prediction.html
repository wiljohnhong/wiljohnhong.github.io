<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
<title>Model-Free Prediction - Wiljohn&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">








    <meta name="description" content="In this and next notes, different from the DP, we do all the things within an unknown MDP, just what the model-free means.  Monte-Carlo Learning The basic idea is very simple, it uses the mean return">
<meta name="keywords" content="RL">
<meta property="og:type" content="website">
<meta property="og:title" content="Model-Free Prediction">
<meta property="og:url" content="http://wiljohn.top/old/RL_prediction.html">
<meta property="og:site_name" content="Wiljohn&#39;s Blog">
<meta property="og:description" content="In this and next notes, different from the DP, we do all the things within an unknown MDP, just what the model-free means.  Monte-Carlo Learning The basic idea is very simple, it uses the mean return">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-1.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-2.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-3.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-4.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-ab.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-5.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-6.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-7.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-8.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-9.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-10.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-11.png">
<meta property="og:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-12.png">
<meta property="og:updated_time" content="2020-10-24T15:22:50.865Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Model-Free Prediction">
<meta name="twitter:description" content="In this and next notes, different from the DP, we do all the things within an unknown MDP, just what the model-free means.  Monte-Carlo Learning The basic idea is very simple, it uses the mean return">
<meta name="twitter:image" content="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-1.png">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">

<link rel="stylesheet" href="/css/style.css">

<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


    
    

    
    
    
    
    


</head>
<body>
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item " href="/archives">Archives</a>
            
            <a class="navbar-item " href="/tags">Tags</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/wiljohnhong">
                
                <i class="fab-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope="" itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            Model-Free Prediction
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2018-10-01T12:10:22.000Z" itemprop="datePublished">Oct 1 2018</time>
        </span>
        
        
        <span class="column is-narrow">
            
            
            11 minutes read (About 1699 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <blockquote>
<p>In this and next notes, different from the DP, we do all the things within an <strong>unknown MDP</strong>, just what the <em>model-free</em> means.</p>
</blockquote>
<h2 id="monte-carlo-learning">Monte-Carlo Learning</h2>
<p>The basic idea is very simple, it uses the mean return of episodes as the true value.</p>
<ul>
<li>Learn from complete <strong>episodes</strong>, which refers to the procedure from current state to the final state. (The opposite learning method is <strong>bootstrapping</strong>, which means updating the current state value from incomplete episodes.)</li>
<li>Restrict to the learning method, MC can only be applied to <em>episodic</em> MDPs, whose all episodes must terminates.</li>
<li>MC is <strong>model-free</strong>, it has no knowledge of MDP transitions or rewards.</li>
</ul>
<a id="more"></a>
<p>In detail, MC's goal is to learn <span class="math inline">\(v_\pi\)</span> from episodes of experience under some policy <span class="math inline">\(\pi\)</span>, like <span class="math inline">\(S_1, A_1,R_2, ...,S_k \sim \pi\)</span>, and using <em>empirical mean-return</em> instead of <em>expected-return</em>: <span class="math display">\[
v_\pi(s) = \mathbb{E}_\pi [G_t|S=s]
\\ \text{estimated}~v_\pi(s) = \frac{1}{N(s)}\sum_{i=1}^{N(s)} (G_{t})_i
\]</span> where <span class="math inline">\(G_t = R_{t+1}+\gamma{R}_{t+2}+…\gamma^{T-1}R_T\)</span> is the return of the whole episode.</p>
<h3 id="basic-algorithm">Basic Algorithm</h3>
<p>There are two slightly different strategies to update <span class="math inline">\(v_\pi(s)\)</span>. Since within a single episode, one state may be visited more than one time, the strategies differ in whether to regard only the first visiting time as a beginning of an episode. In detail, one strategy, the first-visit Monte-Carlo policy evaluation for some state <span class="math inline">\(s\)</span> is</p>
<ol type="1">
<li>For the <strong>FIRST</strong> time-step <span class="math inline">\(t​\)</span> that state <span class="math inline">\(s​\)</span> is visited in an episode</li>
<li>Increment counter <span class="math inline">\(N(s) \leftarrow N(s)+1​\)</span> and total return <span class="math inline">\(S(s) \leftarrow S(s)+G_t​\)</span></li>
<li>Value is evaluated by averaging over the total return <span class="math inline">\(V(s) \leftarrow S(s)/N(s) ​\)</span></li>
<li>Repeat steps 1-3, and by <em>law of large numbers</em>, <span class="math inline">\(\mathop{\lim}_{N(s)\rightarrow \infty}V(s)=v_\pi(s)\)</span></li>
</ol>
<p>The other strategy, every-visit Monte-Carlo policy evaluation sum up the return of <strong>EVERY</strong> time-step <span class="math inline">\(t​\)</span> that state <span class="math inline">\(s​\)</span> is visited into total return <span class="math inline">\(V(s)​\)</span>.</p>
<h3 id="example-blackjack">Example: Blackjack</h3>
<ul>
<li><span class="math inline">\(\mathcal{S}​\)</span> : <span class="math inline">\(10\times 10\times 10=200​\)</span> in total
<ul>
<li>current sum (12-21), if <span class="math inline">\(sum &lt; 12​\)</span> just automatically twist</li>
<li>Dealer's showing card (ace-10)</li>
<li>whether have a "usable" ace (yes-no)</li>
</ul></li>
<li><span class="math inline">\(\mathcal{A}\)</span> : 2 kinds of actions
<ul>
<li>stick -- stop receiving cards</li>
<li>twist -- take another card</li>
</ul></li>
<li><span class="math inline">\(\mathcal{R}​\)</span> and <span class="math inline">\(\mathcal{P}​\)</span>:
<ul>
<li>if stick, game terminates, <span class="math inline">\(R=+1,0,1\)</span> for sum of cards <span class="math inline">\(&gt;,=,&lt;\)</span> dealer's cards, respectively</li>
<li>if twist, <span class="math inline">\(R=-1\)</span> when cards <span class="math inline">\(&gt; 21\)</span> and game terminates, <span class="math inline">\(R=0\)</span> otherwise.</li>
</ul></li>
<li><span class="math inline">\(\mathcal{\gamma}\)</span> : <span class="math inline">\(0.9\)</span></li>
</ul>
<p>For a very simple policy that stick if sum of cards <span class="math inline">\(&gt;20​\)</span> (o.w. twist), the value function after MC learning is shown below</p>
<p><img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-1.png"></p>
<h3 id="incremental-algorithm">Incremental Algorithm</h3>
<p>Since the mean <span class="math inline">\(\mu_1, \mu_2, ...\)</span> of a sequence <span class="math inline">\(x_1, x_2,...\)</span> can be computed incrementally, called <strong>incremental mean</strong> <span class="math display">\[
\begin{align}
\mu_k &amp;= \frac{1}{k} \sum_{j=1}^k x_j
\\ &amp;= \frac{1}{k}\left(x_k+ \sum_{j=1}^{k-1} x_j\right)
\\ &amp;= \frac{1}{k}(x_k+ (k-1)\mu_{k-1} )
\\ &amp;= \mu_{k-1} + \frac{1}{k}(x_k - \mu_{k-1} )
\end{align}
\]</span> Following this idea, we have a new update method. After one episode <span class="math inline">\(S_1, A_1, R_2,...,S_T\)</span>, for <strong>EVERY</strong> visited state <span class="math inline">\(S_t\)</span> with return <span class="math inline">\(G_t\)</span></p>
<ol type="1">
<li><span class="math inline">\(N(S_t) \leftarrow N(S_t)+1\)</span></li>
<li><span class="math inline">\(V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)}(G_t-V(S_t))\)</span></li>
</ol>
<p>Now here comes our final idea, just like momentum is a modified version of SGD, this method keep tracking a <strong>running mean</strong>, by introducing a learning rate <span class="math inline">\(\alpha\)</span> to replace the term <span class="math inline">\({1}/{N(S_t)}\)</span>, so it forgets too old episodes, useful in non-stationary problems <span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))
\]</span></p>
<h2 id="time-difference-learning">Time-Difference Learning</h2>
<p>TD only differs from MC only in learning from <strong>incomplete episodes, by bootstrapping</strong>. Compare</p>
<ul>
<li>Incremental every-visit MC: <span class="math inline">\(V(S_t) \leftarrow V(S_t) + \alpha(\mathbf{G_t}-V(S_t))\)</span></li>
<li>Simplest TD algorithm, TD(0): <span class="math display">\[V(S_t) \leftarrow V(S_t) + \alpha(\mathbf{R_{t+1} + \gamma V(S_{t+1})}-V(S_t))\]</span></li>
</ul>
<p>Here the estimated (not actual as in MC) return <span class="math display">\[R_{t+1} + \gamma V(S_{t+1})\]</span> is call the <strong><em>TD target</em></strong>, <span class="math display">\[\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\]</span> is called the <strong><em>TD error</em></strong>.</p>
<p>Now Let's go through some examples to intuitively feel about the difference between the TD and MC.</p>
<h3 id="example-driving-home">Example: Driving Home</h3>
<table>
<thead>
<tr class="header">
<th>State</th>
<th style="text-align: center;">Elapsed Time (min)</th>
<th style="text-align: center;">Predicted Time to go</th>
<th style="text-align: center;">Predicted Total Time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>leaving office</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
</tr>
<tr class="even">
<td>reach car, raining</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">40</td>
</tr>
<tr class="odd">
<td>exit highway</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">35</td>
</tr>
<tr class="even">
<td>behind truck</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">40</td>
</tr>
<tr class="odd">
<td>home street</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">43</td>
</tr>
<tr class="even">
<td>arrive home</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">43</td>
</tr>
</tbody>
</table>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-2.png" alt="MC vs. TD"><figcaption>MC vs. TD</figcaption>
</figure>
<p>We can see from the first example that</p>
<p><strong>Advantages and Disadvantages of MC vs. TD -- 1</strong></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>MC</th>
<th>TD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Learn before knowing final outcome</strong></td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>Furthermore, <strong>Learn without final outcome</strong></td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<h3 id="example-random-walk">Example: Random Walk</h3>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-3.png" alt="Start from C, choose each direction with equal possibility, without discounting"><figcaption>Start from C, choose each direction with equal possibility, without discounting</figcaption>
</figure>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-4.png" alt="Training error between MC and TD with different learning rate."><figcaption>Training error between MC and TD with different learning rate.</figcaption>
</figure>
<p>(Here are some results without any proof.)</p>
<p><strong>Advantages and Disadvantages of MC vs. TD -- 2</strong></p>
<table style="width:100%;">
<colgroup>
<col style="width: 17%">
<col style="width: 41%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"></th>
<th style="text-align: left;">MC</th>
<th>TD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;"><strong>Variance</strong></td>
<td style="text-align: left;">high variance</td>
<td>low variance</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Bias</strong></td>
<td style="text-align: left;">zero bias</td>
<td>some bias</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Initial value</strong></td>
<td style="text-align: left;">not very sensitive to initial value</td>
<td>sensitive to initial value</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Efficiency</strong></td>
<td style="text-align: left;">simple but time consuming</td>
<td>more efficient than MC</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Convergence</strong></td>
<td style="text-align: left;">good approximation properties (even with function approximation)</td>
<td>only TD(0) converges to <span class="math inline">\(v_\pi(s)\)</span> without bias <strong>(?)</strong> (but not always with function approximation under some special cases)</td>
</tr>
</tbody>
</table>
<h3 id="example-ab">Example: AB</h3>
<p>From the above we know that <span class="math inline">\(V(s)\rightarrow v_\pi(s)\)</span> as experience <span class="math inline">\(\rightarrow \infty\)</span>, but what about we only obtain finite experience, namely, only some episodes (say <span class="math inline">\(K\)</span> episodes) we have experienced, like <span class="math display">\[
s_1^1,a_1^1,r_2^1,...,s_{T_1}^1
\\ \vdots
\\ s_1^K,a_1^K,r_2^K,...,s_{T_K}^K
\]</span> One possible method is to <em>repeatedly</em> sample the <span class="math inline">\(K\)</span> episodes, feed them to find out the value function via MC or TD(0). Consider the situation that we experienced only two states A and B within 8 episodes, what value will we finally get for each state without discounting?</p>
<p><img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-ab.png"></p>
<p>By MC, we will get <span class="math inline">\(V (A) = 0\)</span>, since MC converges to solution with <strong>minimum mean-squared error</strong>, in other words, MC solution best fit to the observed returns <span class="math display">\[
\sum_{k=1}^{K}\sum_{t=1}^{T_k}(G_t^k-V(s_t^k))^2
\]</span> By TD(0), we will get <span class="math inline">\(V (A) = 3/4\)</span>, since TD(0) converges to solution of <strong>max likelihood Markov model</strong>, in other words, TD(0) finds out an MDP that best fits the data <span class="math display">\[
\hat{\mathcal{P}}^a_{ss&#39;}=\frac{1}{N(s,a)} \sum_{k=1}^{K}\sum_{t=1}^{T_k}\mathbf{1}(s_t^k,a_t^k,s_{t+1}^k=s,a,s&#39;)
\\
\hat{\mathcal{R}}^a_{s}=\frac{1}{N(s,a)} \sum_{k=1}^{K}\sum_{t=1}^{T_k}\mathbf{1}(s_t^k,a_t^k=s,a)r_t^k
\]</span> <strong>Advantages and Disadvantages of MC vs. TD -- 3</strong></p>
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 37%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"></th>
<th>MC</th>
<th>TD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;"><strong>Exploit Markov property</strong></td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="even">
<td style="text-align: right;"></td>
<td>Usually more <em>efficient</em> in Markov environments</td>
<td>Usually more <em>effective</em> in non-Markov environments</td>
</tr>
</tbody>
</table>
<h3 id="unified-view-of-dp-mc-and-td">Unified View of DP, MC and TD</h3>
<p>MC backup: <span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha (G_t-V(S_t))
\]</span> <img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-5.png"></p>
<p>TD backup: <span class="math display">\[
V(S_t) \leftarrow V(S_t)+ \alpha(R_{t+1} + \gamma V(S_{t+1}-V(S_t)))
\]</span> <img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-6.png"></p>
<p>DP backup: <span class="math display">\[
V(S_t) \leftarrow \mathbb{E}_\pi[R_{t+1}+\gamma V(S_{t+1})]
\]</span> <img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-7.png"></p>
<p>Comparison:</p>
<table>
<colgroup>
<col style="width: 6%">
<col style="width: 46%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Bootstrapping (update involves an estimate)</th>
<th>Sampling (update samples an expectation)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>MC</strong></td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="even">
<td><strong>TD</strong></td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td><strong>DP</strong></td>
<td>Yes</td>
<td>No</td>
</tr>
</tbody>
</table>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-8.png" alt="Two dimensions stands for sampling depth and width, respectively."><figcaption>Two dimensions stands for sampling depth and width, respectively.</figcaption>
</figure>
<h2 id="tdlambda">TD(<span class="math inline">\(\lambda\)</span>)</h2>
<p>Till now, the TD algorithm we introduced is actually TD(0), which only looks one step forward. What about looking 2 or more steps forward?</p>
<h3 id="n-step-prediction">n-Step Prediction</h3>
<p>First of all, define the n-step returns, <span class="math display">\[
G_t^{(n)} = R_{t+1} + \gamma R_{t+2} +...+ \gamma^{n-1} R_{t+n}+\gamma^nV(S_{t+n})
\]</span> For example, <span class="math display">\[
\begin{align}
\text{(TD)}~~~~~ G_t^{(1)} &amp;= R_{t+1} + \gamma V(S_{t+1})
\\  G_t^{(2)} &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})
\\ &amp;~~\vdots
\\ \text{(MC)}  ~~~ G_t^{(\infty)} &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^{T-1} R_T
\end{align}
\]</span> And so we have the n-step temporal-difference learning <span class="math display">\[
V(S_t) \leftarrow V(S_t)+ \alpha(G_t^{(n)}-V(S_t))
\]</span> <img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-9.png" alt="Two dimensions stands for sampling depth and width, respectively."></p>
<h3 id="example-large-random-walk">Example: Large Random Walk</h3>
<p>A basic problem from n-step TD algorithm is which n should we choose to achieve the best result? Researchers have made predictions under different step <span class="math inline">\(n\)</span>, different learning rate <span class="math inline">\(\alpha\)</span>, and different training methods -- on-line(update the value after every step within an episode) and off-line (update values at the end of one episode).</p>
<figure>
<img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-10.png" alt="Two dimensions stands for sampling depth and width, respectively."><figcaption>Two dimensions stands for sampling depth and width, respectively.</figcaption>
</figure>
<p>We can see from the figure that the best <span class="math inline">\(n\)</span> varies in different settings. In order to make an overall consideration, here we introduce a new parameter <span class="math inline">\(\lambda\)</span> to combine the information from all different time-steps.</p>
<h3 id="lambda-return"><span class="math inline">\(\lambda\)</span>-return</h3>
<p><img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-11.png"></p>
<p>The <span class="math inline">\(\lambda\)</span>-return <span class="math inline">\(G_t^\lambda\)</span> combines all <span class="math inline">\(n\)</span>-step returns <span class="math inline">\(G_t^{(n)}\)</span> by geometrically weighting them, which requires quite a little additional computation cost. <span class="math display">\[
G_t^\lambda = (1-\lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^{(n)}
\\ V(S_t) \leftarrow V(S_t) + \alpha (G_t^\lambda- V(S_t))
\]</span> It can easily proved that by setting the last term's coefficient to <span class="math inline">\(\lambda^{T-t-1}\)</span> we have <span class="math display">\[
\sum_{k=t+1}^{T-1} (1-\lambda)\lambda^{k-t-1} + \lambda^{T-t-1} = 1
\]</span> <img src="http://wiljohn-blog.oss-cn-shanghai.aliyuncs.com/19-02-01/rl4-12.png"></p>
<h2 id="reference">Reference</h2>
<p>[1] <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf" target="_blank" rel="noopener">David Silver's RL course</a></p>

    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/">#</a></span>
    
    </div>
    
    
</article>


<div class="sharebox">
    
<div class="sharethis-inline-share-buttons"></div>
<script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5c1be99859895a00110ffa34&amp;product=inline-share-buttons" async="async"></script>

</div>



<div class="comments">
    <h3 class="title is-4">Comments</h3>
    
<script>
    var disqus_config = function () {
        this.page.url = 'http://wiljohn.top/old/RL_prediction.html';
        this.page.identifier = 'rl-model-free-prediction';
        
        this.language = 'en';
        
    };
    (function() {
        var d = document, s = d.createElement('script');  
        s.src = '//' + 'wiljohnhong-github-io' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<div id="disqus_thread">
    
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>

    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 wiljohn&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" href="https://github.com/wiljohnhong">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>


    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    


<script src="/js/script.js"></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
    
</body>
</html>